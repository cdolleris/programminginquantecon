[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes for 3611: Programming in Quantitative Economics",
    "section": "",
    "text": "Preface\nThis is a collection of notes for the course “Programming in Quantitative Economics”.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction to the Course",
    "section": "",
    "text": "1.1 Standard functions in R\nR comes with a lot of standard functions. We can print stuff to the screen, invert matrices, perform OLS, generate random numbers, etc., so there is no need to reinvent the wheel every time!\nThese functions are also available in R packages. However, these are ‘special’ packages that are included by default in R and don’t need to be loaded. Examples are ‘stats’, ‘base’, ‘graphics’,…\nThese functions generally have sensible names:\nObviously, there are too many to list here. R manuals are freely avaiable online https://cran.r-project.org/",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the Course</span>"
    ]
  },
  {
    "objectID": "intro.html#standard-functions-in-r",
    "href": "intro.html#standard-functions-in-r",
    "title": "1  Introduction to the Course",
    "section": "",
    "text": "print to print something to the screen.\nplot to plot figures.\nsolve to invert a matrix.\nlm to perform OLS (‘linear model’ \\(\\rightarrow\\) ‘lm’)\nsum, median, mean,…",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the Course</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-lec1-se-1",
    "href": "intro.html#sec-lec1-se-1",
    "title": "1  Introduction to the Course",
    "section": "Small Exercise 1",
    "text": "Small Exercise 1\n\n\nOpen and run the .R file called Becoming_a_useR.\n\n\n\n\n\n\n\n\nClick to view “Becoming_a_useR.R”\n\n\n\n\n\n\n# 1. Add and remove elements from the environment\n# Clear the environment: Type ctrl+L for clearing the console \nrm(list = ls()) \nSys.setenv(lang = \"en_US\")\n# Define an integer \niN &lt;- 8 \n\n# Remove the object from the environment\nrm(iN) \n\n# Reintroduce the integer \niN &lt;- 8 \n\n# 2. There is difference between \"=\" and \"&lt;-\" in R. Let's discuss an example\n# A matrix with zero elements can be generated by a syntax: matrix(1, ncol = Nc, nrow = Nr), where Nc and Nr are some integers\n\nmE &lt;- matrix(0, ncol =  2)  #Saying to R the number of columns is 2\nncol                        #Comment\nmR &lt;- matrix(0, ncol &lt;- 2)  #Saying to R the number of ?rowss is 2: 2 is assigned to ncol and it is set as nrow\nncol                        #Comment\nmE # Print matrix E\nmR # Print matrix R\n\n# 3. Some error messages\n# changing the value of iN\niN &lt;- 10 \ncat(\"... but now it is \", iN, \"\\n\")\ncat(\"What am I printing now? \", iN==10, \"\\n\") # Comment\ncat(\"Now I get an error... \", iS, \"\\n\")       # Why?\n\n\n\n\n\nCan you understand what each line of the code does?\nWhat is the output?\n\nWhat are the differences in the matrices defined in line 17-19?\n\nThe matrix mE is defined properly with 2 columns. The matrix mR has 2 rows.\n\n\n\nTry to run lines 27-29. Can you comment the output?\n\nThe error is received since the variable iS is not defined.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the Course</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-lec1-se-2",
    "href": "intro.html#sec-lec1-se-2",
    "title": "1  Introduction to the Course",
    "section": "Small Exercise 2",
    "text": "Small Exercise 2\n\n\nOpen and run the .R file called Exercise_error.\n\n\n\n\n\n\n\nClick to view “Exercise_error.R”\n\n\n\n\n\n\n# 0. Clear the environment:  CTRL + L to clear the Console\nrm(list=ls())\nSys.setenv(lang = \"en_US\")\n# Define vector X\nvX &lt;- c(-1,2)\n\n# Why did the following output produce NAN output?\ncat(\"This is square root of X: \", sqrt(vX), \"\\n\") \n     # Why?\n\n\n\n\n\n\nWhy do you get an error message?\n\nThere is no error message. There is a warning message.\n\n\n\nWhich output do you get from \\(\\sqrt{-1}\\)?\n\nNaturally, you get an error as the square root of a negative number isn’t possible.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the Course</span>"
    ]
  },
  {
    "objectID": "intro.html#external-packages-in-r",
    "href": "intro.html#external-packages-in-r",
    "title": "1  Introduction to the Course",
    "section": "\n1.2 External packages in R",
    "text": "1.2 External packages in R\nMore than 12,000 external packages are avaiable on CRAN: https://cran.r-project.org/web/packages/.\nKnowing the name of a package, say (\"rugarch\"), this can be installed in R with install.packages(\"rugarch\").\nAfter a package has been installed, in order to load it into the R environment, we need to run: library(\"rugarch\").",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the Course</span>"
    ]
  },
  {
    "objectID": "intro.html#types-of-variables",
    "href": "intro.html#types-of-variables",
    "title": "1  Introduction to the Course",
    "section": "\n1.3 Types of variables",
    "text": "1.3 Types of variables\nThe function class() allows you to find the class of an R object. For instance:\n\ncfoo = \"sun\"\nclass(cfoo)\n#&gt; [1] \"character\"\n\n\nbfoo = TRUE\nclass(bfoo)\n#&gt; [1] \"logical\"\n\n\ndfoo = 1.785632458\nclass(dfoo)\n#&gt; [1] \"numeric\"\n\nThe “foo” terminology stands for a variable which is not important, i.e., a variable where intermediate results are stored. We will later see that “c”, “b” , and “d” before the “foo” actually mean something!\n\n1.3.1 Vectors, Matrices, Arrays, Data frames\nData can be organized in different ways in R depending on the particular needs. If we plan to do linear algebra, we want to use:\n\nvY = c(1, 2, 5.48652) # vectors\nmY = matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9.485), ncol = 3) # matrices\naY = array(1:27, dim = c(3, 3, 3)) # arrays\n\nIf we want to organize data (numeric/character or mixed), we use:\n\nd = data.frame(x = 1, y = 1:10, fac = LETTERS[1:10]) # data frame\n\nVectors, matrices and ararays can be: “numeric” or “character” i.e., they can contain only one of the two types of variables (we cannot have a vector with some elements “numeric” and otheres “character”). Data.frames can contain both.\n\n1.3.2 Access elements\nTo access elements of vectors, matrices and arrays, we use the square brackets:\n\nvY[1]\n#&gt; [1] 1\nvY[1:2]\n#&gt; [1] 1 2\nvY[c(3,1)]\n#&gt; [1] 5.48652 1.00000\nmY[1, 1]\n#&gt; [1] 1\nmY[1, ]\n#&gt; [1] 1 4 7\nmY[1, c(1,3)]\n#&gt; [1] 1 7\naY[1, 1, 1]\n#&gt; [1] 1\naY[1, 1, ]\n#&gt; [1]  1 10 19",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the Course</span>"
    ]
  },
  {
    "objectID": "intro.html#how-to-get-help",
    "href": "intro.html#how-to-get-help",
    "title": "1  Introduction to the Course",
    "section": "\n1.4 How to get help",
    "text": "1.4 How to get help\nWhen you are in trouble the help() function is your friend! Whenever you want to understand the functioning of any function, say list, you type:\n\nhelp(list)\n\nThis is equivalent to:\n\n?list\n\nIf you don’t remember the full function name, but only a part of it (or something related to it), you can use the “??” operator, for example:\n\n??download",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the Course</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-lec1-se-3",
    "href": "intro.html#sec-lec1-se-3",
    "title": "1  Introduction to the Course",
    "section": "Small Exercise 3",
    "text": "Small Exercise 3\n\n\nOpen and run the .R file called  Get_2_know_the_helpeR.R.\n\n\n\n\n\n\n\nClick to view “Get_2_know_the_helpeR.R”\n\n\n\n\n\n\n#Clear the environment\nrm(list = ls()) \nSys.setenv(lang = \"en_US\")\n\n# Define a vector A\nvA &lt;- c(1, 2, 3, 4, 5, 6) \n\n# Define a 2x3 matrix A\nmA &lt;- matrix(vA, nrow = 2, ncol = 3, byrow = TRUE)\n\n# Define a 2x2 matrix B \nmB &lt;- matrix(c(9, 8, 7, 6), ncol = 2, byrow = TRUE, nrow = 2) \n\n# Print matrix B\nmB\n\n# Find out what the function crossprod (from the \"base\" package) does via the help() function\nmC &lt;- crossprod(mA,mB) \nmC\n\n# The below matrix will give you an error. Try to reproduce the matrix mC via linear algebra based on what you learned from the help function!\nmD &lt;- mA %*% mB \n# Hint: Try ??transpose and look for Matrix Transpose.\nmD &lt;- t(mA) %*% mB \nmD # Print matrix D\n\n# Use one of the below to test if mC and mD are equal\nall.equal(mC,mD) # Check if all mC and mD are equal, returns \"TRUE\" if this is mC = mD.\nvCheck &lt;- c(mC) == c(mD) # Form a vector of booleans, look up ?'=='\nall(vCheck) # Check if all mC and mD are equal, returns \"TRUE\" if mC = mD.\n\n\n\n\n\nRun the code line by line. Do you understand it?\n\nTry to look up some of the functions with the help-function.\n\nDiscover how to transpose mA to obtain mD?\n\n\n\nmd &lt;- t(mA) %*% mB\n\n\n\nEvaluate whether your mD is equal to mC.\n\n\nall.equal(mC,mD) # Check if all mC and mD are equal, returns \"TRUE\" if this is mC = mD.\nvCheck &lt;- c(mC) == c(mD) # Form a vector of booleans, look up ?'=='\nall(vCheck) # Check if all mC and mD are equal, returns \"TRUE\" if mC = mD.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the Course</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-lec1-se-4",
    "href": "intro.html#sec-lec1-se-4",
    "title": "1  Introduction to the Course",
    "section": "Small Exercise 4",
    "text": "Small Exercise 4\n\nTry to install the package “Rfast”. (Hint: Go back a couple of slides to find the relevant function).\n\n\ninstall.packages(\"Rfast\")\n\n\nLoad the package and try the following code:\n\n\nlibrary(\"Rfast\") # Loads the package\nmA &lt;- matrix(1:100,10,10)\nrowVars(mA)\n\n\n\nThere are only a finite amount of names to call functions (especially if it has to make sense). What happens if you have two packages that have a similar name? E.g., rowVars from Rfast and matrixStats?\n\n\nEither use:\n\ndetach(\"package:Rfast\", unload=TRUE)\n\nOr by the use of namespaces, i.e., Rfast::rowVars(mA) to run the function from the package we want. (This syntax also carries over to other programming languagues, e.g., directly to C++)",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the Course</span>"
    ]
  },
  {
    "objectID": "intro.html#additional-code-in-r",
    "href": "intro.html#additional-code-in-r",
    "title": "1  Introduction to the Course",
    "section": "\n1.5 Additional code in R",
    "text": "1.5 Additional code in R\n\n\n\n\n\n\nClick to view “Week1_lecture1.R”\n\n\n\n\n\n\n# 0. Clear the environment:  CTRL + L to clear the Console\nrm(list=ls())\nSys.setenv(lang = \"en_US\")\n\n# 1. How to install packages: \"rugarch\" a package for Univariate GARCH Models\ninstall.packages(\"rugarch\")\nlibrary(rugarch)\n\n# 2. How to use help to learn more about this package\n# 2.1. General info about the entire package\nhelp(\"rugarch\")\n# or \n?rugarch\n\n# 2.2. Info on a specific command from the package\ntopic &lt;- \"VaRplot\"\npkg_ref &lt;- \"rugarch\"\nhelp((topic),(pkg_ref))\n#or\nhelp(\"VaRplot\")\n#or type if you are not sure about the name\n??VaRp\n\n# 3. Types of variables: Character,Logical,Numeric\ncfoo = \"sun\"\nclass(cfoo)\n\nbfoo = TRUE\nclass(bfoo)\n\ndfoo = 1.785632458\nclass(dfoo)\n\n# 4. Vectors and matrices\n# 4.1. Define a vector\nvX &lt;- c(1,2,3) \n#or alternatively\nvX &lt;- c(1:3)\n#Another alternative for a step size Step=1\nStep&lt;-1\nvX &lt;-seq(1, 3, by=Step) \n#Take a look at the seq() function\n?seq\n\n# Length of the vector\nn&lt;-length(vX)\n# Generate a diagonal matrix with elements of vX on the main diagonal\ndX&lt;- diag(vX)\n# Return a vector consisting of the diagonal elements of dX\nprint(diag(dX))\n# For a scalar K create an identity matrix\nK&lt;-5\nI&lt;-diag(K)\nprint(I)\n\n# 4.2. Different ways of specifying matrices\n# 3x3 Matrices: elements are allocated by row\nmX &lt;- matrix(vX, ncol = 3, nrow = 3, byrow = TRUE)\nprint(mX)\nmX &lt;- matrix(vX, ncol = 3, nrow = 3, byrow = 1) \nprint(mX)\n\n# 3x3 Matrices: elements are allocated  by column\nmX &lt;- matrix(vX, ncol = 3, nrow = 3, byrow = FALSE)\nprint(mX)\nmX &lt;- matrix(vX, ncol = 3, nrow = 3, byrow = 0) \nprint(mX)\n\n# Number of elements in a matrix\nnm&lt;-length(mX)\n\n# Dimensions of a matrix\nnm&lt;-dim(mX)\n\n\n# Accessing Elements\ndC &lt;- mX[2, 3] # 2nd row, 3rd column\nvC &lt;- mX[, 3] # all rows, 3rd column\ndD &lt;- mX[, 3][1] # 1st row, 3rd column\nvD &lt;- mX[2, ] # 2nd row, all columns\n\n\n# Accessing Elements of a vector\nvX[1]\nvX[2]\nvX[n]\n\n# A matrix where each element is h\nh&lt;-1\nmh&lt;-matrix(h, ncol = 3, nrow = 4)\n\n# 4.3. Matrix manipulations\n# 4.3.1. Add, Subtract...\nmA &lt;- matrix(c(1, 2, -4, 7, 2, 3), nrow = 2)\nprint(mA)\nmB &lt;- matrix(c(3, 8, 2, 9, 1, 4), nrow = 2)\nprint(mB)\n\n# Add and subtract\nprint(mA + mB)\nprint(mA - mB)\n\n# 4.3.2. Multiplication, Matrix transpose\n# Element by element multiplication\nprint(mA*mB)\n\n# Matrix multiplication: this Will report an error because... \nprint(mA%*%mB)\n\n#Let's introduce matrix transpose \nprint(t(mB))\nprint(mA%*%t(mB))\n\n# Or\nprint(t(mA)%*%(mB))\n\n#Similarly A'*B can be calculated as\ncrossprod(mA,mB)\n\n# Let's multiply the matrix A with a vector vX\nprint(mA%*%vX)\n\n# 4.3.3. Matrix inversion\n# Define an invertible matrix\nmC &lt;- matrix(c(1, 0.5, 0.9, 0.5, 1, 0.4, 0.9, 0.4, 1), ncol = 3, nrow = 3, byrow = TRUE)\n# Inverse of a matrix\ninvC &lt;- solve(mC)\n# One can verify this invC%*%C must be an identity matrix\nprint(invC%*%mC)\n\n\n# 5. Array\n# 3 matrices each 3x3\naY = array(1:27, dim = c(3, 3, 3))\nprint(aY)\n# First matrix in the array \naY[,,1]\n# Second matrix in the array \naY[,,2]\n# (1,2) element of the Second matrix in the array \naY[1,2,2]\n# (1,2) element of the each matrix in the array \naY[1,2,]\n\n#6. Data frames\nd &lt;- data.frame(x = 1, y = 1:10, l = LETTERS[1:10]) \n#Visualize the data\nprint(d )\n#extract the first series\nd$y\n#extract the second series\nd$x\n#extract the third series\nd$l",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to the Course</span>"
    ]
  },
  {
    "objectID": "getting_started_with_r.html",
    "href": "getting_started_with_r.html",
    "title": "\n2  Getting Started with R\n",
    "section": "",
    "text": "2.1 Working directory\nWhen you run R, it nominates one of the directories on your hard drive as a working directory, which is where it looks for user-written programs and data files.\nYou can determine the current working directory using the command getwd. You can do this using the command setwd(\"sDir\"), where dir is the directory address.\ngetwd()\nsDir &lt;- \"G:/foo\" # this path needs to exist\nsetwd(sDir)\ngetwd()\nIf the path does not exist, R can create the directory (“folder”) for you via the dir.create built-in function and then change to the new directory.\n# Assign a new path to create the new directory\nsPath &lt;- getwd()\nsDir &lt;- file.path(sPath, \"new_dir2\")\ndir.create(sDir) # Creates a new directory\nsetwd(sDir) # set WD to sDir\nYou can also just pass the directory path directly, i.e., dir.create(\"C:/user/foo\").\nNote that R will return an error message if “\\(\\backslash\\)” is used instead of “\\(/\\)” in the path as this is reserved for other operations, e.g., the line change used in Small Exercise 1 ; cat(\"\\n\")\nWhen the working directory is set, we can call other scripts from the currently specified “WD” without specifying the path via the source() function, i.e.:\nsource(\"foo.r\") # tell R to Source/Run the .R file/script\nCalling e.g. scripts from another directory is also possible, but requires a specific path, i.e., \"C:/wd/foo.r\".\nFurthermore, this feature is incredibly useful when writing longer scripts containing several user written functions.\nThis is demonstrated in Example_1.R:\nNote for the script above: The sourced file, example1_fun.r, can be seen below:\nIn addition, source will check your program for completeness before executing it. That is, it will stop processing the call if an error is reported.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "getting_started_with_r.html#working-directory",
    "href": "getting_started_with_r.html#working-directory",
    "title": "\n2  Getting Started with R\n",
    "section": "",
    "text": "Click to view “Example_1.R”\n\n\n\n\n\n\nrm(list = ls()) # clear the environment\ncat(\"\\014\")     # Clear console:equivalent to ctrl + L \n\n\ngetwd() # Check working directory is correct\n\ndir() # Check that the directory contain the desired files.\n\n# Source the functions contained in the script \"example1_fun.r\"\nsource(\"example1_fun.r\") \n\nsquaring(-3)  # Calling the function squaring with input -3\n\nsquaring(3)   # Calling the function squaring with input 3\n\nsquaring(\"a\") # Calling the function squaring with input \"a\"\n\nclc() # Use the newly defined clc() function to clear console\n\n\n\n\n\n\n\n\n\n\n\nClick to view “example1_fun.R”\n\n\n\n\n\n\n# This file contains two functions. \n# At this stage it is not necessary that you completely understand the functions, they will be discussed later.\n\n## Function 1: squaring\n# input numeric types: dX\n# Output the square of the input\nsquaring &lt;- function(dX) {\n  dX^2\n}\n\n\n## Function 2: clc\n# non-input function to clear console\nclc &lt;- function() {\ncat(\"\\014\") # \"\\014\" is equivalent to ctrl + L (Windows) or cmd + L (MacOS)\n}",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "getting_started_with_r.html#arithmetic",
    "href": "getting_started_with_r.html#arithmetic",
    "title": "\n2  Getting Started with R\n",
    "section": "\n2.2 Arithmetic",
    "text": "2.2 Arithmetic\nR uses the usual symbols for addition \\(+\\), subtraction \\(-\\), multiplication \\(*\\), division \\(/\\), and exponentiation \\(\\wedge\\). Parentheses \\(()\\) can be used to specify the order of operations.\n\n(1 + 1/100)^100\n#&gt; [1] 2.704814\n\nNotice that by default, R prints 7 significant digits. You can change the display to x digits using options(digits = x). See help(options) for other options.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "getting_started_with_r.html#variable-types",
    "href": "getting_started_with_r.html#variable-types",
    "title": "\n2  Getting Started with R\n",
    "section": "\n2.3 Variable types",
    "text": "2.3 Variable types\n\n2.3.1 Defining variables\nTo assign a value to a variable, we use (almost equivalently) the assignment commands = and &lt;-. For example:\n\niX &lt;- 100\niX = 100\n# These are equivalent\n\nWith some exceptions as shown last time in Becoming_a_useR.R. We can perform operations in iX:\n\niX\n#&gt; [1] 100\n(1 + 1/iX)^iX\n#&gt; [1] 2.704814\niX &lt;- iX + 1 # is allowed\niX\n#&gt; [1] 101\n\n\n2.3.2 Variable types and syntax\nIf you try to pass the string “a” to the function squaring, you get an error. Functions only take sensible inputs: in this case “numeric” class objects such as double or integers (and even vectors or matrices…)\nIn addition, R has several object type specific functions, i.e., length(vY) will give you the length of a vector, but mY will return the length of vY &lt;- c(mY). Hence, keeping track of variable types or variable classes therefore seems worthwhile.\n\nTo R, everything is an “object”.\nWe’ve seen some examples of objects as the atomic types/classis previously:\n\n\nIntegers, such as 3L or as.integer(3).\n\nNumeric, such as 3, -3, 9.01 (So in principle both doubles and “integers”).\n\nLogicals, such as TRUE or FALSE, “booleans”:\n\nCharacters, such as “asdf”.\nAnd data structures as Matrices, such as \\(\\begin{pmatrix}1 & 2 & 3\\\\4 & 5 & 6\\end{pmatrix}\\)\n\n\n\nWe’ll study these types, and a few other common ones in detail later on.\n\n2.3.3 Variable names and Hungarian notation\nHungarian notation was developed for more restrictive languages, but also has its uses in R.\n\nUse variable names consisting of multiple characters.\nThe first one of two indicate the type of the variable.\nThe remainder forms a descriptive name.\n\nExample: suppose that you are estimating a linear model: \\(y = X\\beta + \\varepsilon\\). So, the input consists of a vector \\(y\\) and a matrix \\(X\\). Variable names could be vY and mX. The output is probably a vector of estimated regression coefficients, which you could call vBeta or vBetaHat or vBetaOLS or something. And perhaps there’s also a vEpsilon or vEps.\nR doesn’t care what you call your variables. As soon as a variable is given a value, R will figure out what type it is automatically.\nSo, if you calculate \\((X'X)^{-1}X'y\\) and want to call the result apple, this is not technically wrong. But it’s not particularly nice to people reading your code either.\n\n2.3.4 Integers\nThe simplest value type is an integer. Its value is always a whole number, minimum \\(\\approx - 2^{31}\\), maximum \\(2^{31} - 1\\) (other bounds in other programming languages). The Hungarian prefix is i. The R command iN &lt;- 5L results in the creation of a variable that has name iN, type integer, and value 5. If we subsequently execute command iN &lt;- 7L, the variable still has the same name and type, but it has a different value. R forgets the value before the change.\nA logical, prefix b, is a special type of integer that holds boolean values. It can only take two different values: TRUE (=1) or FALSE (=0).\n\n2.3.5 Doubles\nFor a real number that’s not an integer, we generally use a double, Hungarian: d. We can write things like, dPi &lt;- 3.14159. Note, there’s nothing that prevents a double variable having an integer value. It would be weird if dX could be 4.9, 4.99, 4.999, 5.1, 5.01, 5.001, but not exactly 5. However, redefining the variable as dX &lt;- 5 will create an integer, not a double. Instead, define dX &lt;- 5.0.\n\n2.3.6 Precision\nNote, that \\((1/3 + 1/3 + 1/3 + 1/3 + 1/3 + 1/3)-2\\) is not exactly zero in R; it’s something like -0.0000000000002220446.\nIntegers aren’t rounded, so integer arithmetic is done exactly. To avoid unnecessary rounding errors, always use the integer type if you know you’re only going to be dealing with integers.\nFor the most part, R will automatically set the type for you as needed, so no need to put too much emphasis on 5 vs 5L. Use Hungarian “i” notation for both.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "getting_started_with_r.html#built-in",
    "href": "getting_started_with_r.html#built-in",
    "title": "\n2  Getting Started with R\n",
    "section": "\n2.4 Built-in",
    "text": "2.4 Built-in\n\n2.4.1 Character/strings\nCharacters are the R objects for storing strings. If we’re interested in storing text (a character string), we use a character. A string needs to be enclosed in quotation marks. Thus, the command to create a string variable that contains Hello world is sText &lt;- \"Hello world\" or sText &lt;- 'Hello world'. We can use functions such as substr() to access part of a string, e.g.:\n\nsText &lt;- \"Hello world\"\nsubstr(sText, start = 6, stop = nchar(sText))\n#&gt; [1] \" world\"\n\nNote that R has built-in functions such as is.character(), as.character() but not is.string() built.in. FOr comparability with other languages, we use Hungarian s, but c is also fine.\n\n2.4.2 Built-in functions\nR has a number of built-in functions, for example sin(x), cos(x), tan(x), (all in radians), exp(x), log(x), and sqrt(x). Some special constants such as pi (\\(\\pi\\)) are also predefined.\n\nexp(1)\n#&gt; [1] 2.718282\noptions(digits = 16)\nexp(1)\n#&gt; [1] 2.718281828459045\npi\n#&gt; [1] 3.141592653589793\nsin(pi/6)\n#&gt; [1] 0.4999999999999999\n\nThe functions floor(x) and ceiling(x) round down and up, respectively, to the nearest integer.\n\n2.4.3 Functions\nIn mathematics, a function takes one or more arguments (or inputs) and produces one or more outputs (or return values). Functions in R work in an analogous way. Consider the seq function. This function allows you to create a sequence of numbers:\n\nseq(from = 1, to = 9, by = 2)\n#&gt; [1] 1 3 5 7 9\n\nNotice that by default (see help(seq)), the by argument is equal to 1, such that:\n\nseq(from = 1, to = 3)\n#&gt; [1] 1 2 3\n\nFor a sequence with by equal to 1, R has a shorthand notation, se we may use to:from;\n\n1:4\n#&gt; [1] 1 2 3 4\n\n\n2.4.4 Arguments of functions\nEvery function has a default order for the arguments. For seq this is: from, to, by. If you provide arguments in this order, then they do not need to be named explicitly.\n\nseq(1, 9, 2)\n#&gt; [1] 1 3 5 7 9\n\nBut you can choose to give the arguments out of order provided you give them names in the format argument_name = expression.\n\nseq(by = 2, to = 9, from = 1)\n#&gt; [1] 1 3 5 7 9\n# Is different from\nseq(2, 9, 1)\n#&gt; [1] 2 3 4 5 6 7 8 9",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "getting_started_with_r.html#vectors",
    "href": "getting_started_with_r.html#vectors",
    "title": "\n2  Getting Started with R\n",
    "section": "\n2.5 Vectors",
    "text": "2.5 Vectors\nWe have already seen the c function. However, vectors can be created by any function that returns vectors as output. For instance, seq and rep. The rep(vX, n) function exactly replicates the vector vX, \\(n\\) times.\n\nvX = c(1, 3, 4)\nrep(vX, 3)\n#&gt; [1] 1 3 4 1 3 4 1 3 4\n\nThe function c also accepts vectors as inputs:\n\nvX &lt;- seq(1, 20, by = 2)\nvY &lt;- rep(3, 4)\nvZ &lt;- c(vY, vX)\nvZ\n#&gt;  [1]  3  3  3  3  1  3  5  7  9 11 13 15 17 19\n\n\n2.5.1 Vector operations\nAll algebraic operations are defined for vectors and act on each element separately, that is, element-wise:\n\nvX &lt;- c(1, 2, 3)\nvY &lt;- c(4, 5, 6)\nvX * vY\n#&gt; [1]  4 10 18\nvX + vY\n#&gt; [1] 5 7 9\nvY ^ vX\n#&gt; [1]   4  25 216\n\nHowever, care should be taken when vectors of unequal length are used:\n\nc(1, 2, 3) + c(1, 2)\n#&gt; Warning in c(1, 2, 3) + c(1, 2): longer object length is not a multiple of\n#&gt; shorter object length\n#&gt; [1] 2 4 4\n\nWhen you apply an algebraic expression to two vectors of unequal length, R automatically repeats the shorter vector until it has something the same length as the longer vector.\n\nc(1, 2, 3, 4) + c(1, 2)\n#&gt; [1] 2 4 4 6\n(1:10) ^ c(1, 2)\n#&gt;  [1]   1   4   3  16   5  36   7  64   9 100\n\nThis happens even when the shorter vector is of length 1, allowing the shorthand notation:\n\n2 + c(1, 2, 3)\n#&gt; [1] 3 4 5\n2 * c(1, 2, 3)\n#&gt; [1] 2 4 6\n\nFor example, using the modulus operator: a mod b (%%):\n\n1:20 %% 3\n#&gt;  [1] 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "getting_started_with_r.html#missing-values",
    "href": "getting_started_with_r.html#missing-values",
    "title": "\n2  Getting Started with R\n",
    "section": "\n2.6 Missing values",
    "text": "2.6 Missing values\n\n2.6.1 Missing date: NA\nIn real experiments, it is often the case, for one reason or another, that certain observations are missing. Depending on the statistical analysis involved, missing data can be ignored or invented (a process called imputation). R presents missing observations through the data value NA. They can be mixed in with all other kinds of data:\n\nvA &lt;- c(11, NA, 13)\nvA\n#&gt; [1] 11 NA 13\n\nPerforming analysis with NAs can be problematic:\n\nmean(vA) # NAs can propagate\n#&gt; [1] NA\n\nThe logical argument na.rm is often specified to deal with NAs.\n\nmean(vA, na.rm = TRUE) # NAs can be removed\n#&gt; [1] 12\n\n\n2.6.2 Searching for NAs\nSometimes you want to search for NAs in your dataset to prevent misleading results at the end of your analysis. THe function is.na searches for NAs inside vectors and returns a logical output of the same size of the input provided:\n\nvA &lt;- c(11, NA, 13)\nis.na(vA) # identify missing elements\n#&gt; [1] FALSE  TRUE FALSE\n\nWhen length(vA) is very large, we might want to write:\n\nany(is.na(vA)) # are any missing?\n#&gt; [1] TRUE\n\nIn order to remove NAs we can use:\n\nna.omit(vA)\n#&gt; [1] 11 13\n#&gt; attr(,\"na.action\")\n#&gt; [1] 2\n#&gt; attr(,\"class\")\n#&gt; [1] \"omit\"",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "getting_started_with_r.html#small-exercise",
    "href": "getting_started_with_r.html#small-exercise",
    "title": "\n2  Getting Started with R\n",
    "section": "\n2.7 Small Exercise",
    "text": "2.7 Small Exercise\n\n\nDefine a vector vX containing the double 5.01, the string “hello”, the logical/boolean FALSE and the integer 10L.\n\nvX &lt;- c(5.01,\"hello\",FALSE,10L)\n\n\n\nDetermine the type of this vector? (Use the typeof() built-in function or look at values in the environment).\n\nThe vector is of type character.\n\n\n\nDo you have an explanation for why it is this type? (Try instead to assign all the above values to vX but not “hello” i.e., c(5.01, FALSE, 10L) and repeat the second bullet point).\n\nTypes are coerced in a fixed order: character, double, integer and logical. This only occurs for vectors. Lists are able to retain the correct datatypes.\n\n\n\nDefine a vector vY as (vY &lt;- c(NaN, NA, 6L))\n\n\ncompare the output for is.na(vY) and is.nan(vY).\n\n\nis.nan does not recognize the NA.\n\n\n\nWhat is the difference between NaN and NA? Try to print 0/0 in the console\n\nNaN: “Not a Number”, which refers to things that cannot be calculated.\nNA: “Not Available”, referring to values that are unknown or missing.\n\n\n\n\n\n\n\n\n\nClick to view the complete solution to the exercise\n\n\n\n\n\n\nrm(list = ls()) # clear environment\ncat(\"\\014\")     # Clear console:equivalent to ctrl + L \n\n# Small exercise with vectors and missing values\n# Exercises 1\ns &lt;- c(5.01,\"hello\",FALSE,10L)\ntypeof(s) # Note the vector is of mode \"character\"\ns &lt;- c(5.01,FALSE,10L)\ntypeof(s) # Without the character element the vector is now of mode \"double\"\ns &lt;- c(FALSE,10L) \ntypeof(s) # ... Now of mode integer\ns &lt;- c(FALSE)\ntypeof(s) # ... Now of mode logical\n\n# Exercise 2\nvY &lt;- c(NaN,NA,6L) # define vector\nvY\nis.na(vY)  # look for NA-value\nis.nan(vY) # look for NaN-value\n# NaN: \"Not a Number\", used for things that cannot be calculated\n# NA: \"Not Available\", values that are unknown or missing\n\n# NaN examples:\n0/0      # is NaN\nsqrt(-1) # is NaN\nInf-Inf  # is NaN\nNaN/NA   # is NaN\n\n## Extra question..\n# Can NA matrices be used for more memory efficient \n# usage as predefined matrix/vectors prior to i.e. a loop?\nobject.size(matrix(NA,1000,1000)) # Size: 4000216 bytes\nobject.size(matrix(0,1000,1000))  # Size: 8000216 bytes",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "getting_started_with_r.html#logical",
    "href": "getting_started_with_r.html#logical",
    "title": "\n2  Getting Started with R\n",
    "section": "\n2.8 Logical",
    "text": "2.8 Logical\n\n2.8.1 Logical expression\nA logical expression is formed using the comparison operators:\n\n\\(&lt;\\) lower than\n\\(&gt;\\) greater than\n\\(\\le\\) lower than or equal to\n\\(\\ge\\) greater than or equal to\n\\(==\\) equal to\n\\(!=\\) not equal to\n\\(\\&\\) and\n\\(|\\) or\n\\(!\\) not\n\nThe operators return a logical output:\n\nvX = c(1, 2, 3, 4)\nvX == 2\n#&gt; [1] FALSE  TRUE FALSE FALSE\nvX != 2\n#&gt; [1]  TRUE FALSE  TRUE  TRUE\n\nVectors (but also matrices and arrays) can be accessed also using logical indicators:\n\nvX &lt;- c(1, 3, 4, 18)\nvX &gt; 2\n#&gt; [1] FALSE  TRUE  TRUE  TRUE\nvX[vX &gt; 2 & vX &lt; 10]\n#&gt; [1] 3 4\n\nThe subset function can be used for a similar scope:\n\nsubset(vX, subset = vX &gt; 2)\n#&gt; [1]  3  4 18\n\n\n2.8.2 Logical expressions: & and |\nTake two logical objects:\n\nvX = 4\nvY = vX &gt; 2\nvY\n#&gt; [1] TRUE\nvZ = vX &lt; 3\nvZ\n#&gt; [1] FALSE\n\nThe & operator returns TRUE if both vY and vZ are TRUE, and FALSE otherwise:\n\nvY & vZ\n#&gt; [1] FALSE\n\nThe | operator returns TRUE if y or z are TRUE, and FALSE otherwise:\n\nvY | vZ\n#&gt; [1] TRUE\n\n\n2.8.3 Sequential && and ||\nThe logical operators && and || are sequantially evaluated version of & and |, respectively.\nTo evaluate vX & vY, R first evaluates vX and vY, then returns TRUE, if vX and vY are both TRUE, and FALSE otherwise.\nTo evaluate vX && vY, R first evaluates vX. If vX is FALSE then R returns FALSE without evaluating vY. If vX is TRUE, R evaluates vY and returns TRUE if vY is TRUE, and FALSE otherwise.\nSequantial evaluation of vX and vY is useful when vY is not always well defined, or when vY takes a long time to compute.\nAs an example of the first instance, suppose we wish to know if dX * sin(1/dX) = 0:\n\ndX &lt;- 0\ndX * sin(1/dX) == 0\n#&gt; Warning in sin(1/dX): NaNs produced\n#&gt; [1] NA\n(dX == 0) | (sin(1/dX) == 0)\n#&gt; Warning in sin(1/dX): NaNs produced\n#&gt; [1] TRUE\n(dX == 0) || (sin(1/dX) == 0)\n#&gt; [1] TRUE\n\nNote that %% and || only work on scalars, whereas & and | work on vectors on a element-by-element basis:\n\nvX &lt;- c(1:10) # Note: c() is not necessary here.\nvX\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10\nbTest &lt;- FALSE\n(vX == 1 & bTest == TRUE)\n#&gt;  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n(vX == 1 && bTest == TRUE) # Not even possible to run\n#&gt; Error in vX == 1 && bTest == TRUE: 'length = 10' in coercion to 'logical(1)'\n(vX == 2 && bTest == FALSE)\n#&gt; Error in vX == 2 && bTest == FALSE: 'length = 10' in coercion to 'logical(1)'\n(vX == 1 && bTest == FALSE)\n#&gt; Error in vX == 1 && bTest == FALSE: 'length = 10' in coercion to 'logical(1)'",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "getting_started_with_r.html#matrices",
    "href": "getting_started_with_r.html#matrices",
    "title": "\n2  Getting Started with R\n",
    "section": "\n2.9 Matrices",
    "text": "2.9 Matrices\nWe have already seen how to build matrices in R:\n\nmA &lt;- matrix(1:6, nrow = 2, ncol= 3, byrow = TRUE)\n\nTo retrieve the dimension of a matrix, use dim:\n\ndim(mA)\n#&gt; [1] 2 3\n\nUseful functions for matrices:\n\ndiag, extract the diagonal elements and return a vector or create a diagonal matrix, see help(diag).\nrbind, join matrices with rows of the same length (stacking vertically).\ncbind, join matrices with columns of the same length (stacking horizontally).\nsolve, invert a matrix.\neigen, extract eigenvalues and associated eigenvectors of a matrix.\nt, transpose of matrix.\n\n\n2.9.1 Multiplication with matrices\nDefine:\n\nmA &lt;- matrix(c(3, 5, 2, 3), nrow = 2, ncol = 2)\nmB &lt;- matrix(c(9, 4, 1, 2), nrow = 2, ncol = 2)\nmA\n#&gt;      [,1] [,2]\n#&gt; [1,]    3    2\n#&gt; [2,]    5    3\nmB\n#&gt;      [,1] [,2]\n#&gt; [1,]    9    1\n#&gt; [2,]    4    2\n\nElement-wise multiplication, \\(A \\circ B\\) (Hadamard product):\n\nmA * mB\n#&gt;      [,1] [,2]\n#&gt; [1,]   27    2\n#&gt; [2,]   20    6\n\nMatrix multiplication, \\(AB\\):\n\nmA %*% mB\n#&gt;      [,1] [,2]\n#&gt; [1,]   35    7\n#&gt; [2,]   57   11\n\nKronecker product, \\(A \\otimes B\\):\n\nkronecker(mA, mB)\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]   27    3   18    2\n#&gt; [2,]   12    6    8    4\n#&gt; [3,]   45    5   27    3\n#&gt; [4,]   20   10   12    6",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "getting_started_with_r.html#factors",
    "href": "getting_started_with_r.html#factors",
    "title": "\n2  Getting Started with R\n",
    "section": "\n2.10 Factors",
    "text": "2.10 Factors\nR has a special data structure for Categorical variables that take ordinal or cardinal values.\n\nOrdinal (rankable): Low, medium, high.\nCardinal (Unrankable values): Colour, marital status, names, etc…\n\nIn R, we (may) deal with categorical variables in factor variables; each category is called a level.\n\nPackages or statistical models may facilitate easy use of the factor class, .e.g, in lm to create dummies from the categorical variable.\nMemory efficient data storage.\n\n\nphys.act &lt;- c(\"L\", \"H\", \"H\", \"L\", \"M\", \"M\")\nphys.act[2] &gt; phys.act[1]\n#&gt; [1] FALSE\nphys.act &lt;- factor(phys.act, levels = c(\"L\", \"M\", \"H\"), ordered = TRUE)\nis.ordered(phys.act)\n#&gt; [1] TRUE\nphys.act[2] &gt; phys.act[1]\n#&gt; [1] TRUE",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "getting_started_with_r.html#lists",
    "href": "getting_started_with_r.html#lists",
    "title": "\n2  Getting Started with R\n",
    "section": "\n2.11 Lists",
    "text": "2.11 Lists\nWe have seen that all the elements of a vector have to be of the same type: numeric, character, or logical. The type of the vector is called: mode.\nA list is an indexed set of objects (and so has a length) whose elements can be of different types, including other lists! The mode of a list is list.\nA list is just a generic container for other objects. The power and utility of lists comes from this generality. In R, lists are often used for collecting and storing complicated function outputs. For example, the first element of a list can be a vector, the second can be another list and the third can be a matrix.\nA list is created using the list(...) command, with comma-separated arguments:\n\nmy.list &lt;- list(\"one\", TRUE, 3, c(\"f\", \"o\", \"u\", \"r\"))\n\nDouble-square brackets are used to extract a single element:\n\nmy.list[[1]]\n#&gt; [1] \"one\"\nmode(my.list[[1]])\n#&gt; [1] \"character\"\n\nSingle-square brackets are used to select a sublist:\n\nmy.list[1]\n#&gt; [[1]]\n#&gt; [1] \"one\"\nmode(my.list[1])\n#&gt; [1] \"list\"\n\nWhen displaying a list, R uses double-square brackets [[1]], and [[2]], etc. to indicate list element. The single-square brackets [1], [2], etc. incide vector elements within the list.\nThe elements of a list can be named when the list is created, using arguments of the form name1 = x1, name2 = x2, etc.:\n\nmy.list &lt;- list(first = \"one\", second = TRUE, third = 3, fourth = c(\"f\", \"o\", \"u\", \"r\"))\nnames(my.list)\n#&gt; [1] \"first\"  \"second\" \"third\"  \"fourth\"\nmy.list$second\n#&gt; [1] TRUE\n\nNote the use of the $ operator to access named lists. Alternatively, a list element can be named later by assigning a value to the names attribute:\n\nmy.list &lt;- list(\"one\", TRUE, 3, c(\"f\", \"O\", \"u\", \"r\"))\nnames(my.list) &lt;- c(\"first\", \"second\", \"third\", \"fourth\")\n\nTo flatten a list x, i.e., convert it to a vector, we use unlist(x):\n\nx &lt;- list(1, c(2, 3), c(4, 5, 6))\nunlist(x)\n#&gt; [1] 1 2 3 4 5 6\n\nIf the list object itself comprises lists, then these lists are also flattened, unless the argument recursive = FALSE is set in unlist.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "getting_started_with_r.html#dataframes",
    "href": "getting_started_with_r.html#dataframes",
    "title": "\n2  Getting Started with R\n",
    "section": "\n2.12 Dataframes",
    "text": "2.12 Dataframes\nWe have already seen how to work in R with numbers, strings, and logical valuels. We have also worked with homogeneous collections of such objects, grouped into numeric, character, or logical vectors.\nThe defining characteristic of the vector data structure in R is that all componenets must be of the same mode. Obviously, to work with datasets from real experiments, we need a way to group data of differing modes.\nSuppose to have the following dataset representing a forestry experiment in which we randomly selected a number of plots and then from each plot selected a number of trees.\n\n\nPlot\nTree\nSpecies\nDiameter (cm)\nHeight (m)\n\n\n\n2\n1\nDF\n39\n20.5\n\n\n2\n2\nWL\n48\n33.0\n\n\n3\n2\nGF\n52\n30.0\n\n\n3\n5\nWC\n36\n20.7\n\n\n3\n8\nWC\n38\n22.5\n\n\n:\n:\n:\n:\n:\n\n\n\nFor each tree, we measured its height and diameter (which are numeric), and also the species of tree (which is a character string).\nAs experimental data collected in a table looks like an array, you may be tempted to represent it in R as a matrix. But in R, matrices cannot contain heterogeneous data (data of different modes, like numeric and character)\nLists and dataframes are able to store much more complicated data structures than matrices. A dataframe is a list of vectors restricted to be of equal length. Each vector (column of the dataframe) can be of any of the basic modes of object. To create a dataframe, we write:\n\nmData &lt;- data.frame(Plot = c(1, 2, 2, 5, 8, 8),\n                    Tree = c(2, 2, 3, 3, 3, 2),\n                    Species = c(\"DF\", \"WL\", \"GF\", \"WC\", \"WC\", \"GF\"),\n                    Diameter = c(39, 48, 52, 35, 37, 30),\n                    Height = c(20.5, 33.0, 30.0, 20.7, 22.5, 20.1))\n\n\n2.12.1 Dataframes: Extract\nEach column, or variable, in a dataframe has a unique name. We can extract that variable by means of the dataframe name, the column name, the a dollar sign, or as we do for a matrix:\n\nmData$Diameter\n#&gt; [1] 39 48 52 35 37 30\nmData[[\"Diameter\"]]\n#&gt; [1] 39 48 52 35 37 30\nmData[[4]]\n#&gt; [1] 39 48 52 35 37 30\nmData[, 4]\n#&gt; [1] 39 48 52 35 37 30\nmData[, \"Diameter\"]\n#&gt; [1] 39 48 52 35 37 30\n\n\n2.12.2 Dataframes: Assign\nTo assign a new variable to a dataframe, we write:\n\nmData$newdata &lt;- c(1, 2, 3, 4, 5, 6)\n\nIf the new variable is the same across all the observations, we can write:\n\nmData$newdata2 &lt;- TRUE\n\nThis also works as long as the number of rows of the dataframe is a multiple of the length of the new variable (if it is not a multiple, we get an error):\n\nmData$newdata3 &lt;- c(\"one\", \"two\")\nmData\n#&gt;   Plot Tree Species Diameter Height newdata newdata2 newdata3\n#&gt; 1    1    2      DF       39   20.5       1     TRUE      one\n#&gt; 2    2    2      WL       48   33.0       2     TRUE      two\n#&gt; 3    2    3      GF       52   30.0       3     TRUE      one\n#&gt; 4    5    3      WC       35   20.7       4     TRUE      two\n#&gt; 5    8    3      WC       37   22.5       5     TRUE      one\n#&gt; 6    8    2      GF       30   20.1       6     TRUE      two",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "getting_started_with_r.html#small-exercises-with-logical-operators-matrices-lists-and-dataframes",
    "href": "getting_started_with_r.html#small-exercises-with-logical-operators-matrices-lists-and-dataframes",
    "title": "\n2  Getting Started with R\n",
    "section": "\n2.13 Small exercises with logical operators, matrices, lists and dataframes",
    "text": "2.13 Small exercises with logical operators, matrices, lists and dataframes\n\n\nDefine a 10x4 matrix mA that holds the values from a vector of the sequence from 1 to 40 that increments by 1.\n\nPrint mA and visually determine if R fills in the matrix by column or by row.\n\nAssign mA to the variable dfA as a dataframe (Try ?as.data.frame).\n\nAssign the column names “one”, “two”, “three” and “four” to the data frame (Try ?colnames).\n\nAssign a new variable called “five” that contains the logical FALSE to the data frame dfA.\n\nVerify that dfA is indeed both considered a list and a data frame using a logical operator along with is.list and is.data.frame.\nIf time permits; remove the column named “two” from the data frame using a logical operator.\n\nSolution\n\nmA &lt;- matrix(1:40, 10, 4)\nmA # it's filled by columns\n#&gt;       [,1] [,2] [,3] [,4]\n#&gt;  [1,]    1   11   21   31\n#&gt;  [2,]    2   12   22   32\n#&gt;  [3,]    3   13   23   33\n#&gt;  [4,]    4   14   24   34\n#&gt;  [5,]    5   15   25   35\n#&gt;  [6,]    6   16   26   36\n#&gt;  [7,]    7   17   27   37\n#&gt;  [8,]    8   18   28   38\n#&gt;  [9,]    9   19   29   39\n#&gt; [10,]   10   20   30   40\ndfA &lt;- as.data.frame(mA) # convert to dataframe\ncolnames(dfA) &lt;- c(\"one\", \"two\", \"three\", \"four\") # add headers\ndfA # view dataframe\n#&gt;    one two three four\n#&gt; 1    1  11    21   31\n#&gt; 2    2  12    22   32\n#&gt; 3    3  13    23   33\n#&gt; 4    4  14    24   34\n#&gt; 5    5  15    25   35\n#&gt; 6    6  16    26   36\n#&gt; 7    7  17    27   37\n#&gt; 8    8  18    28   38\n#&gt; 9    9  19    29   39\n#&gt; 10  10  20    30   40\ndfA$five &lt;- FALSE\ndfA # view the new column\n#&gt;    one two three four  five\n#&gt; 1    1  11    21   31 FALSE\n#&gt; 2    2  12    22   32 FALSE\n#&gt; 3    3  13    23   33 FALSE\n#&gt; 4    4  14    24   34 FALSE\n#&gt; 5    5  15    25   35 FALSE\n#&gt; 6    6  16    26   36 FALSE\n#&gt; 7    7  17    27   37 FALSE\n#&gt; 8    8  18    28   38 FALSE\n#&gt; 9    9  19    29   39 FALSE\n#&gt; 10  10  20    30   40 FALSE\nis.list(dfA) # check if it's a list\n#&gt; [1] TRUE\nis.data.frame(dfA) # check if it's a dataframe\n#&gt; [1] TRUE\ndfA[, !(names(dfA) == \"two\")] # remove the column with the name two\n#&gt;    one three four  five\n#&gt; 1    1    21   31 FALSE\n#&gt; 2    2    22   32 FALSE\n#&gt; 3    3    23   33 FALSE\n#&gt; 4    4    24   34 FALSE\n#&gt; 5    5    25   35 FALSE\n#&gt; 6    6    26   36 FALSE\n#&gt; 7    7    27   37 FALSE\n#&gt; 8    8    28   38 FALSE\n#&gt; 9    9    29   39 FALSE\n#&gt; 10  10    30   40 FALSE",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "getting_started_with_r.html#additional-code-in-r",
    "href": "getting_started_with_r.html#additional-code-in-r",
    "title": "\n2  Getting Started with R\n",
    "section": "\n2.14 Additional code in R",
    "text": "2.14 Additional code in R\n\n\n\n\n\n\nClick to view “Week1_lecture1.R”\n\n\n\n\n\n\n#0.\nrm(list = ls()) # Clear Environment\ncat(\"\\014\")     # Clear console: equivalent of ctrl+L\n\n#1. Manipulations with the directory\n#Check the current directory\ngetwd() \n\n#You can change it by typing a new address with a command setwd(\".../Newdirectory\")\n\n#You can create a new folder in the directory by typing  dir.create(\"NameOFtheFolder\")\n\n#2. Interactions with the directory: Also have a look at the example1.R\nmX &lt;- matrix(0, ncol = 3, nrow = 3) \nsave(mX, file = file.path(getwd(), \"mX.RData\"))\nload(\"mX.RData\") \n\n#Load the matrix from a particular folder\nload(file.path(getwd(),\"mX.RData\"))\n\n#Load the file containing some functions that we would like to use here\nsource(\"example1_fun.R\") \nsource(file.path(getwd(), \"example1_fun.R\")) \n\n#3. some useful functions for strings\nsText_1 &lt;- \"hello\"\nsText_2 &lt;- \"world\"\n\n# Combine together\nsText_3 &lt;- paste(sText_1, sText_2, sep = \" \")\n\n# Manipulate with this line:it allows to extract parts of the expression\nsubstr(sText_3, 7, nchar(sText_3)) \n\n#4. some useful numerical functions\nsqrt(2)\nsin(2)\npi\n\n# Logarithms\n# Natural log\nlog(exp(1)) # the log() function defaults to natural log\n\n# Log with base-10\nlog(100, 10) \n\n# Rounding numbers\nround(pi, 4)\nceiling(pi)\nfloor(pi)\n\n# Setting how many digits you want to have the numbers displayed\noptions(digits = 10)\n\n# generating sequences\nseq(1, 9, by = 0.5)\n2:17 # quicker way of generating a sequence\n17:2\n\n# mod\n14 %% 3 \nvX &lt;- seq(1, 10)\nvX %% 2\n\n#5. NAs and NANs\n# NaN: \"Not a Number\", used for things that cannot be calculated\n# NA: \"Not Available\", values that are unknown or missing\n\n# NaN examples:\n0/0      # is NaN\nsqrt(-1) # is NaN\nInf-Inf  # is NaN\nNaN/NA   # is NaN\n\n#NAs \nvA &lt;- c(1, 2, NA)\n\n# The way NAs affect calculations\nmean(vA) \nmean(vA, na.rm = TRUE) \n\n# Similarly we can do\nvA[!is.na(vA)] \nmean(vA[!is.na(vA)])\n\n# Or equivalently\nmean(vA[which(!is.na(vA))])\n\n##  NaNs\nvB &lt;- c(1, 2, NaN, NA)\n\nis.nan(vB)\nany(is.nan(vB))\nis.na(vB)\n\n#6. Manipulations using logicals\ndX = 2\ndX == 3 # is equal to\n\n# | or:just one of the constraints is true\n((2 == 3) | (2 &gt; 1)) \n\n# & and:all constraints to be true\n((2 == 3) & (2 &gt; 1)) \n\nvX &lt;- c(1, 2, 3)\nvY &lt;- c(4, 5, 6)\n\n# In case of vectors logical operators check element-wise\n(vX &lt; 2 | vY &gt; 3) \n(vX &lt; 2 & vY &gt; 3)\n\n\n#7. List and data frames\n# 7.1 List\n# Create an empty one and put elements in it\nlList1 &lt;- list()\n\nlList1[[1]] &lt;- c(1,2,3)\nlList1[[2]] &lt;- matrix(1:4, ncol = 2)\nlList1[[3]] &lt;- c(TRUE, FALSE, FALSE)\nnames(lList1) &lt;- c(\"vector\", \"matrix\", \"boolean\")\n\nlList1$vector[2]\nlList1$matrix[1, 2]\n\n# Equivalently the list can be created this way\nlList2 &lt;- list(vector = c(1,2,3), matrix = matrix(1:4, ncol = 2), boolean = c(TRUE, FALSE, FALSE))\nlList2\n\nlList2[[2]]\nlList2$matrix\nmX &lt;- lList2$matrix\n\n# 7.2 Data-frame\nmData &lt;- data.frame(Plot = c(1, 2, 2, 5, 8, 8), Tree = c(2, 2, 3, 3, 3, 2),\n                    Species = c(\"DF\", \"WL\", \"GF\", \"WC\", \"WC\", \"GF\"), Diameter = c(39, 48, 52, 35, 37, 30),\n                    Height = c(20.5, 33.0, 30.0, 20.7, 22.5, 20.1))\n\nmX &lt;- matrix(c(1, 2, \"hello\", TRUE), ncol = 2) # a matrix cannot contain both numerical values, characters, and booleans\n\n# accessing the contents of a dataframe is easy\nmData$Plot\nmData[, 2]\nmData[, \"Height\"]",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "basic_programming.html",
    "href": "basic_programming.html",
    "title": "\n3  Basic Programming\n",
    "section": "",
    "text": "3.1 The if statement\nThis lecture introduces a set of basic programming constructs, which are the building blocks of most programs.\nSome of these tools are used by practically all programming languages, for example, conditional execution by if statements, and looped execution by for and while statements.\nNotice that code that seems to be efficient in another language may not be efficient in R. For example, R is known to be quite slow in computing for and while loops. On the other hand, R is fast in doing vector-based operations.\nA program or script is just a list of commands, which are executed one after the other. A program is usually composed by three parts:\nUsually, we write the list of commands in separate files, called scripts, which we can save on our hard drive.\nSuppose we have a program saved as prog.r in the working directory (recall the getwd and setwd functions). In order to run or execute the program use the command:\nIt is often useful to choose the execution of some or other part of a program to depend on a condition. The if function has the form:\nif (logical_expression) {\n  expression_1 #code to run if logical_expression is TRUE\n  ...\n}\nA natural extension of the if command includes an else part:\nif (logical_expression) {\n  expression_1 \n  ...\n} else {\n  expression_2\n  ...\n}\nWhen an if expression is evaluated, if logical_expression is TRUE then the first group of expressions is executed and the second group of expressions is not executed. Conversely, if logical_expression is FALSE then only the second group of expressions is executed.\nExample:\ndX &lt;- rnorm(1) # random number from standard normal distribution\nif (dX &gt; 0) {\n  print(\"x is positive\")\n} else {\n  print(\"x is non-positive\")\n}\n#&gt; [1] \"x is non-positive\"\nNote: be careful with the use of the curly brackets {}!",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Programming</span>"
    ]
  },
  {
    "objectID": "basic_programming.html#the-if-statement",
    "href": "basic_programming.html#the-if-statement",
    "title": "\n3  Basic Programming\n",
    "section": "",
    "text": "3.1.1 Small Exercise\n\nGenerate a random number from a standard Gaussian distribution.\n\nIf the number is negative, then compute the square of it and print the value of x *along with the output, e.g., “the value of x is _ and its square is _” (Hint: use the* catfunction).\n\n\nOtherwise, compute the cube and print the value of x along with the output.\n\n\nSolution:\n\ndX &lt;- rnorm(1) # Generates 1 observation, mean 0 and standard deviation 1\nif (dX &lt; 0) {\n  cat(\"The value of x is \", dX, \" and its square is \", dX^2)\n} else {\n  cat(\"The value of x is \", dX, \" and its cube is \", dX^3)\n}\n#&gt; The value of x is  -0.09977998  and its square is  0.009956044\n\n\n3.1.2 Nested if statements\nNested if statements are constructed using the else if command:\n\nif (logical_expression_1) {\n  expression_1 # code to run if logical_expression_1 is TRUE\n  ...\n} else if (logical_expression_2) {\n  expression_2 # code to run if logical_expression_1 is FALSE and logical_expression_2 is TRUE\n  ...\n} else if (logical_expression_2) {\n  expression_3 # code to run if logical_expression_1 is FALSE and logical_expression_2 is FALSE and logical_expression_3 is TRUE\n  ...\n} else {\n  expression_4 # code to run if all logicals are FALSE\n}\n\nExample:\n\ndX - runif(1) # Random number from the uniform distribution\n#&gt; [1] -0.4349783\nif (dX &lt; 0.5) {\n  print(\"x is less than 0.5\")\n} else if (dX &lt; 0.75) {\n  print(\"x is less than 0.75\")\n} else {\n  print (\"x is greather than or equal to 0.75\")\n}\n#&gt; [1] \"x is less than 0.5\"\n\n\n3.1.3 An extension to the exercise solution above\nExtending the code from above to also include a nested else if such that it includes an action for the event of a non-negative value of x.\n\ndX &lt;- rnorm(1) # random number from the Gaussian distribution\nif (dX &lt; 0) {\n  cat(\"the value of x is \", dX, \" and its square is \", dX^2)\n} else if (dX &gt;= 0) {\n  cat(\"x is non-negative, taking the value \", dX, \" and its square is \", dX^2)\n} # Note the tautological nature of the logical expression in the else if part...\n#&gt; x is non-negative, taking the value  0.4157251  and its square is  0.1728273",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Programming</span>"
    ]
  },
  {
    "objectID": "basic_programming.html#the-for-loop",
    "href": "basic_programming.html#the-for-loop",
    "title": "\n3  Basic Programming\n",
    "section": "\n3.2 The for loop",
    "text": "3.2 The for loop\nThe for command has the following form, where x is a simple variable and vector is a vector.\n\nfor (x in vector) {\n  expression_1\n  ...\n}\n\nWhen executed, the for command executes the group of expressions within the braces {} once for each element of vector:\n\nfor (iX in 1:50) {\n  print(iX)\n}\n\nNote that vector can also be a list.\nExample:\n\nvX &lt;- seq(1, 9, by = 2)\ndSum_x &lt;- 0\nfor (iX in vX) {\n  dSum_x &lt;- dSum_x + iX\n  cat(\"The current loop element is \", iX, \"\\n\")\n  cat(\"The cumulative total is \", dSum_x, \"\\n\")\n}\n#&gt; The current loop element is  1 \n#&gt; The cumulative total is  1 \n#&gt; The current loop element is  3 \n#&gt; The cumulative total is  4 \n#&gt; The current loop element is  5 \n#&gt; The cumulative total is  9 \n#&gt; The current loop element is  7 \n#&gt; The cumulative total is  16 \n#&gt; The current loop element is  9 \n#&gt; The cumulative total is  25\nsum(vX)\n#&gt; [1] 25\ncumsum(vX)\n#&gt; [1]  1  4  9 16 25\n\n\n3.2.1 Exercise: a combination of for loops and if conditions\n\n\nWithin a for loop, that runs from 1 to 100, generate a standard normal random variable.\n\nFor each draw, check whether it is positive or negative.\nCalculate the fraction of the positive and negative realizations.\nStore the simulated variables in a vector\n\nSolution:\n\niPos &lt;- 0\niNeg &lt;- 0 # This integer is technically redundant, as the knowledge of the positive values and the total sum allows us to infer the negative values...\nvSimVals &lt;- vector()\nfor (iX in 1:100) {\n  dX &lt;- rnorm(1)\n  if (dX &gt; 0) {\n    iPos &lt;- iPos + 1\n  } else {\n    iNeg &lt;- iNeg + 1\n  }\n  vSimVals[iX] &lt;- dX\n}\ncat(\"The fraction of positive realizations is: \", iPos, \"/\", (iNeg + iPos), \" or \", iPos / (iNeg + iPos), \"\\n\")\n#&gt; The fraction of positive realizations is:  52 / 100  or  0.52\ncat(\"The first of the simulated values are: \", vSimVals[1:5], \"...\")\n#&gt; The first of the simulated values are:  -1.746912 -0.5711924 -0.8690102 -0.4154249 1.603575 ...",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Programming</span>"
    ]
  },
  {
    "objectID": "basic_programming.html#the-while-loop",
    "href": "basic_programming.html#the-while-loop",
    "title": "\n3  Basic Programming\n",
    "section": "\n3.3 The while loop",
    "text": "3.3 The while loop\nOften, we do not know beforehand how many times we need to go around a loop. That is, each time we go around the loop, we check some condition to see if we are done yet.\nIn this situation, we use a while loop, which has the form:\n\nwhile (logical_expression) {\n  # This should have an impact on logical expression\n  expression_1\n  ...\n}\n\nWhen a while command is executed, logical_expression is evaluted first. If it is true, then the group of expressions in braces {} is executed.\nUntil logical_expression becomes false, the while command executes the group of expressions in braces {}. warning: while loops can run forever, if a stopping criterion has not been specified!\nExample:\nConsider the following example where we use the while loop to compute the square of numbers until 6:\n\niN &lt;- 1\nwhile (iN &lt;= 6) {\n  print(iN ^ 2)\n  iN &lt;- iN + 1\n}\n#&gt; [1] 1\n#&gt; [1] 4\n#&gt; [1] 9\n#&gt; [1] 16\n#&gt; [1] 25\n#&gt; [1] 36\n\n\n3.3.1 Exercise with while loops\n\nGenerate a random number from a Gaussian distribution with mean 100 and variance 100.\n\nStart a while loop that at each iteration divides the result obtained at the previous iteration by two\n\nSet the stopping criterion when the result at the previous iteration is lower than 1.\n\nPrint the number of iterations when the while loop has terminated.\n\n\nCould you accomplish the same with a for-loop? How?\n\n\nSolution:\n\ndCurrent &lt;- rnorm(1, mean = 100, sd = sqrt(100)) \ndIte &lt;- 0\nwhile (dCurrent &gt;= 1) {\n  dCurrent &lt;- dCurrent / 2\n  dIte &lt;- dIte + 1\n}\nprint(dIte)\n#&gt; [1] 7\n\nThe same cannot be accomplished with a for-loop, because the for-loop requires a predetermined number of iterations which is not possible to know beforehand with a random number.\n\n\n\n\n\n\nClick to view an example of nested while and if statements\n\n\n\n\n\n\nWant.2.bake.cake &lt;- FALSE\nbowl &lt;- matrix(0,1,1)\n\nwhile(Want.2.bake.cake == FALSE) {\n\n  Want.2.bake.cake &lt;- (runif(1) &lt; 0.5)\n\n  if (Want.2.bake.cake == TRUE) {\n\n    print(\"You start baking, find the appropriate bowl\")\n    Sys.sleep(2)\n\n    print(\"looking for bowl...\")\n\n    while ((prod(dim(bowl)) != 100 | nrow(bowl) != ncol(bowl))) {\n\n      bowl &lt;- matrix(0,sample(1:100,1), sample(1:100,1))\n\n      if (!(prod(dim(bowl)) != 100 | nrow(bowl) != ncol(bowl))){\n        Sys.sleep(2)\n\n        print(\"Wrong bowl... look for another one..\")\n        print(\"looking for another...\")\n\n      }\n    }\n  }\n}\n\nPlease note that the code is actually wrong! The logical expression in the if condition after sampling of a bowl is only true if the correct bowl is found, not when the wrong bowl is found.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Programming</span>"
    ]
  },
  {
    "objectID": "basic_programming.html#vector-based-programming",
    "href": "basic_programming.html#vector-based-programming",
    "title": "\n3  Basic Programming\n",
    "section": "\n3.4 Vector-based programming",
    "text": "3.4 Vector-based programming\nIt is often necessary to perform an operation upon each of the elements of a vector.\nR is set up such that these programming tasks can be accomplished using vector operations rather than looping.\nUsing vector operations is more efficient computationally as well as more concise literally. For example, consider the while loop example of computing the square of the first 6 numbers from before, now computed using vector based programming.\n\nvX &lt;- (1:6) ^ 2\nvX\n#&gt; [1]  1  4  9 16 25 36\n\n\n3.4.1 Example\nIf we want to find the sum of the first n = 100 squares, we might use a for loop:\n\niN &lt;- 100\ndS &lt;- 100\nfor (i in 1:iN) {\n  dS &lt;- dS + i ^ 2\n}\ndS\n#&gt; [1] 338450\n\nOr use vector-based programming:\n\nsum((1:iN)^2)\n#&gt; [1] 338350\n\nComparing the computational time, we see that vector-based programming is more efficient.\n\niN &lt;- 1e8\nsystem.time({\n  dS &lt;- 0\n  for (i in 1:iN) {\n    dS &lt;- dS + i ^2\n  }\n  dS\n})\n#&gt;   bruger   system forløbet \n#&gt;     2.28     0.00     2.28\nsystem.time({sum((1:iN) ^ 2)})\n#&gt;   bruger   system forløbet \n#&gt;     0.31     0.09     0.40",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Programming</span>"
    ]
  },
  {
    "objectID": "basic_programming.html#load-data-in-r",
    "href": "basic_programming.html#load-data-in-r",
    "title": "\n3  Basic Programming\n",
    "section": "\n3.5 Load data in R",
    "text": "3.5 Load data in R\nR implements functionalities to read from text files like: .txt and .csv. In order to read excel files external libraries are required, such as: excel, gdata, rodbc, XLConnect, xls, xlsReadWrite, xlsx.\nR provides a number of ways to read data from a file: scan, read.table, read.csv. In this course, we will mostly use read.table.\n\nread.table(file, header = FALSE, sep = \"\", dec = \".\", row.names, col.names, na.strings = \"NA\", skip = 0, ...)\n\nThe dots … means that other arguments are available. See help(read.table).\nfile: the name of the file which the data are to be read from. Each row of the table appears as one line of the file. If it does not contain an absolute path, the file name is relative to the current working directory, getwd().\nheader: a logical value indicating whether the file contains the names of the variables as its first line.\nsep: the field separator character. Values on each line of the file are separated by this character. If sep = \"\" (the default), the separator is ‘white space’.\ndec: the character used in the file for decimal points.\nrow.names: a vector of row names. This can be a vector giving the actual row names, or a single number giving the column of the table which contains the row names, or character string giving the name of the table column containing the row name.\ncol.names: a vector of optional names for the variables. The default is to use “V” followed by the column number.\nna.string: a character vector of strings which are to be interpreted as NA values.\nskip: the number of lines of the data file to skip before beginning to read data.\n\n3.5.1 Load MAERSK historical prices\nThe data set with historical prices for MAERSK is contained in the MAERSK-B.CO.csv-file. In the case below, the file is in the folder named ‘data’.\n\nmData &lt;- read.table(file = \"data/MAERSK-B.CO.csv\", sep = \",\", dec = \".\", header = TRUE, row.names = 1, na.strings = \"null\")\nhead(mData)\n#&gt;               Open    High     Low   Close Adj.Close Volume\n#&gt; 2000-02-01 7640.00 7666.67 7333.33 7466.67  2267.703   4590\n#&gt; 2000-02-02 7466.67 7666.67 7400.00 7566.67  2298.074   5640\n#&gt; 2000-02-03 7741.07 7800.00 7659.00 7800.00  2368.939   3090\n#&gt; 2000-02-04 7800.00 7800.00 7525.80 7600.00  2308.197   1185\n#&gt; 2000-02-07 7660.00 7666.67 7566.67 7600.00  2308.197   1035\n#&gt; 2000-02-08 7600.00 8200.00 7600.00 8110.80  2463.332  12990\ndim(mData)\n#&gt; [1] 4571    6\n\n\n3.5.2 Descriptive statistics of the MAERSK log-returns\n\nany(is.na(mData[, \"Adj.Close\"])) # Search for NAs\n#&gt; [1] TRUE\nlength(which(is.na(mData[, \"Adj.Close\"]))) # How many NAs?\n#&gt; [1] 59\n# Compute financial log-returns omitting the NAs:\nvY &lt;- diff(log(na.omit(mData[, \"Adj.Close\"])))\n# Compute descriptive statistics\nc(\"mean\" = mean(vY), \"sd\" = sd(vY), \"median\" = median(vY))\n#&gt;         mean           sd       median \n#&gt; 0.0003439318 0.0220065696 0.0000000000\n\nOn a slightly more advanced note, very quick and efficient summary statistics can be obtained with summary().\n\n3.5.3 Output to a file\nR provides a number of commands for writing output to a file.\nWe will generally use write.table for writing numeric values and cat for writing text, or a combination of numeric and character values.\nThe command write.table has the form:\n\nwrite.table(x, file = \"\", quote = TRUE, sep = \" \", na = \"NA\", dec = \".\", row.names = TRUE, col.names = TRUE, ...)\n\nx: the object to be written, preferably a matrix or dataframe.\nfile: character string naming a file.\nquote: a logical value (TRUE or FALSE). If TRUE, any character columns will be surrounded by double quotes (““).\nsep and dec work as in read.table.\nna: the string to use for missing values in the data.\nrow.names: either a logical value indicating whether the row names of x are to be written along with x, or a character vector of row names to be written.\ncol.names: either a logical value indicating whether the column names of x are to be written along with x, or a character vector of column names to be written.\nExample:\n\nwrite.table(vY, file = \"MAERSK_returns.txt\", row.names = FALSE, col.names = FALSE, dec = \".\")",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Programming</span>"
    ]
  },
  {
    "objectID": "basic_programming.html#plotting",
    "href": "basic_programming.html#plotting",
    "title": "\n3  Basic Programming\n",
    "section": "\n3.6 Plotting",
    "text": "3.6 Plotting\nR provides a huge number of plotting routines. The generic function for plotting of R objects is plot.\nExternal pakcages can also be used for plotting (e.g, ggplot2, see http://ggplot2.org/ if you’re interested), however, we will focus on the built-in function plot in this course.\nR generally has good graphical capabilities. Get an initial overview of these capabilities by typing demo(graphics) in your R console.\nplot can be used to represent a pair of data points xand y, e.g.:\n\nvX &lt;- seq(-10, 10, 0.1)\nvY &lt;- sin(vX)\nplot(vX, vY, type = \"l\")\n\n\n\n\n\n\n\nOr a single vector of points, e.g.:\n\nvZ &lt;- runif(10)\nplot(vZ)\n\n\n\n\n\n\n\n\n3.6.1 Basic arguments for plot\nThe plot command offerse a wide variety of options for customizing the graphic. Some of these are:\n\ntype determines the type of the plot, e.g., type = \"p\" for points (the default), type = \"l\" for lines, type = \"o\" for both lines and points, type = \"s\" for a step function, type = \"n\" for no plotting, etc. etc…\nxlim = c(a,b) and ylim = c(a,b) will set the lower and upper limits of the x- and y-axis to be a and b, respectively.\nmain = \"Plot title goes here\" provides the plot title.\nxlab = \"X axis label goes here\" provides the label for the x-axis.\nylab = \"Y axis label goes here\" provides the label for the y-axis.\ncol = \"black\" colour for lines and points. Type colours() for a list of colours known to R (there are a lot!).\nlty determines the line type, e.g., 0=blank, 1=solid (default), 2=dashed, 3=dotted, etc…\nlwd determines the line width.\npch = k determines the shape of points with k taking a value from 1 to 25.\nTo add points (x[1], y[1]), (x[2], y[2]), ... to the current plot, use points(x, y).\nTo add lines instead use lines(x, y).\nTo add text use text(x, y, \"test\").\nVertical and horizontal lines can be drawn using abline(v = expos) and abline(h = ypos).\nThe functions points, lines, abline also accept the col, lty, lwd arguments.\nThe command par is used to set many different parametrs that control how graphics are produced (see help(par)). E.g., the command par(mfrow = c(nr, nc)) creates a grid of plots with nr rows and nc columns, which is filled row by row. par(mfcol = c(nr, nc)) is similar but fills the plots column by column.\n\n3.6.2 Example:\n\npar(mfrow = c(1, 2))\nx &lt;- seq(0, 5, by = 0.01)\ny.upper &lt;- 2 * sqrt(x)\ny.lower &lt;- -2 * sqrt(x)\ny.max &lt;- max(y.upper)\ny.min &lt;- min(y.lower)\nplot(c(-2, 5), c(y.min, y.max), type = \"n\", xlab = \"x\", ylab = \"y\", main = \"Parabola Graph\")\nlines(x, y.upper, lwd = 2, col = \"deepskyblue\")\nlines(x, y.lower, lwd = 2, col = \"deepskyblue\")\nabline(v = -1, lty = 4)\npoints(1, 0, col = \"tomato\", pch = 5)\ntext(1, 0, \"focus (1,0)\", pos = 4)\n\nx = rnorm(10000)\nhist(x, col = \"lavender\", main = \"Normal Histogram\")\n\n\n\n\n\n\n\n\n3.6.3 Exercise\n\n\nGenerate a 1000 standard uniforms sort (in ascending order) your pseudo data. (Hint: ?runif and ?sort).\nPlot the random uniforms using the plot-function (Add any arguments you find necessary, i.e., legend or axis text)\n\nThen save the plot as a .pdf in your working directory (Check your WD with getwd()) using the following wrapper:\n\n\n\npdf(\"./myplot.pdf\") # Your plot function with input\ndev.off()\n\nSolution:\n\nx &lt;- sort(runif(1000), decreasing = FALSE)\nplot(x, main = \"Uniform sample\", xlab = \"Ordering\", ylab = \"Value\") # A simple scatterplot\npoints(x, cex = .5, col = \"dark red\") # Colors",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Programming</span>"
    ]
  },
  {
    "objectID": "basic_programming.html#additional-code-in-r",
    "href": "basic_programming.html#additional-code-in-r",
    "title": "\n3  Basic Programming\n",
    "section": "\n3.7 Additional code in R",
    "text": "3.7 Additional code in R\n\n\n\n\n\n\nClick to view “Week1_lecture1.R”\n\n\n\n\n\n\n#0.1. clear the workspace\nrm(list = ls())\n\n#0.2. set the working directory\nsetwd(\"...Exercises\")\n\n#1. If statement exercise\n# Simulate a standard random normal variable\ndX &lt;- rnorm(1)\n\n# Plain if\nif (dX &lt; 0) {\ndX_2 &lt;- dX^2\nprint(dX_2)\n} else {\ndX_3 &lt;- dX^3\nprint(dX_3)\n}\n\n# Simulate a standard uniform random variable\ndX &lt;- runif(1) \n\n# Nested if\nif (dX &lt; 0.5) {\n  print(\"x is less than 0.5\")\n} else if (dX &lt; 0.75) {\n  print(\"x is less than 0.75\")\n} else {\n  print(\"x is greater than or equal to 0.75\")\n}\n\n#2. While loop \niN &lt;- 1\nwhile(iN &lt;= 6) {\n  print(iN^2)\n  iN &lt;- iN + 1\n}\n\n\ndX &lt;- rnorm(1, mean = 100, sd = sqrt(100))\niN &lt;-0 \n\nwhile (dX &gt; 1) {\n  dX &lt;- dX/2\n  iN &lt;- iN + 1\n}\n\nprint(iN)\n\n\n#3.Load  data\n#Comma delimited: sep=\",\"\n#   dec=\".\": character used in the file for decimal points\n#   header=TRUE :the names of the variables as its first line\n#   row.names=1: Column containing row names na.strings = \"null\"\n# na.strings = \"null\": String \"null\" in the data to NA\nmData &lt;- read.table(file = \"C:/Users/au698611/OneDrive - Aarhus Universitet/Desktop/Lectures/PQE 2022/Lecture 3/Exercises/MAERSK-B.CO.csv\",sep=\",\",dec=\".\", header=TRUE, row.names=1,na.strings = \"null\")\n\n# Display part of the data\nhead(mData)\n\n# Dimensions of the data\ndim(mData)\n\n# Row names:vector of date entries\nrow.names(mData)\n\n#Search for NAs \nany(is.na(mData[,\"Adj.Close\"]))\n\n#How many?\nlength(which(is.na(mData[,\"Adj.Close\"])))\n\n#4. Construct a new data-set\n#Compute financial log-returns omitting the NAs: log(P_t)-log(P_{t-1})\nvY &lt;- diff(log(na.omit(mData[,\"Adj.Close\"])))\n\n\n#Compute descriptive statistics\nc(\"mean\"=mean(vY), \"sd\"=sd(vY), \"median\"=median(vY))\n\nR.names &lt;- rownames(mData)[which(mData[,\"Adj.Close\"]!=\"null\")]\nR.names &lt;- R.names[-1] #remove the first element\n\n#Write the output to a file and check it\nwrite.table(vY, file &lt;- \"C:/Users/au698611/OneDrive - Aarhus Universitet/Desktop/Lectures/PQE 2022/Lecture 3/Exercises/MAERSK_returns.csv\",sep=\",\", row.names = R.names, col.names = c(\"time,Log return\"), dec = \".\")\n\n#5.Plots\n# A sine function plot\nvX &lt;- seq(-10, 10, 0.1)\nvY &lt;- sin(vX)\nplot(vX, vY, type = \"l\")\n\n#Plot of  random uniform variables\nvZ = runif(10)\nplot(vZ)\n\n#Create a window for two plots\npar(mfrow=c(1,2))\n\n# Parabola: type(\"n\") just prepares a frame\nx &lt;- seq(0,5,by=0.01)\ny.upper &lt;- 2*sqrt(x)\ny.lower &lt;- -2*sqrt(x)\ny.max &lt;- max(y.upper)\ny.min &lt;- min(y.lower)\nplot(c(-2,5), c(y.min,y.max), type=\"n\", xlab = \"x\", ylab = \"y\", main = \"Parabola Graph\")\nlines(x, y.upper, lwd = 2, col = \"deepskyblue\")\nlines(x, y.lower, lwd = 2, col = \"deepskyblue\")\nabline(v=-1, lty = 3)  #v: specifies x-value for vertical line, \n#lty- different line types:1-solid line,3-dotted, etc.\npoints(1,0, col = \"tomato\", pch = 5) #(1,0) stands for (x,y) coordinates. pch is a symbol-5 is diamond\n#Add a text: (1,0) stands for (x,y) coordinates\n# 1, 2, 3 and 4  respectively indicate positions below, to the left of, above and to the right of  (x,y) coordinates.\ntext(1,0,\"focus (1,0)\",pos=4) #(1,0) stands for (x,y).\n\n#Histogram of standard normal distribution\nx = rnorm(10000)\nhist(x, col = \"lavender\", main = \"Normal Histogram\")",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Programming</span>"
    ]
  },
  {
    "objectID": "programming_w_functions.html",
    "href": "programming_w_functions.html",
    "title": "\n4  Programming with Functions\n",
    "section": "",
    "text": "4.1 Definition of functions in R\nWe have already seen and used a number of different pre-defined functions, such as:\nA function is a piece of code that takes input (in the form of variables), performs a specified action using the input, and produces output (variables, or a table of numbers, or a graph, etc.).\nA function has the general form:\nname &lt;- function(argument_1, argument_2, ...) {\n  expression_1\n  expression_2\n  #&lt;some other expression&gt;\n  return(output)\n}",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Programming with Functions</span>"
    ]
  },
  {
    "objectID": "programming_w_functions.html#definition-of-functions-in-r",
    "href": "programming_w_functions.html#definition-of-functions-in-r",
    "title": "\n4  Programming with Functions\n",
    "section": "",
    "text": "argument_1, argument_2, etc., are the names of variables (the input).\nexpression_1, expression_2, and output are all regular R expressions.\nname is the name of the function.\n\n\n4.1.1 Running a Function\nTo call or run a function, we type (as an example):\n\nname(x1, x2)\n\nThe value of this expression is the value of the expression output.\nTo calculate the value of output, the function first copies the value of x1 to argument_1, the value of x2 to argument_2, and so on. The arguments have then been passed to the function and act as variables within the function.\nNext, the function evaluates the grouped expressions contained in the braces {}, and the value of the expression output is returned as the value of the function.\n\n4.1.2 Examples\nExample 1\nConsider the following example of a very simple function that computes the value of x to the power of y:\n\npower_fct &lt;- function(vX, power) {\n  return(vX ^ power)\n}\n\nThe function is then carried out as:\n\npower_fct(vX = 2, power = 3)\n#&gt; [1] 8\n\nExample 2\nConsider the following example of a function that swaps the values of the first and second elements of a vector:\n\nswap &lt;- function(vZ) {\n  dTemp &lt;- vZ[2]\n  vZ[2] &lt;- vZ[1]\n  vZ[1] &lt;- dTemp\n  return(vZ)\n}\n\nThe function is then carried out as:\n\nswap(vZ = c(7, 8, 9))\n#&gt; [1] 8 7 9\n\n\n4.1.3 Notes on Functions\nNote that some functions have no arguments, e.g., getwd(). If there is no return(output) statements then the value returned by the function is the value of the last expression in the braces.\n\n4.1.4 Small Exercises\n\nWrite a function that returns the sum of the first N natural numbers.\n\n\n## There are multiple solutions to this:\n# The efficient\nfEfficientSum &lt;- function(iN) {\n  # Input validation: Check if n is a non-negative integer\n  if (!is.numeric(iN) || iN &lt; 0 || iN %% 1 != 0) {\n    stop(\"Input must be a non-negative integer.\")\n  }\n  return(iN * (iN + 1) / 2)\n}\n# The for loop\nfForSum &lt;- function(iN) {\n  sum &lt;- 0\n  for (iIndex in 1:iN) {\n    sum = sum + iIndex\n  }\n  return(sum)\n}\n# The recursive\nfRecursiveSum &lt;- function(iN) {\n  if (iN &lt;= 0) {\n    return(iN)\n  } else {\n    return(iN + fRecursiveSum(iN - 1))\n  }\n}\nfEfficientSum(5)\n#&gt; [1] 15\nfForSum(5)\n#&gt; [1] 15\nfRecursiveSum(5)\n#&gt; [1] 15\n\n\nCreate a function that removes missing values from its vector input if it contains any.\n\n\nfRemoveMissing &lt;- function(vInput) {\n  if (any(is.na(vInput)) == TRUE) {\n    return(na.omit(vInput))\n  } else {\n    return(vInput)\n  }\n}\nvVector &lt;- c(1, NA, 2)\nvVector &lt;- fRemoveMissing(vVector)\n#&gt; 1 2\n\n\n4.1.5 Example: roots of a quadratic function\nAssume we want to find the roots of: \\[\nax^2 + bx + c = 0\n\\] The analytic solution is: \\[\n(x_1, x_2) = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n\\] The associated code can be written as a function:\n\nrootfinder &lt;- function(dA, dB, dC) {\n  vOut = numeric(2)\n  vOut[1] = (-dB + sqrt(dB ^ 2 - 4 * dA * dC)) / (2 * dA)\n  vOut[2] = (-dB - sqrt(dB ^ 2 - 4 * dA * dC)) / (2 * dA)\n  return(vOut)\n}\n\nBut there are some problems related to the efficiency of the code. We are computing the algebraic operations too many times, and we are not exploiting vector-operations.\n\nWhat if \\(a=b=c=0\\)?\nWhat if \\(a=b=0\\)?\nWhat if \\(a=0\\)?\nWhat if \\(b^2-4ac=0\\)?\nWhat is \\(b^2-4ac&lt;0\\)?\n\nReformulate the analytical solution as:\n\\[\n(x_1, x_2) =\n\\begin{cases}\n\\mathbb{R}, & \\text{if } a=b=c=0 \\\\\n\\emptyset, & \\text{if } a=b=0 \\wedge c \\ne 0 \\\\\n-\\frac{c}{b}, & \\text{if } a=0 \\wedge b \\ne 0 \\wedge c \\ne 0 \\\\\n\\frac{-b \\pm \\sqrt{\\lvert\\Delta\\rvert} \\sqrt{sgn(\\Delta)}}{2a}, & \\text{otherwise}\n\\end{cases}\n\\]\nWhere \\(\\Delta = b^2 - 4ac\\). Note that:\n\\[\n\\sqrt{sgn(\\Delta)} =\n\\begin{cases}\n1, \\text{if } \\Delta \\ge 0 \\\\\ni, \\text{if } \\Delta &lt; 0\n\\end{cases}\n\\]\nThus, we modify our function:\n\nrootfinder &lt;- function(dA, dB, dC) {\n  if (dA == 0 && dB == 0 && dC == 0) {\n    vRoots &lt;- Inf\n  } else if (dA == 0 && dB == 0) {\n    vRoots &lt;- NULL\n  } else if (dA == 0) {\n    vRoots &lt;- -dC / dB\n  } else {\n    # calculate the discriminant\n    dDelta &lt;- dB^2 - 4 * dA * dC\n    if (dDelta &gt; 0) {\n      vRoots &lt;- (-dB + c(1, -1) * sqrt(dDelta)) / (2 * dA)\n    } else if (dDelta == 0) {\n      vRoots &lt;- rep(-dB / (2 * dA), 2)\n    } else {\n      di &lt;- complex(1, 0, 1)\n      vRoots &lt;- (-dB + c(1, -1) * sqrt(-dDelta) * di) / (2 * dA)\n    }\n  }\n  return(vRoots)\n}\n\nSuppose we have saved the function rootfinder in the script rootfinder.R which is located int he script folder inside our working directory.\nTo use the function, we first load it (using source or by copying and pasting into R), then call it, supplying suitable arguments.\n\n# source(\"./scripts/rootfinder.r)\nrootfinder(dA = 1, dB = 1, dC = 0)\n#&gt; [1]  0 -1\nrootfinder(dA = 1, dB = 0, dC = -1)\n#&gt; [1]  1 -1\nrootfinder(dA = 1, dB = -2, dC = 1)\n#&gt; [1] 1 1\nrootfinder(dA = 1, dB = 1, dC = 1)\n#&gt; [1] -0.5+0.8660254i -0.5-0.8660254i\n\n\n4.1.6 Advantages of coding with functions\nOnce a function is loaded, it can be used again and again without having to reload it.\nUser-defined functions can be used in the same way as predefined functions are used in R. In particular, they can be used within other functions.\nThe use of functions allows you to break down a programming task into smaller, logical units.\nLarge programs are typically made up of a number of smaller functions, each of which does a simple, well-defined task.\n\n4.1.7 Variables inside a function\nThe arguments and variables that are defined within a function exist only within that function.\nIf you define and use a variable x inside a function, it does not exist outside the function.\n\ntest &lt;- function(x) {\n  y &lt;- x + 1\n  return(y)\n}\ntest(1)\nx # This will return an error\ny # This will return an error\n\n\n4.1.8 The scope of a variable\nIf variables with the same name exist inside and outside a function, then they are separate and do not interact at all.\nYou can think of a function as a separate environment that communicates with the outside world only through the values of its arguments and its output expression.\nThat part of a program in which a variable is defined is called its scope. Restricting the scope of variables within a function provides an assurance that calling the function will not modify variables outside the function, except by assigning the returned value.\nBeware, however, the scope of a variable is not symmetric!\nVariables defined inside a fucntion cannot be seen outside, but variables defined outside the function can be seen inside the function, provided there is not a variable with the same name defined inside.\nConsider for example:\n\ntest2 &lt;- function(x) {\n  y &lt;- x + z\n  return(y)\n}\nz &lt;- 1\ntest2(1)\n#&gt; [1] 2\nz &lt;- 2\ntest2(1)\n#&gt; [1] 3\n\nBest practice is always to pass variables to be used inside the function as additional arguments:\n\ntest2 &lt;- function(x, z) {\n  y &lt;- x + z\n  return(y)\n}",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Programming with Functions</span>"
    ]
  },
  {
    "objectID": "programming_w_functions.html#arguments-of-a-function",
    "href": "programming_w_functions.html#arguments-of-a-function",
    "title": "\n4  Programming with Functions\n",
    "section": "\n4.2 Arguments of a function",
    "text": "4.2 Arguments of a function\nThe arguments of a function can be mandatory or optional depending on the function specification.\nThe arguments of an existing function can be obtained by calling the formals function:\n\nformals(rootfinder)\n#&gt; $dA\n#&gt; \n#&gt; \n#&gt; $dB\n#&gt; \n#&gt; \n#&gt; $dC\n\n\n4.2.1 Default arguments\nIn order to simplify calling functions, some arguments may be assigned default values. Default values are used in the case where the argument is not provided in the call to the function:\n\ntest3 &lt;- function(x = 1) {\n  return(x)\n}\ntest3(2)\n#&gt; [1] 2\ntest3()\n#&gt; [1] 1\n\nCalling formals on a function with default arguments, the value of the default arguments will be stated as well\n\nformals(matrix)\n#&gt; $data\n#&gt; [1] NA\n#&gt; \n#&gt; $nrow\n#&gt; [1] 1\n#&gt; \n#&gt; $ncol\n#&gt; [1] 1\n#&gt; \n#&gt; $byrow\n#&gt; [1] FALSE\n#&gt; \n#&gt; $dimnames\n#&gt; NULL\nmatrix()\n#&gt;      [,1]\n#&gt; [1,]   NA\n\nIf an argument is omitted in a function call, R automatically assigns arguments to variables from the left and uses the default values for the unassigned arguments:\n\ntest4 &lt;- function(x = 1, y = 1, z = 1) {\n  return(x * 100 + y * 10 + z)\n}\ntest4(2, 2)\n#&gt; [1] 221\ntest4(y = 2, z = 2)\n#&gt; [1] 122\n\nIn general, naming the arguments in the function call is good practice, because it increases the readability and eliminates one potential source of errors.\n\n4.2.2 Default arguments: Partial matching\nSometimes you will want to define arguments so that they can take only a small number of different values, and the function will stop informatively if an inappropriate value is passed.\nWhen writing the function, we include a vector of the permissable values for any such argument, and then check them using the match.arg function. For example:\n\nMyFunc &lt;- function(vX, method = c(\"add\", \"multiply\")) {\n  method &lt;- match.arg(method)\n  if (method == \"add\") {\n    return(sum(vX))\n  } else {\n    return(prod(vX))\n  }\n}\nMyFunc(c(1, 2, 3, 4), method = \"multiply\")\n#&gt; [1] 24\nMyFunc(c(1, 2, 3, 4), method = \"add\")\n#&gt; [1] 10\n\nThe R function ar, for example, which fits an autoregressive time series model to data, takes (among others) the argument specifying the method by which the model is estimated.\n\nmethod = c(\"yule-walker\", \"burg\", \"ols\", \"mle\", \"yw\")",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Programming with Functions</span>"
    ]
  },
  {
    "objectID": "programming_w_functions.html#ellipsis",
    "href": "programming_w_functions.html#ellipsis",
    "title": "\n4  Programming with Functions\n",
    "section": "\n4.3 Ellipsis",
    "text": "4.3 Ellipsis\nR provides a very useful means of passing arguments, unaltered, from the function that is being called to the functions that are called within it.\nThese arguments do not need to be named explicitly in the outer function, hence providing great flexibility.\nThe use this facility, you need to include … in your argument list. These three dots (an ellipsis) act as a placeholder for any extra arguments given to the function.\nConsider for example the function ‘square of the mean’:\n\nSquareMean &lt;- function(vX, ...) {\n  dSM &lt;- mean(vX, ...)^2\n  return(dSM)\n}\nSquareMean(c(1, 3, 5, NA))\n#&gt; [1] NA\nSquareMean(c(1, 3, 5, NA), na.rm = TRUE)\n#&gt; [1] 9",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Programming with Functions</span>"
    ]
  },
  {
    "objectID": "programming_w_functions.html#vector-based-programming-using-functions",
    "href": "programming_w_functions.html#vector-based-programming-using-functions",
    "title": "\n4  Programming with Functions\n",
    "section": "\n4.4 Vector-based programming using functions",
    "text": "4.4 Vector-based programming using functions\nMany R functions are vectorized, meaning that given vector input the function acts on each element separately, and a vector output is return, e.g.:\n\nvX &lt;- c(1, NA, 3, 8)\nis.na(vX)\n#&gt; [1] FALSE  TRUE FALSE FALSE\n\nThis is a very powerful aspect of R that allows for compact, efficient, and readable code. Moreover, for many R functions, applying the function to a vector is much faster than if we were to write a loop to apply it to each element one at a time.\nTo further facilitate vector-based programming, R provides a family of powerful and flexible functions that make it easier for user-defined functions to handle vector inputs. These belong to the apply family of functions.\nHere we cover, apply, sapply, lapply, and mapply. Other functions belonging to this family are: tapply, vapply, and eapply, which require a bit of advanced R programming knowledge.\n\n4.4.1 sapply\n\nThe effect of sapply(vX, FUN) is to apply the function FUN to every element of vector vX. That is, sapply returns a vector whose i-th element is the value of the expression FUN(vX[i]).\nConsider the function, f, “sum of all integers lower than X”:\n\nf &lt;- function(dX) {\n  if (dX &lt; 0) {\n    stop(\"dX needs to be positive.\")\n  }\n  dSum = 0.0\n  iC = 0\n  while (iC &lt;= dX) {\n    dSum = dSum + iC\n    iC = iC + 1\n  }\n  return(dSum)\n}\n\nSuppose we want to apply the function f to the following vector of observations vX = c(1, 4, 9.478, 6, 75, 0.48). We can of course write a for loop:\n\nvX &lt;- c(1, 4, 9.478, 6, 75, 0.48)\nvSum &lt;- numeric(length(vX))\nfor (i in 1:length(vSum)) {\n  vSum[i] &lt;- f(vX[i])\n}\nvSum\n#&gt; [1]    1   10   45   21 2850    0\n\nOr we can use the sapply function:\n\nvSum &lt;- sapply(vX, f)\nvSum\n#&gt; [1]    1   10   45   21 2850    0\n\nNote that R performs a loop over the elements of vX, so execution of this code is not faster than execution of an equivalent loop - it is just faster to type.\nIf FUN has arguments other than X[i], then they can be included using the three dots (an ellipsis) as shown above. That is, sapply(X, FUN, ...) returns FUN(X[i]; ...) as the i-th element.\n\ng &lt;- function(dX, iN) {\n  if (dX &lt; 0) {\n    stop(\"dX needs to be positive.\")\n  }\n  dSum &lt;- 0.0\n  iC &lt;- 0\n  while (iC &lt;= dX) {\n    dSum &lt;- dSum + iC^iN\n    iC &lt;- iC + 1\n  }\n  return(dSum)\n}\nsapply(vX, g, iN = 2)\n#&gt; [1]      1     30    285     91 143450      0\n\nlapply does the same as sapply but always returns a list.\n\n4.4.2 sapply and lapply\n\nThat is, the arguments … are passed directly from sapply to FUN, thus allowing you to use a function with more than one argument.\nNote that the values of the arguments … are the same for each of the different arguments of vx. In order to vectorize over more than one argument (over both vX and …) we need to use mapply.\nlapply does the same as sapply, but always returns a list.\n\n4.4.3 mapply\n\nmapply is a multivariate version of sapply and can be used to call a function FUN over vectorized arguments one index at a time. In other words, the function is first called over elements at index 1 of all vectors (or lists), it is then called over all elements at index 2 and so on.\nConsider for example the function rep which takes arguments: x and times. mapply allows you to write:\n\nmapply(rep, times = c(4, 3), x = c(2, 4))\n#&gt; [[1]]\n#&gt; [1] 2 2 2 2\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 4 4 4\n\nOr using mapply on our power_fct(vX, power):\n\nmapply(power_fct, vX = 1:4, power = c(2, 2, 3, 3))\n#&gt; [1]  1  4 27 64\n\n\n4.4.4 apply\n\nIf you wish to apply a function that takes a vector argument to each of the rows (or columns) of a matrix (or array), then use the function apply. Its formula is:\n\napply(X, MARGIN, FUN, ...)\n\nWhere:\n\nX is an array.\nMARGIN is the index/indices of the array to which apply FUN.\nFUN is the function to apply.\n… are extra arguments for FUN.\n\nNote that MARGIN can be a vector of indices. If X is a matrix (an array of 2 dimensions), then MARGINS = 1 indicates rows, while MARGINS = 2 indicates columns.\nExample with apply\nTo compute the cumulative sum over the columns of a matrix mX we write:\n\nmX &lt;- matrix(1:16, 4, 4)\nmX\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    1    5    9   13\n#&gt; [2,]    2    6   10   14\n#&gt; [3,]    3    7   11   15\n#&gt; [4,]    4    8   12   16\napply(mX, 2, cumsum)\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    1    5    9   13\n#&gt; [2,]    3   11   19   27\n#&gt; [3,]    6   18   30   42\n#&gt; [4,]   10   26   42   58\n\n\n4.4.5 Exercise\n\nDefine the 3 x 4 matrix:  \\[\nmA =\n\\begin{bmatrix}\n1 & 10 & 7 & 4 \\\\\n8 & 5 & 2 & 11 \\\\\n6 & 3 & 9 & 12\n\\end{bmatrix}\n\\]\nUse the apply function to compute the row sums and the column sums of your matrix.\nUse the apply function to sort the columns of mA in a decreasing order and the rows of mA in an increasing order.\nUse the sapply function to find sine of all elements of the first row of mA and cosine of all elements of the second column of mA. Is this the easiest way to find the sine and cosine of a row/column of mA?\n\nSolution\n\nmA &lt;- matrix(c(1, 10, 7, 4, 8, 5, 2, 11, 6, 3, 9, 12), 3, 4, byrow = TRUE)\nmA\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    1   10    7    4\n#&gt; [2,]    8    5    2   11\n#&gt; [3,]    6    3    9   12\nt(apply(mA, 1, cumsum)) # compute the row sums\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    1   11   18   22\n#&gt; [2,]    8   13   15   26\n#&gt; [3,]    6    9   18   30\napply(mA, 2, cumsum) # compute the column sums\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    1   10    7    4\n#&gt; [2,]    9   15    9   15\n#&gt; [3,]   15   18   18   27\napply(mA, 2, sort, decreasing = TRUE) # sort columns in decreasing order\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    8   10    9   12\n#&gt; [2,]    6    5    7   11\n#&gt; [3,]    1    3    2    4\nt(apply(mA, 1, sort, decreasing = FALSE)) # sort rows in increasing order\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    1    4    7   10\n#&gt; [2,]    2    5    8   11\n#&gt; [3,]    3    6    9   12\nsapply(mA[1, ], sin) # sine of first row using sapply\n#&gt; [1]  0.8414710 -0.5440211  0.6569866 -0.7568025\nsapply(mA[, 2], cos) # # cosine of second column using sapply\n#&gt; [1] -0.8390715  0.2836622 -0.9899925\n# the easier way is to do 'sin(mA[, 1])'\n\n\n4.4.6 Recursive programming\nRecursive programming is a powerful programming technique, made possible by functions. A recursive program is simply one that calls itself. This is useful because many algorithms are recursive in nature.\nConsider for example the evaluation of \\(n!\\), i.e., the factorial of the non-negative integer \\(n\\). We know that \\(n! = n(n-1)!\\), such that we can write a function like:\n\nmyfactorial &lt;- function(iN) {\n  if ((iN == 0) | (iN == 1)) {\n    return(1)\n  } else {\n    return(iN * myfactorial(iN - 1))\n  }\n}\nmyfactorial(5)\n#&gt; [1] 120\n\n\n4.4.7 Multiple outputs\nA function can generate multiple output values.\nExample: parameter estimates, their standard errors, and the optimized log-likelihood value.\nA strategy in R is to create a list with all your outputs and return it as a single output of a function. For example:\n\nlOut = list(Par = vParam, SE = vStdErr, LLK = dLogLik)\nreturn(lOut)",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Programming with Functions</span>"
    ]
  },
  {
    "objectID": "programming_w_functions.html#comments",
    "href": "programming_w_functions.html#comments",
    "title": "\n4  Programming with Functions\n",
    "section": "\n4.5 Comments",
    "text": "4.5 Comments\nRemember, you often want to re-use a code. It is often hard to read code that someone else has written, or code that you have written yourself months ago. Using sensible function and variable names, and Hungarian notation helps… somewhat.\nBut also: comment your code!\nIn R, # signals a comment to the end of the line:\n\nvBeta &lt;- c(0, 1, -5) # Initial values\n\nIt is best practice to declare what a script does at its beginning. Furthermore, at the start of every function, put something like:\n\n##\n## FunctionName(Inputs)\n##\n## Purpose:\n##  Description of what the function does\n##\n## Input:\n##  List of inputs, describing what they represent\n##\n## Output:\n##  List of outputs, describing what they represent\n##\n## Return value:\n##  List of return values, describing what they represent\n##\n\nIn RStudio, you can use the Ctrl+Shift+C shortcut to comment multiple lines at once.\nIn addition, add a general description of the program at the top of the .R file. This should include your name and the date for future reference. It may seem like a lot of work, but it will par off in the end. So: comment a lot!\n\n4.5.1 Exercise\n\n\nDefine a function, f, that calculates the output of the following recursive algorithm, where \\(n \\ge 0\\) is an integer.\n\n\nIf \\(n&lt;2\\), return \\(n\\).\n\nIf \\(n \\ge 2\\), return \\(f(n)=f(n-1)+f(n-2)\\)\n\n\n\nRun the function\n\nNow pass the function in sapply along with the additional vector of integers, e.g., (1:20), do you recognize the pattern?\n\nBonus: Comment the code\n\nSolution\n\nf &lt;- function(iN) {\n  if (iN &lt; 2) { # stopping criterion\n    return(iN)\n  } else {\n    return(f(iN - 1) + f(iN - 2)) # recursive part\n  }\n}\nsapply(1:20, f) # apply function f to a vector\n#&gt;  [1]    1    1    2    3    5    8   13   21   34   55   89  144  233  377  610\n#&gt; [16]  987 1597 2584 4181 6765\n# regarding the output: it's the fibonacci series",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Programming with Functions</span>"
    ]
  },
  {
    "objectID": "efficiency_and_parallel_computing.html",
    "href": "efficiency_and_parallel_computing.html",
    "title": "\n5  Program Efficiency and Parallel Computing\n",
    "section": "",
    "text": "5.1 Good practice for programming",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Program Efficiency and Parallel Computing</span>"
    ]
  },
  {
    "objectID": "efficiency_and_parallel_computing.html#good-practice-for-programming",
    "href": "efficiency_and_parallel_computing.html#good-practice-for-programming",
    "title": "\n5  Program Efficiency and Parallel Computing\n",
    "section": "",
    "text": "5.1.1 Programming in theory - plan ahead\n\nResearch question: What do I want to know?\nData: What inputs do I have?\nOutput: What kind of output do I expect/need?\nModelling:\n\nWhat is the structure of the problem?\nCan I write it down in equations?\n\n\nEstimation: What procedure for estimation is needed (OLS, ML, GMM, etc)?\n\n5.1.2 Closer to practice:\n\nBlocks:\n\nIs the project separable into blocks? What separate routines could I write?\nAre these blocks, independent, or possibly dependent?\nWhat separate routines could I write?\nAre there any routines available, in my own old code, or from other sources?\nCheck to see if the routines are working, as well as how the program is running across multiple routines.\nHow can I name functions and variables that I am confident I will remember later (i.e., after 3 months)? Make use of Hungarian notation.\n\n\nOn paper, define the following for each routine/step/function:\n\nWhat inputs does it have? What are the outputs?\n\n\n\n5.1.3 Use of Functions and computational efficiency\nWhy all the small functions when you could do everything in one?\n\nAvoiding the use of duplicate code. You don’t want to type in the entire code for a specific operation every time.\nHelps to identify where things go wrong.\nUsing previously written code.\nCode sharing.\n\nOnce you’ve identified a bottleneck, you must speed it up:\n\nLook for existing solutions.\nVectorize.\nParallelize.\nAvoid copies.\n\nWhen using a computer to perform intensive numerical calculations, we must keep two things in mind: code efficiency and execution time. Advanced topics and other parallelization strategies not covered in this lecture are available at the “High-Performance and Parallel Computing with R” CRAN Task View: https://CRAN.R-project.org/view=HighPerformanceComputing",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Program Efficiency and Parallel Computing</span>"
    ]
  },
  {
    "objectID": "efficiency_and_parallel_computing.html#code-execution-time",
    "href": "efficiency_and_parallel_computing.html#code-execution-time",
    "title": "\n5  Program Efficiency and Parallel Computing\n",
    "section": "\n5.2 Code execution time",
    "text": "5.2 Code execution time\nUltimately, we measure how efficient a program is by how long it takes to run, which will depend on the language it is written in and the computer it is run on.\nAlso, computers typically are doing a number of things at once, so the time taken to run a program will also depend on what else the computer is doing at the same time.\nR provides the function system.time to measure how many CPU (Computer Processing Unit) seconds are spent evaluating an expression:\n\nsystem.time({\n  mA &lt;- matrix(rnorm(1000^2), 1000, 1000)\n  solve(mA)\n})\n#&gt;    user  system elapsed \n#&gt;    1.11    0.00    1.19\n\nWe know that in R, creating or changing the size of a vector (also called redimensioning an array) is relatively slow. Consequently, operations like:\n\nvX &lt;- 0\nfor (i in 1:1e5) {\n  vX &lt;- c(vX, i)\n}\n\nShould be avoided in favour of:\n\nvX &lt;- numeric(1e5 + 1)\nfor (i in 1:1e5) {\n  vX[i + 1] = i\n}\n\nIndeed, the code speeds up by a factor of about 537x:\n\nsystem.time({\n  vX &lt;- 0\n  for (i in 1:1e5) {\n    vX &lt;- c(vX, i)\n  }\n})\n#&gt;    user  system elapsed \n#&gt;    8.78    1.75   10.56\n\nsystem.time({\n  vX &lt;- numeric(1e5 + 1)\n  for (i in 1:1e5) {\n   vX[i + 1] = i\n  }\n})\n#&gt;    user  system elapsed \n#&gt;       0       0       0",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Program Efficiency and Parallel Computing</span>"
    ]
  },
  {
    "objectID": "efficiency_and_parallel_computing.html#loops-versus-vectors",
    "href": "efficiency_and_parallel_computing.html#loops-versus-vectors",
    "title": "\n5  Program Efficiency and Parallel Computing\n",
    "section": "\n5.3 Loops versus vectors",
    "text": "5.3 Loops versus vectors\nIn R, vectorized operations are faster than equivalent loops. The difference is because R is a very high-level language.\nIn R, it is relatively easy to create and manipulate variables. The price we pay for this flexibility is speed.\nWhen you evaluate an expression in R, it is ‘translated’ into a faster lower-level language before being evaluated, then the result is translated back into R. The translation is what takes much of the time, and vectorization saved on the amount of translation required.\nTake the following code to square each element of vX:\n\nfor (i in 1:length(vX)) vX[i] &lt;- vX[i]^2\n\nEach time we evaluate the expression vX[i] &lt;- vX[i]^2, we translate vX all at once and then square it, before translating the answer back: all the work takes place in our faster lower-level language.\nMany of R’s functions are vectorized, which means that if the first argument is a vector, then the output will be a vector of the same length, computed by applying the function element-wise to the input vector.\n\n5.3.1 But… If you have to use a loop in R…\nSometimes, loops can be intuitive and more easily visualized than vectorized code or matrix operations. For instance, many algorithms stated in academic papers are listed in terms of for-loop syntax.\nIf vectorization simply takes too long time or simply is impossible then by all means use a loop, but…\n\nPre-allocate vector/matrix prior to the loop\nIterate as few times as possible\n\nBasically, everything that is not needed in the loop should not be in it! But, do avoid loops in R if possible! The penalty of loops in R is simply much larger than lower level languages, i.e., C++. Note that it’s relatively easy to make R code much faster!\n\n5.3.2 The flexibility vs speed trade-off\nThe mean() function example:\n\nlibrary(\"microbenchmark\")\n#&gt; Warning: package 'microbenchmark' was built under R version 4.3.3\nvY &lt;- runif(1E7)\nmicrobenchmark(sum(vY) / length(vY), mean(vY))\n#&gt; Unit: milliseconds\n#&gt;                expr     min       lq      mean   median       uq     max neval\n#&gt;  sum(vY)/length(vY)  7.6395  7.66935  7.731422  7.72375  7.75925  8.0807   100\n#&gt;            mean(vY) 15.3455 15.43570 15.528918 15.46365 15.51770 16.4608   100\nall.equal(sum(vY) / length(vY), mean(vY))\n#&gt; [1] TRUE\n\n… also you would probably not have too hard of a time programming a tailored simple linear regression function that beats the lm() built-in R function. But lm() offers a lot of flexibility…\n\n5.3.3 Arithmetic is not always the fastest\n\nlibrary(\"microbenchmark\")\nvY &lt;- runif(1E5)\nmX &lt;- replicate(10, runif(1E5))\nmicrobenchmark(vY^(1/2), sqrt(vY))\n#&gt; Unit: microseconds\n#&gt;      expr    min      lq     mean  median      uq  max neval\n#&gt;  vY^(1/2) 2462.8 2514.10 2658.327 2711.40 2739.55 2896   100\n#&gt;  sqrt(vY)  345.6  374.45  560.680  568.15  593.90 4573   100\nall.equal(vY^(1/2), sqrt(vY))\n#&gt; [1] TRUE\nmicrobenchmark(t(mX) %*% vY, crossprod(mX, vY))\n#&gt; Unit: milliseconds\n#&gt;               expr    min      lq     mean  median     uq     max neval\n#&gt;       t(mX) %*% vY 3.0532 3.11030 3.787389 3.16155 3.2645 10.8240   100\n#&gt;  crossprod(mX, vY) 1.6651 1.70045 1.724691 1.71520 1.7392  1.9011   100\nall.equal(t(mX) %*% vY, crossprod(mX, vY))\n#&gt; [1] TRUE\n\n\n5.3.4 Vectorization: Tips and Tricks for Speed\n\nVector-based programming (also mentioned in previous lectures)\nVectorized functions\nVectorized if/else?\nSubsetting/subscripting of data structures\n\n5.3.5 Summing the columns of a matrix\nConsider the problem of evaluating the sum of the columns of a 10000 x 10000 matrix mA:\n\nmA &lt;- matrix(rnorm(1e8), nrow = 10000)\n\nTo solve our problem, we have several options:\n\nA double loop of summations\n\n\nsystem.time({\n  vColSum &lt;- rep(NA, ncol(mA))\n  for (i in 1:ncol(mA)) {\n    s &lt;- 0\n    for (j in 1:nrow(mA)) {\n      s &lt;- s + mA[j, i]\n    }\n    vColSum[i] &lt;- s\n  }\n})\n#&gt;    user  system elapsed \n#&gt;    3.41    0.00    3.41\n\n\nThe use of apply\n\n\nsystem.time(\n  vColSum &lt;- apply(mA, 2, sum)\n)\n#&gt;    user  system elapsed \n#&gt;    0.69    0.37    1.06\n\n\nA single loop of sums\n\n\nsystem.time({\n  vColSum &lt;- numeric(ncol(mA))\n  for (i in 1:ncol(mA)) {\n    vColSum[i] &lt;- sum(mA[, i])\n  }\n})\n#&gt;    user  system elapsed \n#&gt;    0.29    0.33    0.62\n\n\nExploit the mathematical formulation: \\(\\textbf{\\textit{u'A}}\\), where \\(\\textbf{\\textit{u}}\\) is a vector of ones with length equal to the number for columns of \\(\\textbf{\\textit{u}}\\):\n\n\nvU &lt;- rep(1, ncol(mA))\nsystem.time({\n  vColSum &lt;- vU %*% mA\n})\n#&gt;    user  system elapsed \n#&gt;    0.17    0.00    0.17\n\n\nUsing the dedicated R function:\n\n\nsystem.time({\n  vColSum &lt;- colSums(mA)\n})\n#&gt;    user  system elapsed \n#&gt;    0.08    0.00    0.08\n\n\n5.3.6 Comparison\n\n\n\n\n\n\n\nStrategy\nCPU Time\nGain over double loop\n\n\n\nDouble loop\n3,7\n\n\n\napply\n1,29\n287%\n\n\nOne loop with sum\n0,77\n481%\n\n\n\\(\\textbf{\\textit{u'A}}\\)\n0,17\n2176%\n\n\ncolSums\n0,08\n4625%\n\n\n\nSometimes the easiest solution is also the preferred one.\n\n5.3.7 Vectorized if/else statement\nTHe if else statement in R is one of the few things that does not take vector inputs.\nOne possibility is to use loops around the if-statements. Another is to use the ifelse() function that takes vector/matrix inputs. For a single vector, this should be faster than a for-loop approach.\nA simple for-loop if-statement function:\n\nfun &lt;- function(vY) {\n  iN &lt;- length(vY) # Number of elements\n  vD &lt;- numeric(iN) # Pre-allocate vector\n  for (i in 1:iN) {\n    if(vY[i] &lt; 0.5) vD[i] &lt;- -1 else vD[i] &lt;- 1\n  }\n  vD\n}\n\nVs. the vectorized function ifelse():\n\nifelse(vY &lt; 0.5, -1, 1)\n\nUsing the microbenchmark package to time the two functions a 100 times. Which function is the fastest?\n\nlibrary(\"microbenchmark\")\nvY &lt;- runif(1E5)\nmicrobenchmark(fun(vY), ifelse(vY &lt; 0.5, -1, 1))\n#&gt; Unit: milliseconds\n#&gt;                     expr    min      lq     mean  median     uq     max neval\n#&gt;                  fun(vY) 6.5357 6.57100 6.658758 6.61520 6.6590 10.1134   100\n#&gt;  ifelse(vY &lt; 0.5, -1, 1) 3.1296 3.19465 3.300467 3.27565 3.3952  3.5818   100\nall.equal(fun(vY), ifelse(vY &lt; 0.5, -1, 1))\n#&gt; [1] TRUE\n\nifelse() is definitely faster and simpler!\nSuppose:\n\nmY &lt;- matrix(4E2, 200, 200)\n\nThe function now is:\n\nfun3 &lt;- function(mY) {\n  iN &lt;- nrow(mY) # Number of rows of mY\n  iK &lt;- ncol(mY) # Number of columns of mY\n  mD &lt;- matrix(0, iN, iK) # Pre-allocate to avoid growing objects!\n  for (i in 1:iN) { # Outer loop\n    for (j in 1:iK) { # Nested loop\n      if (mY[i, j] &lt; 0.5) mD[i, j] &lt;- -1 # If true\n      else mD[i, j] &lt;- 1 # Otherwise\n    }\n  }\n  mD\n}\n\nWhile, ifelse is still ifelse(mY &lt; 0.5, -1, 1). And obviously, faster:\n\nmicrobenchmark(ifelse(mY &lt; 0.5, -1, 1), fun3(mY))\n#&gt; Unit: microseconds\n#&gt;                     expr    min      lq     mean  median     uq    max neval\n#&gt;  ifelse(mY &lt; 0.5, -1, 1)  413.2  698.30  848.213  919.75  961.7 1521.9   100\n#&gt;                 fun3(mY) 2947.6 3046.95 3153.900 3084.00 3131.8 8442.4   100\nall.equal(ifelse(mY &lt; 0.5, -1, 1), fun3(mY))\n#&gt; [1] TRUE\n\n\n5.3.8 Subsetting/subscripting: One of the most powerful tools!\nSubsetting is usually overlooked when attempting to vectorize code in R, but is usually quite effective and nearly always faster than alternatives.\nYou have already used it (or at least seen it in action) plenty of times, for extracting or removing rows/columns on different data structures.\nIn practice, subscripting is an invaluable tool for data management, i.e., cleaning or sample selection of data.\nOn a 200x200 matrix mY:\n\nfun2 &lt;- function(mY) {\n  mBool &lt;- (mY &lt; 0.5) # Matrix of booleans\n  mY[mBool] &lt;- -1 # If true\n  mY[!mBool] &lt;- 1 # If false\n  mY # output\n}\nmicrobenchmark(ifelse(mY &lt; 0.5, -1, 1), fun2(mY), fun3(mY))\n#&gt; Unit: microseconds\n#&gt;                     expr    min      lq     mean  median      uq    max neval\n#&gt;  ifelse(mY &lt; 0.5, -1, 1)  394.3  913.15  841.074  942.55  970.85 1127.3   100\n#&gt;                 fun2(mY)  226.7  518.45  529.344  528.60  543.00 2609.2   100\n#&gt;                 fun3(mY) 2953.3 3058.20 3092.473 3079.25 3109.85 3339.5   100\nall.equal(fun2(mY), fun3(mY))\n#&gt; [1] TRUE\n\n\n5.3.9 Exercises in vectorization\n\nReformulate exercise 5 from Exercise set 2 (the if else statement one) in terms of the ifelse() function (Hint: some functions take functions as input).\nSuppose you use a vector of draws from a standard normal dist. as input instead. Can you beat the ifelse() you just made somehow? Write a function that is faster than the code you wrote in the above question. Compare using the microbenchmark package.\nFind a way to vectorize calculations of rowMeans of a 100 by 100 matrix of 10000 random draws from a uniform distribution.\n\nSolution:\n\n# Using ifelse()\nslowSolution &lt;- function(vInput) {\n  # ifelse(vInput &lt;= 0, vInput &lt;- - vInput ^ 3, ifelse(vInput &lt;= 1, vInput &lt;- vInput ^ 2, sqrt(vInput)))\n  \n  for (i in 1:length(vInput)) {\n    ifelse(vInput[i] &lt;= 0, vInput[i] &lt;- - vInput[i] ^ 3, vInput[i])\n    ifelse(vInput[i] &lt;= 1, vInput[i] &lt;- vInput[i] ^ 2, vInput[i])\n    ifelse(vInput[i] &gt; 1, vInput[i] &lt;- sqrt(vInput[i]), vInput[i])\n  }\n  return(vInput)\n}\n\n# Fast solution using indexing\nfastSolution &lt;- function(vInput) {\n  vInput[vInput &lt;= 0] &lt;- - vInput[vInput &lt;= 0] ^ 3\n  vInput[vInput &lt;= 1] &lt;- vInput[vInput &lt;= 1] ^ 2\n  vInput[vInput &gt; 1] &lt;- sqrt(vInput[vInput &gt; 1])\n  return(vInput)\n}\n\nset.seed(1)\nvX &lt;- rnorm(10)\n\nmicrobenchmark(slowSolution(vX), fastSolution(vX))\n#&gt; Unit: microseconds\n#&gt;              expr  min   lq    mean median   uq    max neval\n#&gt;  slowSolution(vX) 37.1 37.8 115.337   38.1 39.0 7713.8   100\n#&gt;  fastSolution(vX)  2.0  2.2  48.662    2.5  2.9 4593.5   100\nall.equal(slowSolution(vX), fastSolution(vX))\n#&gt; [1] TRUE\n\n# Vectorization of rowMeans\nvX &lt;- rnorm(10000)\nmX &lt;- matrix(vX, 100, 100)\nfRowMeans &lt;- function(mInput) {\n  return(rowSums(mInput) / ncol(mInput))\n}\n\nmicrobenchmark(rowMeans(mX), fRowMeans(mX))\n#&gt; Unit: microseconds\n#&gt;           expr  min   lq   mean median   uq    max neval\n#&gt;   rowMeans(mX) 20.8 21.0 21.972   21.1 21.4   41.4   100\n#&gt;  fRowMeans(mX) 21.6 21.8 39.252   21.9 22.1 1548.9   100\nall.equal(rowMeans(mX), fRowMeans(mX))\n#&gt; [1] TRUE",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Program Efficiency and Parallel Computing</span>"
    ]
  },
  {
    "objectID": "efficiency_and_parallel_computing.html#parallel-processing",
    "href": "efficiency_and_parallel_computing.html#parallel-processing",
    "title": "\n5  Program Efficiency and Parallel Computing\n",
    "section": "\n5.4 Parallel processing",
    "text": "5.4 Parallel processing\n\n5.4.1 Warning before parallellization\nOptimizing code to make it run faster is an iterative process:\n\nFind the biggest bottleneck (the slowest part of your code).\nTry to eliminate it (you may not succeed but that’s ok)\nRepeat until your code is “fast enough”.\n\nThis sounds easy, but it’s not. Parallel processing should first be considered when you cannot achieve efficiency gains by any other means… Or if you are really pressured on time.\n\n5.4.2 Parallel Processing\nConsider the problem of summing a vector of length n, which requires n - 1 separate additions. If each addition takes 1 second, and we do them one after another, then the whole calculation takes n - 1 seconds.\nNow, suppose we split the vector into two and give each half to a different calculator. Each calculator spend n/2 - 1 seconds adding up their half, which happens concurrently, then we spend 1 second adding together the two bits, for a total of n/2 seconds.\nNote, however, that communications between the processors requires time: i.e., the total time is usually n/2 + \\(\\varepsilon\\).\nClearly, parallelization requires multiple CPU’s, or cores (computers). The R community has developed tools that support the splitting of computations among different machines on a network, and among different cores on a single machine.\nThe base package parallel provides tools that will work across most of the platforms that are supported by R. We first load the package and determine how many cores can be detected on the machine:\n\nlibrary(parallel)\ndetectCores()\n#&gt; [1] 6\n\n\n5.4.3 Creating a cluster\nWe initialize a cluster using the makeCluster function:\n\ncluster &lt;- makeCluster(2)\n\nA cluster with 2 workers has been created and its details are collected in the object cluster.\nIf you now open the Task Manager, you will find 2 new R instances. Having made the cluster, we can then test the cluster by using a simple example that calls a function using each worker within the cluster.\n\nclusterCall(cluster, function(x) print(\"Pick me!\"))\n#&gt; [[1]]\n#&gt; [1] \"Pick me!\"\n#&gt; \n#&gt; [[2]]\n#&gt; [1] \"Pick me!\"\n\n\n5.4.4 Running jobs in parallel\nProvided that our job can run in parallel (there is nothing recursive that cannot be allocated to different workers), we can exploit the parallelised version of the apply family of functions. For instance:\n\napply \\(\\rightarrow\\) parApply\nlapply \\(\\rightarrow\\) parLapply\nsapply \\(\\rightarrow\\) parSapply\n\nThe use of these functions is exactly analogous to the ones belonging to the apply family. The only difference is that they accept an extra argument cl which accepts a cluster object. FOr instance:\n\nwaste.of.time &lt;- function(x) for (i in 1:10000) i\nsystem.time(lapply(1:10000, waste.of.time))\n#&gt;    user  system elapsed \n#&gt;    0.98    0.00    0.98\nsystem.time(parLapply(cl = cluster, 1:10000, waste.of.time))\n#&gt;    user  system elapsed \n#&gt;    0.00    0.00    0.42\n\n\n5.4.5 Kill your cluster\nOnce the cluster is no longer necessary, we kill it with:\n\nstopCluster(cluster)\n\nIf we omitted this command, then the cluster would be disbanded only when the R session that created it is terminated. This means that until you close R, your computer’s resources will be occupied by unnecessary processes.\n\n5.4.6 Small exercises on the parallel universe\n\nInstall and load the package “parallel” into your R session.\nDetect how many cores R can detect on your system.\nIf more than 1, then form a cluster using all but one of the cores detected above.\nUse the function(s) waste.of.time() and/or fibo()(fibo() can be found in the script from the last exercise from last lecture)\nPass your function to the sapply() function and pass inputs 1 to 30 by sapply to your function.\nInstead, pass the function to the parSapply() function. (Remember to export the function to the clusters)\nIs parSapply() faster than regular sapply? (Don’t do too many repetitions of sapply()/parSapply(); only at most 1:30 with fibo().)\nTerminate the assigned cluster!\n\nSolution:\n\n# Install if necessary and load the library quietly\nif(!require(\"parallel\")) install.packages(\"parallel\")\nsuppressMessages(library(parallel))\n\n# Make cluster with more cores than 1 if possible\nif (detectCores() &gt; 1) cluster &lt;- makeCluster(detectCores() - 1)\n\nwaste.of.time &lt;- function(x) for (i in 1:10000) i\nfibo &lt;- function(n) {\n  if (n &lt; 2) return(n)\n  return(fibo(n - 1) + fibo(n - 2))\n}\n\n# `fibo` is a recursive function and thus needs to be exported to the cluster\nclusterExport(cluster, \"fibo\")\n\n# Without a cluster\nsystem.time(sapply(1:30, waste.of.time))\nsystem.time(sapply(1:30, fibo))\n\n# Using the cluster\nsystem.time(parSapply(cl = cluster, 1:30, waste.of.time))\nsystem.time(parSapply(cl = cluster, 1:30, fibo))\n\n# Kill the cluster\nstopCluster(cluster)",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Program Efficiency and Parallel Computing</span>"
    ]
  },
  {
    "objectID": "efficiency_and_parallel_computing.html#memory-issues",
    "href": "efficiency_and_parallel_computing.html#memory-issues",
    "title": "\n5  Program Efficiency and Parallel Computing\n",
    "section": "\n5.5 Memory issues",
    "text": "5.5 Memory issues\nComputer memory comes in a variety of forms. For most purposes it is sufficient to think in terms of RAM (random access memory), which is fast, and the hard disk, which is slow.\nVariables require memory. R stores variables in virtual memory, which is a seamless combination of RAM and hard disk space, managed by the underlying operating system.\nThe operating system will use R in preference to disk space, but if your variables require more memory than the amount of RAM physically installed, then it will have to use disk space as well, and your program will slow down as a result.\nAs soon as a vector (or list) is too large to store in RAM all at once, the speed at which you can use it will drop dramatically.\nIf the vector is sufficiently large, then it may not be possible to store it at all, in which case you are said to have run out of memory.\nAlso note that R has an absolute limit on the length of a vector of \\(2^{31}-1=2,147,483,647\\).\nIn the case your code runs out of memory, you will need to break you vectors down into small subvectors and deal with each in turn.\nAlternatively, you can use the rm function to delete objects from memory and free space:\n\ndA &lt;- 5\nls()\n#&gt; [1] \"dA\"\nrm(\"dA\")\n\nTo remove all objects, you can use rm(list = ls()).",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Program Efficiency and Parallel Computing</span>"
    ]
  },
  {
    "objectID": "efficiency_and_parallel_computing.html#small-hints",
    "href": "efficiency_and_parallel_computing.html#small-hints",
    "title": "\n5  Program Efficiency and Parallel Computing\n",
    "section": "\n5.6 Small hints",
    "text": "5.6 Small hints\n\nAdding / subtracting tends to be better than multiplying.\nHence, log-likelihood \\(\\sum \\log L_i\\) is better than likelihood \\(\\prod L_i\\)\n\nSimplify your equations, minimize number of operations.\nDon’t do \\(x = \\exp(\\log(z))\\) if you can avoid it.\nSplit your program into smaller tasks. Keep it simple!",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Program Efficiency and Parallel Computing</span>"
    ]
  },
  {
    "objectID": "introduction_to_simulation.html",
    "href": "introduction_to_simulation.html",
    "title": "\n6  Introduction to Simulation\n",
    "section": "",
    "text": "6.1 Seeding\nSimulation is essential for quantitative analysis. It is widely applied in estimation, hypothesis testing, equation solving, numerical computing, etc.\nOnce the seed is known, the whole sequence of random variables can be reproduced. This property facilitates repetition of experimentation.\nFor example, run first:\nset.seed(10086)\nrunif(5)\n#&gt; [1] 0.708768032 0.384298612 0.003025926 0.277903935 0.714458605\nNow run again:\nrunif(5)\n#&gt; [1] 0.3166092 0.6863317 0.4725650 0.2655531 0.6193426\nSetting seed back to 10086, we can reproduce results in the first frame:\nset.seed(10086)\nrunif(5)\n#&gt; [1] 0.708768032 0.384298612 0.003025926 0.277903935 0.714458605\nYou can also save the current state by:\nRNG.state &lt;- .Random.seed\nrunif(5)\n#&gt; [1] 0.3166092 0.6863317 0.4725650 0.2655531 0.6193426\nNow try to implement the following codes:\n.Random.seed &lt;- RNG.state\nrunif(5)\n#&gt; [1] 0.3166092 0.6863317 0.4725650 0.2655531 0.6193426\nIn this way you reproduce the results.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Simulation</span>"
    ]
  },
  {
    "objectID": "introduction_to_simulation.html#seeding",
    "href": "introduction_to_simulation.html#seeding",
    "title": "\n6  Introduction to Simulation\n",
    "section": "",
    "text": "set.seed(seed) is used to specify the seed.\nThe current state of the random number generator is kept in the vector .Random.seed.\n\nIf the random number generator is not initialized before generating pseudo-random numbers, R initializes it using a value taken from the system clock.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Simulation</span>"
    ]
  },
  {
    "objectID": "introduction_to_simulation.html#discrete-random-variables",
    "href": "introduction_to_simulation.html#discrete-random-variables",
    "title": "\n6  Introduction to Simulation\n",
    "section": "\n6.2 Discrete Random variables",
    "text": "6.2 Discrete Random variables\n\n6.2.1 Simulate Discrete Random Variables\nSuppose that we would like to simulate a discrete random variable \\(X\\).\n\n\n\\(X\\) takes values in {0,1,2,…}\n\n\\(X\\) has the CDF \\(F\\)\n\n\nSuppose \\(m\\) is any number from the set \\({0,1,2,...}\\) such that it satisfies the following equation, given that \\(U\\) follows the uniform distribution over \\((0,1)\\).\n\\[\nP(X=m)=F(m)-F(m-1)=P(F(m-1)&lt;U \\le F(m))\n\\]\nHence, the algorithm for simulating \\(X\\) is:\n\nSimulate a random variable \\(U \\sim \\text{Uniform}(0,1)\\);\nFind the minimal \\(m\\) such that \\(F(m) \\ge U\\), that is,\n\n\\[\nm = \\min\\{k \\ge 0 : F(k) \\ge U\\}\n\\]\n\nSet \\(X = m\\).\n\nThis algorithm vectorizes the function and takes advantage of the ellipsis …, since at times additional arguments need to be passed to the cumulative distribution function.\n\nDiscrete.Simulate &lt;- function(F, size, ...) {\n  \n  # F - Cumulative distribution function\n  # size - number of i.i.d. random variables to be simulated\n\n  m &lt;- 0\n  U &lt;- runif(size)\n  X &lt;- rep(NA, size)\n  X[F(0, ...) &gt;= U] &lt;- 0\n\n  while (any(F(m, ...) &lt; U)) {\n    m &lt;- m + 1\n    X[(F(m, ...) &gt;= U) & (F(m - 1, ...) &lt; U)] &lt;- m\n  }\n\n  return(X)\n}\n\nFor example, suppose that we would like to simulate 1000 i.i.d. discrete random variables whose probability density function is:\n\\[\np(x) =\n\\begin{cases}\n0.2, \\text{ if } x = 1 \\\\\n0.3, \\text{ if } x = 2 \\\\\n0.5, \\text{ if } x = 3\n\\end{cases}\n\\]\nThe corresponding cumulative distribution function is:\n\\[\nF(x) =\n\\begin{cases}\n0.2, \\text{ if } 1 \\le x &lt; 2 \\\\\n0.5, \\text{ if } 2 \\le x &lt; 3 \\\\\n1, \\text{ if } x \\ge 3\n\\end{cases}\n\\]\nThe following program makes use of Discrete.Simulate we just wrote to accomplish the task, and in addition produces the histogram:\n\n# cumulative distribution function\n\nF &lt;- function(x) {\n  return(0.2 * ((x &gt;= 1) && (x &lt; 2)) + 0.5 * ((x &gt;= 2) && (x &lt; 3)) + (x &gt;= 3))\n}\n\n# draw 1000 i.i.d. random variables whose CDF is F\n\nset.seed(10086)\nX &lt;- Discrete.Simulate(F, 1000)\n\n# histogram\nh &lt;- hist(X, breaks = seq(0.55, 3.55, 0.1), plot = FALSE)\nh$density &lt;- h$counts / 1000\nplot(h, freq = FALSE, col = \"cornflowerblue\",\n     main = \"\", xlab = \"n\", ylab = \"Percentage\")\n\n\n\n\n\n\n\nSimulate a binomial random variable \\(X \\sim \\text{Binomial}(n, p)\\).\n\nThe density is given as\n\n\\[\np(k) = \\mathbb{P}(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}, \\text{for } 0 \\le k \\le n\n\\]\n\nThe CDF is given as\n\n\\[\nF(x) = \\sum_{k=0}^{ \\lbrack x \\rbrack} \\binom{n}{k} p^k (1-p)^{n-k}\n\\]\nThe following program draws 1000 random variables with the distribution Binomial(10, 0.5). Here, we see how ellipsis … renders Discrete.Simulate more flexible.\n\n# cumulative distribution function of binomial distribution\n\nF &lt;- function(x, n, p) {\n  Index &lt;- 0:floor(x)\n  Density &lt;- choose(n, Index) * p^(Index) * (1-p)^(n-Index)\n  return(sum(Density))\n}\n\n# draw 1000 i.i.d. binomial random variables\n\nset.seed(10086)\nX &lt;- Discrete.Simulate(F, 1000, 10, 0.5)\n\nAlternatively, you can make use of the built-in function rbinom:\n\nY &lt;- rbinom(1000, 10, 0.5)\n\nCompare the histograms:\nh1 &lt;- hist(X, breaks = seq(-0.5, 10.5, 0.2), plot = FALSE)\nh1$density &lt;- h1$counts / 1000\nplot(h1, freq = FALSE, col = \"cornflowerblue\",\n     main = \"\", xlab = \"n\", ylab = \"Percentage\")\nh2 &lt;- hist(Y, breaks = seq(-0.5, 10.5, 0.2), plot = FALSE)\nh2$density &lt;- h2$counts / 1000\nplot(h2, freq = FALSE, col = \"cornflowerblue\",\n     main = \"\", xlab = \"n\", ylab = \"Percentage\")\n\n\n\n\n\n\n\nUse Discrete.Simulate\n\n\n\n\n\n\n\nUse rbinom\n\n\n\n\n\n\nFigure 6.1",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Simulation</span>"
    ]
  },
  {
    "objectID": "introduction_to_simulation.html#inversion",
    "href": "introduction_to_simulation.html#inversion",
    "title": "\n6  Introduction to Simulation\n",
    "section": "\n6.3 Inversion",
    "text": "6.3 Inversion\n\n6.3.1 Inversion Method\nThus far, we have discussed about simulation of random variables taking values in {0,1,…}. Next, we explain how to simulate random variable taking value in \\(\\mathbb{R}\\).\n\nSuppose F is a cumulative distribution function on \\(\\mathbb{R}\\) and U is a random variable uniformly distributed over \\((0,1)\\), then:\n\n\\[\nX=F^{-1}(U)\n\\]\n\nwhere \\(F^{-1}(\\cdot)\\) is an inverse function of \\(F\\).\nInversion method: As long as you can compute \\(F^{-1}\\), you can simulate a random variable using a uniformly distributed random variable.\n\nRecall, that is \\(X \\sim \\text{Exp}(\\lambda)\\), its cumulative distribution function is given by:\n\\[\nF(x)=\n\\begin{cases}\n0, \\text{ for } x &lt; 0 \\\\\n1-e^{- \\lambda x}, \\text{ for } x \\ge 0\n\\end{cases}\n\\]\nOne can show that the inverse \\(F^{-1}\\) is given by:\n\\[\nF^{-1}(y)=- \\lambda ^{-1} \\log{(1-y), 0 \\le y \\le 1}\n\\]\nWhen, \\(y=1\\), \\(F^{-1}(y)=\\infty\\).\nBy means of \\(- \\lambda ^ {-1} \\log{(1-U)}\\), we can simulate the desired exponential distribution with intensity \\(\\lambda\\). Observe that \\(1-U\\) also has uniform distribution over \\((0,1)\\). Hence, it is equivalent to use \\(- \\lambda ^ {-1} \\log{(U)}\\), which is more often used in practice.\nHere is an example of drawing 1000 i.i.d. random variables with distribution exp(0.7).\n\nExponential.Simulate &lt;- function(lambda, size) {\n  \n  # lambda - intensity of the exponential distribution\n  # size - number of i.i.d. random variables to be drawn\n  \n  U &lt;- runif(size)\n  return(-1/lambda * log(U))\n  \n}\n\n# A test of the function\nset.seed(10086)\nX &lt;- Exponential.Simulate(0.7, 1000)\n\nAlternatively, we can also make use of the built-in function rexp, but an example is not provided.\nBelow is a histogram of the sample drawn with Exponential.Simulate. We also superimpose the theoretical density function (now, with code included!):\n\n# 1. Create the Histogram\nhist(X, \n     freq = FALSE,        # Use density instead of frequency on y-axis\n     breaks = 15,         # Adjust number of breaks for desired resolution\n     col = \"cornflowerblue\", \n     xlab = \"\",             # Remove default x-label (add it later with mtext)\n     ylab = \"Density\",\n     main = \"\",\n     xlim = c(0, 12))\n\n# 2. Superimpose the Theoretical Density Curve\ncurve(dexp(x, rate = 0.7),  # Theoretical exponential density function\n      from = 0,             # Start of the curve\n      to = 12, # End of the curve \n      col = \"red\",      \n      lwd = 2,            # Adjust line thickness\n      add = TRUE)         # Add to the existing plot (histogram)\n\n# 3. Add Labels and Title (using mtext for better control)\nmtext(\"x\", side = 1, line = 2.5)  # Add x-axis label below the axis\nmtext(\"Histogram of Exponential Sample and Theoretical Density\", \n      side = 3, line = 1, outer = TRUE) # Title at the top margin\n\n# 4. Add a Legend\nlegend(\"topright\",                    # Position of the legend\n       legend = c(\"histogram\", \"density\"), # Text for the legend\n       col = c(\"cornflowerblue\", \"red\"),   # Colors of the legend elements\n       lwd = 2)                         # Line width for the legend\n\n\n\n\n\n\n\n\n6.3.2 Small Exercise\n\nConsider the random variable With Gumble distribution \\(X \\sim \\text{Gumbel}(0,1)\\). The CDF is given by \\(F_{X}(x)= \\exp(- \\exp(-x))\\)\n\nNow, suppose you want to simulate random draws from this distribution by use of the inverse transform method.\n\nDerive the inverse transform \\(F_{X}^{-1}(y)\\) from this cdf. (Hint: solve \\(\\exp(- \\exp(-F_{X}^{-1}(y)))=y\\)). Write a function in R implementing the derived inverse transform.\nSet the seed to 1234 and use 100 random draws of standard uniforms.\nThe first 4 draws will be: -0.7766432 0.7458437 0.7022162 0.7495061\n\n\n\nSolution\nNote for the first part:\n\\[\n\\exp(- \\exp(-F_{X}^{-1}(y)))=y\n\\] \\[\n\\Rightarrow F_{X}^{-1}(y)=-\\log(-\\log(y))\n\\]\nThen, implementing the inverse transform:\n\nGumbel.Simulate &lt;- function(min, max, size) {\n  U &lt;- runif(size, min, max)\n  return(-log(-log(U)))\n}\n\nset.seed(1234)\n\nX &lt;- Gumbel.Simulate(0, 1, 100)\nX[1:4]\n#&gt; [1] -0.7766432  0.7458437  0.7022162  0.7495061",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Simulation</span>"
    ]
  },
  {
    "objectID": "introduction_to_simulation.html#acceptance-rejection",
    "href": "introduction_to_simulation.html#acceptance-rejection",
    "title": "\n6  Introduction to Simulation\n",
    "section": "\n6.4 Acceptance-Rejection",
    "text": "6.4 Acceptance-Rejection\nThe inversion method works fine if we can find \\(F^{-1}\\) analytically. It becomes less feasible when \\(F^{-1}\\) does not admit a closed-form representation, although it is still possible to employ (time-consuming) numerical methods to approximate \\(F^{-1}\\).\nYet, there is another clever method that is often applicable, called the acceptance-rejection method.\nSuppose we would like to simulate a random variable \\(X\\) whose cumulative distribution is \\(F\\), and\n\\[\nF(x) = \\int_{-\\infty}^{x} f(t) \\, dt\n\\]\nfor the probability density \\(f\\).\nThe basic idea of the acceptance-rejection method is to find another distribution \\(G\\) with probability density function \\(g\\), such that:\n\nA random variable with distribution \\(G\\) can be simulated using some efficient method (such as inversion method).\n\\(g\\) is nonzero on the support of \\(f\\), that is, \\(g(x) &gt; 0\\) as long as \\(f(x) &gt; 0\\).\n\\(f/g\\) is uniformly bounded, that is, \\(c=\\sup_{X} \\frac{f(x)}{g(x)}&lt;\\infty\\)\n\nOnce the forgoing conditions are satisfied, we shall say that \\(g\\) envelopes \\(f\\).\nThe acceptance-rejection algorithm for generating a random variable distributed as \\(F\\):\n\nGenerate a random variable \\(Y\\) distributed as \\(G\\)\nGenerate \\(U \\sim \\text{Uniform}(0,1)\\), which is independent from \\(Y\\)\nIf\n\n\\[\nU \\le \\frac{f(Y)}{cg(Y)}\n\\]\nThen set \\(X=Y\\) (“accept”); otherwise go back to step 1 (“reject”).\nIn practice, choosing the distribution function \\(G\\) is a kind of “art”.\nFirst of all, \\(G\\) should be chosen such that the corresponding random variable could be simulated efficiently.\nSecondly, \\(G\\) should be chosen such that \\(c=\\sup_{X} \\frac{f(x)}{g(x)}\\) is as close to \\(1\\) as possible.\nWe consider the example of generating a random variable from the Gamma distribution. For1 \\(k&gt;1\\) and \\(\\theta &gt; 0\\), the \\(\\text{Gamma}(k, \\theta)\\) density is given by:\n\\[\nf(x)=\\frac{\\theta^{k}}{\\Gamma{(k)}}x^{k-1}e^{-\\theta x}, x&gt;0\n\\]\nTo envelope \\(f\\), we choose the probability density function \\(g(\\cdot ; \\lambda)\\) of the \\(\\text{Exp}(\\lambda)\\), that is,\n\\[\ng(x;\\lambda)=\\lambda e^{-\\lambda x}, x&gt;0\n\\]\nThen:\n\\[\nh(x;\\lambda) = \\frac{f(x)}{g(x;\\lambda)}=\\frac{\\theta^{k}}{\\Gamma{(k) \\lambda}}x^{k-1}e^{(\\lambda-\\theta) x}\n\\]\nIf \\(h \\ge \\theta\\), apparently \\(\\sup_{x&gt;0}h(x;\\lambda)=\\infty\\), so we require \\(\\lambda &lt; \\theta\\) below. Given such \\(lambda\\), \\(h(x;\\lambda)\\) is maximized at \\(x_{\\lambda}= \\frac{k-1}{\\theta - \\lambda}\\), and\n\\[\n\\sup_{x}\\frac{f(x)}{g(x;\\lambda)}=h(x_\\lambda ; \\lambda)\n= \\frac{\\theta ^ k (k-1) ^ {k-1}}{\\Gamma{(k)} \\lambda (\\theta - \\lambda) ^{k-1}} = e^{-1(k-1)}\n\\]\nAs said above, we would like to choose the envelope density function such that the above supremum is as small as possible. In other words, we should choose \\(lambda\\) to minimize \\(h(x_\\lambda ; \\lambda\\). This is equivalent to maximizing \\(\\lambda (\\theta - \\lambda) ^{k-1}\\) by optimizing the first order condition wrt. \\(\\lambda\\). This leads to:\n\\[\n\\lambda = \\frac{\\theta}{k}\n\\]\nPlugging this back to the supremum, we get:\n\\[\nc = \\frac{k^k e^{-(k-1)}}{\\Gamma (k)}\n\\]\nThe code for the acceptance-rejection method for simulating a Gamma distributed random variable is.\n\n### Auxiliary functions ###\n\ngamma.pdf &lt;- function(x, k, theta) {\n  return(theta^k * x^(k-1) * exp(-theta*x)/gamma(k))\n}\n\nexponential.pdf &lt;- function(x, lambda) {\n  return(lambda * exp(-lambda*x))\n}\n\nExponential.Simulate &lt;- function(lambda, size = 1) {\n  V &lt;- runif(size)\n  return(-1/lambda * log(V))\n}\n\nGamma.Simulate &lt;- function(k, theta, size = 1) {\n  \n  lambda &lt;- theta/k\n  c &lt;- k^k * exp(-k+1) / gamma(k)\n  \n  U &lt;- rep(NA, size)\n  Y &lt;- rep(NA, size)\n  X &lt;- rep(NA, size)\n  Unaccepted &lt;- rep(TRUE, size)\n  \n  while (any(Unaccepted)) {\n    \n    UnacceptedCount &lt;- sum(Unaccepted)\n    \n    U &lt;- runif(UnacceptedCount)\n    Y &lt;- Exponential.Simulate(lambda, UnacceptedCount)\n    \n    Accepted_ThisTime &lt;- Unaccepted[Unaccepted] &\n      ( U &lt;= ( gamma.pdf(Y, k, theta) / exponential.pdf(Y, lambda)/c ) )\n    \n    X[Unaccepted][Accepted_ThisTime] &lt;- Y[Accepted_ThisTime]\n    Unaccepted[Unaccepted] &lt;- !Accepted_ThisTime\n    \n  }\n  \n  return(X)\n  \n}\n\nWe test the function by simulating \\(10^6\\) i.i.d. random variables with distribution \\(\\text{Gamma}(2,1)\\). Compare the histogram of the simulated data and the theoretical density:\n\nset.seed(10086)\n\nX &lt;- Gamma.Simulate(2, 1, 10^6)\n\n# histogram\n\nhist(X, breaks = 30, freq = FALSE,\n     main = \"Theoretical and simulated Gamma(2,1) density\",\n     col = \"cornflowerblue\",\n     xlim = c(0, 14), ylim = c(0, 0.35),\n     xlab = \"x\",\n     cex.main = 0.7)\n\nxticks = seq(0, max(X), 0.1)\nlines(xticks, dgamma(xticks, 2, 1), col = \"red\")\nlegend(\"topright\", legend = c(\"Simulated\", \"Theoretical\"), lty = c(1, 1), lwd = c(5, 1), col = c(\"cornflowerblue\", \"red\"))",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Simulation</span>"
    ]
  },
  {
    "objectID": "introduction_to_simulation.html#simulate-normals",
    "href": "introduction_to_simulation.html#simulate-normals",
    "title": "\n6  Introduction to Simulation\n",
    "section": "\n6.5 Simulate Normals",
    "text": "6.5 Simulate Normals\nIf \\(Z \\sim N(0,1)\\) then \\(\\mu + \\sigma Z \\sim N(\\mu, \\sigma^2)\\). Hence, it suffices to have an algorithm for simulating a \\(N(0,1)\\) random variable, i.e., a standard normal variable.\nWe cover three ways of simulating a standard normal random variable:\n\nCentral limit theorem applied to a uniformly distributed random variable.\nAcceptance-Rejection method with exponential envelope\nBox-Muller algorithm\n\n\n6.5.1 Simulate Normals: Using uniformly distributed random variable\nLet \\({U_i, i=1,2,...}\\) be a sequence of i.i.d. random variables uniformly distributed over \\((0,1)\\).\nNote that \\(\\mathbb{E}[U_i]=1/2\\) and \\(\\text{var}[U_i]=1/12\\) respectively. The classic central limit theorem entails:\n\\[\n\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n} \\frac{U_i-\\frac{1}{2}}{\\sqrt{\\frac{1}{12}}} \\rightarrow N(0,1)\n\\]\nIn particular, for some \\(n\\) a random variable \\(Z=\\sum_{i=1}^{n}U_i-n/2\\) is approximately a standard normal distribution.\nThe resulting program is very simple - set \\(n=12\\):\n\n### Simulate standard normal random variable ###\n\nNormal.Simulate &lt;- function(size = 1) {\n  \n  Z &lt;- rep(NA, size)\n  \n  for (i in 1:size) {\n    Z[i] &lt;- sum(runif(12)) - 6\n  }\n  return(Z)\n}\n\nDespite its simpleness, this method works well in fact.\n\nset.seed(10086)\n\nX &lt;- Normal.Simulate(10^6)\n\n# histogram\n\nhist(X, breaks = 35, freq = FALSE,\n     main = \"Theoretical and simulated N(0,1) density\",\n     col = \"cornflowerblue\",\n     xlim = c(-3.5, 3.5), ylim = c(0, 0.4),\n     xlab = \"x\",\n     cex.main = 0.7)\n\nxticks = seq(min(X), max(X), 0.1)\nlines(xticks, dnorm(xticks, 0, 1), col = \"red\")\nlegend(\"topright\", legend = c(\"Simulated\", \"Theoretical\"), lty = c(1, 1), lwd = c(5, 2), col = c(\"cornflowerblue\", \"red\"))\n\n\n\n\n\n\n\nBut it is possible to do better.\n\n6.5.2 Simulate Normals: Acceptance-Rejection with Exponential Envelope\nWhen \\(Z \\sim N(0,1)\\), we let \\(X=\\lvert Z \\rvert\\). Suppose that \\(S\\) is a random variable which is independent of \\(Z\\) and uniformly distributed over \\({-1,1}\\), that is,\n\\[\n\\mathbb{P}(S=-1)=\\mathbb{P}(S=1)=\\frac{1}{2}\n\\]\nThen it is easy to check\n\\[\nSX \\sim N(0,1)\n\\]\nThe probability density function of \\(X\\) is:\n\\[\nf(x) =\n\\begin{cases}\n\\sqrt{\\frac{2}{\\pi}} \\exp(-\\frac{1}{2}x^2), \\text{ if } x &gt; 0 \\\\\n0, \\text{ if } x \\le 0\n\\end{cases}\n\\]\nWhich is the half \\(N(0,1)\\) scaled by 2 to maintain a proper density.\nWe use the envelope density function:\n\\[\ng(x) =\n\\begin{cases}\n\\exp(-x), \\text{ if } x &gt; 0 \\\\\n0, \\text{ if } x \\le 0\n\\end{cases}\n\\]\nWhich is the probability density function of \\(\\text{Exp}(1)\\) distribution. Easily,\n\\[\nc=\\sup_{x}\\frac{f(x)}{g(x)}=\\sup_{x}\\sqrt{\\frac{2}{\\pi}}\n\\exp \\left( x-\\frac{x^2}{2} \\right) =\\sqrt{\\frac{2e}{\\pi}}\n\\]\nThis thus gives the following algorithm:\n\nDraw \\(V \\sim \\text{Uniform}(0,1)\\) and set \\(Y=-\\log(V)\\)\nDraw \\(U \\sim \\text{Uniform}(0,1)\\), which is independent of \\(Y\\)\nIf \\[\n  U \\le \\exp \\left( -\\frac{1}{2}(Y-1)^2 \\right)\n  \\] then generate \\(S\\) uniformly distributed over \\({-1,1}\\) and return \\(Z=SY\\); Otherwise go back to step 1.\n\n6.5.3 Simulate Normals: Box-Muller Algorithm\nThe Box-Muller algorithm is grounded on the following theorem:\n\nTheorem 6.1 (Box-Muller transform) Let \\(U\\) and \\(V\\) be two independent random variables uniformly distributed over \\((0,1)\\). Define:\n\\[\nX = \\sqrt{-2 \\log (U)} \\cos (2\\pi V), \\text{ } Y = \\sqrt{-2 \\log (U)} \\sin (2\\pi V)\n\\] Then \\((X,Y)^{\\top}\\) is a standard bivariate normal random vector, i.e.,\n\\[\n(X, Y)^{\\top} \\sim N\\left( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\right)\n\\]\n\nHence, we have the Box-Muller algorithm, which draws two independent standard normal random variables at a time:\n\nDraw independent \\(U \\sim \\text{Uniform}(0,1)\\) and \\(V \\sim \\text{Uniform}(0,1)\\)\nSet \\(R=\\sqrt{-2 \\log(U)}\\) and \\(\\Theta = 2\\pi V\\)\nReturn \\(X=R \\cos{\\Theta}\\) and \\(Y=R \\sin{\\Theta}\\)\n\nThe resulting program is very simple:\n\nBoxMuller &lt;- function(size = 1) {\n  \n  # size: number of standard bivariate normal random variables to simulate\n  \n  U &lt;- runif(size)\n  V &lt;- runif(size)\n  \n  X &lt;- sqrt(-2*log(U)) * cos(2*pi*V)\n  Y &lt;- sqrt(-2*log(U)) * sin(2*pi*V)\n  \n  return(c(X,Y))\n  \n}\n\nThe histogram of 1000 standard normal random variables simulated using Box-Muller transformation:\n\nset.seed(10086)\n\nX &lt;- BoxMuller(1000)\n\n# histogram\n\nhist(X, breaks = 35, freq = FALSE,\n     main = \"Theoretical and simulated N(0,1) density\",\n     col = \"cornflowerblue\",\n     xlim = c(-3.5, 3.5), ylim = c(0, 0.4),\n     xlab = \"x\",\n     cex.main = 0.7)\n\nxticks = seq(min(X), max(X), 0.1)\nlines(xticks, dnorm(xticks, 0, 1), col = \"red\")\nlegend(\"topright\", legend = c(\"Simulated\", \"Theoretical\"), lty = c(1, 1), lwd = c(5, 2), col = c(\"cornflowerblue\", \"red\"))",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Simulation</span>"
    ]
  },
  {
    "objectID": "introduction_to_simulation.html#monte-carlo-integration",
    "href": "introduction_to_simulation.html#monte-carlo-integration",
    "title": "\n6  Introduction to Simulation\n",
    "section": "\n6.6 Monte Carlo Integration",
    "text": "6.6 Monte Carlo Integration\n\n6.6.1 Monte Carlo Integration - a brief introduction\nSuppose that you want to compute the integral:\n\\[\nI = \\int_{a}^{b} f(x)dx\n\\]\nIt is not always possible to calculate it analytically. In such cases, we can use Monte Carlo by noting that:\n\\[\nI=(b-a)\\mathbb{E}[f(U)], \\space U \\sim \\text{Uniform}(a,b)\n\\]\nLet \\(U_i : i=1,2,...\\) be an i.i.d. sequence of random variables uniformly distributed over \\((a,b)\\). The strong law of large numbers implies:\n\\[\n\\frac{b-a}{n} \\sum_{i=1}^{n}f(U_i) \\xrightarrow{a.s.}I \\text{, as }n \\rightarrow \\infty\n\\]\nWhere \\(\\xrightarrow{a.s.}\\) means almost sure convergence.\n\n6.6.2 Monte Carlo Integration - the code\nHence, you may pick a large \\(n\\), draw i.i.d. \\(U_i \\sim \\text{Uniform}(a,b)\\), and use \\(\\frac{b-a}{n} \\sum_{i=1}^{n}f(U_i)\\) as an approximate of \\(I\\). The following function does this:\n\nMonteCarlo.Integration &lt;- function(f, n, a, b) {\n  \n  U &lt;- runif(n, min = a, max = b)\n  return( (b-a)*mean(f(U)) )\n  \n}\n\n\n6.6.3 Monte Carlo Integration - examples\nSet seed to 10086 and consider two examples.\nExample 1: \\(\\int_{-1}^{1} \\frac{1}{1+x^2}dx\\) with the accurate value \\(\\frac{\\pi}{2} \\approx 1.570796\\). We get:\n\nset.seed(10086)\nMonteCarlo.Integration(function(x) 1/(1+x^2), 100, -1, 1)\n#&gt; [1] 1.589363\n\nExample 2: \\(\\int_{0}^{1} \\arctan{x}dx\\) with the accurate value \\(0.4388246\\).\n\nset.seed(10086)\nMonteCarlo.Integration(function(x) atan(x), 100, 0, 1)\n#&gt; [1] 0.445995\n\n\n6.6.4 Small Exercise\nSuppose you want to measure the area of the unit circle via the Monte Carlo integration using the following steps:\n\nSimulate \\(U \\sim U(0,1)\\) and \\(V \\sim U(0,1)\\) to obtain \\(R^2 = U^2 + V^2\\), \\(100\\) times.\nDefine a function \\(f(U,V) = 1_{ \\left\\{ U^2+V^2&lt;1 \\right\\} }\\): \\(1_{\\text{Condition}}\\) is an indicator function that returns \\(1\\) if the condition is true and \\(0\\) otherwise.\nImplement the monte carlo integration of the unit circle in R using the formula \\((b-a)E[f(U)]\\).\nNow, note that your result likely is somewhat lower than the area of the unit circle. Why is that? (Hint: try to multiply your obtained result by 4).\nTry to increase the number of standard uniform draws used to approximate the area.\n\nNote that you should have successfully approximated a famous mathematical constant.\nSolution\n\nvU &lt;- runif(10000)\nvV &lt;- runif(10000)\n\nfLessThanOne &lt;- function(vInput) {\n  return(vInput &lt; 1)\n}\n\ndResult &lt;- (1 - 0) * mean(fLessThanOne(vU^2 + vV^2))\ndResult * 4\n#&gt; [1] 3.1716",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Simulation</span>"
    ]
  },
  {
    "objectID": "introduction_to_simulation.html#footnotes",
    "href": "introduction_to_simulation.html#footnotes",
    "title": "\n6  Introduction to Simulation\n",
    "section": "",
    "text": "For \\(0&lt;k&lt;1\\) it’s still possible to employ the acceptance-rejection method, but we need another envelope probability density function, which we don’t elaborate on here.↩︎",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Simulation</span>"
    ]
  },
  {
    "objectID": "rootfinding.html",
    "href": "rootfinding.html",
    "title": "\n7  Root-finding\n",
    "section": "",
    "text": "7.1 Main problem\nSuppose \\(f : \\mathbb{R} \\rightarrow \\mathbb{R}\\) is a continuous function. A root of \\(f\\) is a solution to the equation \\(f(x)=0)\\).\nThat is, a root is a number \\(a \\in \\mathbb{R}\\) such that \\(f(a)=0\\), i.e.,\n\\[\nR=\\{ a\\in \\mathbb{R}:f(a)=0\\}\n\\]\nIf we draw the gparh of our function, say \\(y=f(x)\\), which is a curve in the plane, a solution of \\(f(x)=0\\) is the \\(x\\)-coordinate of a point at which the curve crosses the \\(x\\)-axis.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Root-finding</span>"
    ]
  },
  {
    "objectID": "rootfinding.html#main-problem",
    "href": "rootfinding.html#main-problem",
    "title": "\n7  Root-finding\n",
    "section": "",
    "text": "7.1.1 The Roots of a Function\nExcept for some very special functions, it is not possible to find an analytic expression for the root/roots.\nA commonly arising function is the polynomial function of the form:\n\\[\nP_{n}(x)=a_0+a_1x+a_2x^2+\\cdots+a_nx^n\n\\]\nWe know that the roots of the quadratic function \\(P_2(x)=a_0+a_1x+a_2x^2\\) can be obtained analytically by the expression:\n\\[\nx=\\frac{-a_1 \\pm \\sqrt{a_1^2-4a_0a_2}}{2a_0}\n\\]\nUnfortunately, such analytical formulas for the roots do not exist for polynomials of degree 5 or greater. Thus, most computational methods for root-finding are iterative in nature.\nIn general, the solution to a physical problem can often be expressed as the root of a suitable function, and we will see different methods of how to solve such problems.\nBut first, some examples.\n\n7.1.2 Example: Loan repayments\nA loan has an initial amount \\(P\\), a monthly interest rate \\(r\\), a duration of \\(N\\) months and a monthly repayment of \\(A\\). The remaining debt after \\(n\\) months is given by \\(P_n\\):\n\\[\nP_0=P\n\\] \\[\nP_{n+1}=P_n(1+r)-A\n\\]\nAfter iterative substitutions, we obtain:\n\\[\n\\begin{split}\nP_n & = P(1+r)^n-A\\sum_{i=0}^{n-1}(1+r)^i \\\\\n& = P(1+r)^n-A[(1+r)^n-1]/r\n\\end{split}\n\\]\nAt the end of the period (\\(n=N\\)), we know that \\(P_n=0\\) (we have repaid our loan), we get:\n\\[\n\\frac{A}{P}=\\frac{r(1+r)^N}{(1+r)^N-1}\n\\]\nKnowing the loan’s amount \\(P\\), the number of months \\(N\\) and the monthly repayment \\(A\\), what is the interest rate \\(r\\) that we pay? The answer is the solution of \\(f(r)=0\\), where:\n\\[\nf(r)=\\frac{A}{P}=\\frac{r(1+r)^N}{(1+r)^N-1}\n\\]\nWhich is not available in closed form.\n\nf &lt;- function(dR, dA, dP, iN) {\n  dOut = dA/dP - (dR * (1.0 + dR)^iN)/((1.0 + dR)^iN - 1)\n  return(dOut)\n}\n\nvR &lt;- seq(1e-5, 0.2, 0.001)\n\nplot(vR, f(vR, dA = 10, dP = 100, iN = 20), type = \"l\")\nabline(h = 0, col = \"red\")\n\n\n\n\n\n\n\nOther examples:\n\nEstimation of Econometric Models\nProblems in Finance\nPhysics (area minimization, shooting problems, orbital motions, …)\n…",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Root-finding</span>"
    ]
  },
  {
    "objectID": "rootfinding.html#numerical-methods-for-root-finding",
    "href": "rootfinding.html#numerical-methods-for-root-finding",
    "title": "\n7  Root-finding\n",
    "section": "\n7.2 Numerical Methods for Root-finding",
    "text": "7.2 Numerical Methods for Root-finding\nAs suggested above, when an analytical expression is not available for the root of some function, \\(f\\), i.e., the solution to \\(f(x)=0\\), we have to use numerical methods that are iterative in nature.\nThe idea behind an iterative method is the following:\nStarting with an initial approximation, \\(x_0\\), construct a sequence of iterates \\(\\{ x_n \\}\\) using an iteration formula with hope that this sequence converges to a root of \\(f(x)=0\\).\nAs concrete examples of such numerical methods, we will consider the Newton-Raphson method, the secant method and the bisection method.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Root-finding</span>"
    ]
  },
  {
    "objectID": "rootfinding.html#the-newton-raphson-method",
    "href": "rootfinding.html#the-newton-raphson-method",
    "title": "\n7  Root-finding\n",
    "section": "\n7.3 The Newton-Raphson Method",
    "text": "7.3 The Newton-Raphson Method\nThe Newton-Raphson method is a root-finding algorithm that uses an iterative process to approach one root of a funtion.\n\nSuppose the function \\(f\\) is differentiable with continuous derivative \\(f'\\) and a root \\(a\\).\nLet \\(x_0 \\in \\mathbb{R}\\) and think of \\(x_0\\) as our current ‘guess’ at \\(a\\).\nNow, the straight line through the point \\((x_0,f(x_0))\\) with slope \\(f'(x_0)\\) is the best straight approximation to the function \\(f(x)\\) at the point \\(x_0\\).\nThe equation of this straight line is given by \\(y=f(x_0)+f'(x_0)(x-x_0)\\)\nThis line crosses the \\(x\\)-axis at a point \\(x_1\\), which should be a better approximation than \\(x_0\\) to \\(a\\).\n\nWe find \\(x_1\\) by solving the equation for the straight line for \\(x\\) with \\(y=0\\):\n\\[\nx_1=x_0-\\frac{f(x_0)}{f'(x_0)}\n\\] i.e., we update our best guess to be \\(x_1\\) which is obtained from \\(x_0\\) by subtracting a correction term \\(f(x_0)/f'(x_0)\\).\n\n\nNow that we have \\(x_1\\), we use the same procedure to get the next best guess:\n\\[\nx_2=x_1-\\frac{f(x_1)}{f'(x_1)}\n\\]\n\n\n[] (source: https://commons.wikimedia.org/wiki/File:NewtonIteration_Ani.gif)\n\n7.3.1 Notes for the Newton-Raphson method\nIt can be shown that if \\(f\\) is ‘well-behaved’ at \\(a\\) (in particular, if \\(f'(a) \\ne 0\\) and \\(f''\\) is finite and continuous at \\(a\\)) and you start with \\(x_0\\) ‘close enough’ to \\(a\\), then \\(x_n\\) will converge to \\(a\\) quickly.\n\nUnfortunately, we don’t know if \\(f\\) is well behaved at \\(a\\) until we know \\(a\\), and we don’t know beforehand how close is close enough.\n\nSince we are expecting \\(f(x) \\rightarrow 0\\), a good stopping condition for the Newton-Raphson algorithm is \\(|f(x_n)| \\le \\varepsilon\\) for some tolerance \\(\\varepsilon\\).\nWe also always want a hard limit on the maximum number of iterations, \\(n_{max}\\), in case we get stuck in an infinite loop.\nImplementing the Newton-Raphson method in R is straightforward; we just need the function \\(f\\), its derivative \\(f'\\), an initial guess \\(x_0\\), and the two stoppoing criteria \\(\\varepsilon\\) (the convergence tolerance) and \\(n_{max}\\) (the maximum number of iterations).\n\n7.3.2 The Newton-Raphson algorithm\nThe Newton-Raphson algorithm proceeds as follows:\n\nDefine inputs: \\(f(x)\\), \\(x_0\\), \\(\\varepsilon\\), \\(n_{max}\\). For \\(n=0,1,...\\) until stopping condition reached, do:\nCompute \\(f(x_n)\\) and \\(f'(x_n)\\).\nCompute \\(x_{n+1}=x_n-\\frac{f(x_n)}{f'(x_n)}\\)\n\nStopping conditions:\n\nIf \\(|f(x_n)| \\le \\varepsilon\\) then set \\(a=x_n\\) and stop; Algorithm converged.\nIf \\(n=n_{max}\\) then stop; Maximum number of iterations has been reached, algorithm failed to converge.\n\n\n\n7.3.3 Drawbacks of the Newton-Raphson method\nThe Newton-Raphson root-finding method works by producing a sequence of guesses to the root and, under favourable circumstances, converges rapidly to the root from an initial guess.\n\nUnfortunately, it cannot be guaranteed to work.\n\nIt can be a costly task to calculate the required derivative \\(f'(x_n)\\) for every iteration, when we are not able to analytically derive it.\nInstability: if the iteration hits a value \\(x_n\\) where the function is close to flat (i.e., \\(f'(x_n)\\) close to zero), the iteration is sent far away from the current point.\nNo guarantee of convergence: the iteration might get caught up in an infinite loop.\nIn general, we need to pick a starting point relatively close to the desired root (could be more than one).\n[]\n\n7.3.4 Loan example\nConsider the loan repayments problem with objective function:\n\\[\nf(x) = \\frac{A}{P}-\\frac{x(1+x)^N}{(1+x)^N-1}\n\\]\nand first derivative:\n\\[\nf'(x) = \\frac{[(1+x)^N+Nx(1+x)^{N-1}][(1+x)^N-1]-Nx(1+x)^{2N-1}}{[(1+x)^N-1]^2}\n\\]\n\nf &lt;- function(dX, dA, dP, iN) {\n  dOut = dA/dP - (dX * (1.0 + dX)^iN)/((1.0 + dX)^iN - 1)\n  return(dOut)\n}\n\nf_prime &lt;- function(dX, dA, dP, iN) {\n  dNum1 = ((1.0 + dX)^iN + iN * dX * (1.0 + dX)^(iN - 1)) * ((1.0 + dX)^iN - 1)\n  dNum2 = iN * dX * (1.0 + dX)^(2 * iN - 1)\n  dDen = ((1.0 + dX)^iN - 1)^2\n  dOut = -(dNum1 - dNum2)/dDen\n  return(dOut)\n}\n\nNR &lt;- function(f, f_prime, dX0, dTol = 1e-9, max.iter = 1000, ...) {\n  dX &lt;- dX0\n  fx &lt;- f(dX, ...)\n  iter &lt;- 0\n  while ((abs(fx) &gt; dTol) && (iter &lt; max.iter)) {\n    dX &lt;- dX - f(dX, ...)/f_prime(dX, ...)\n    fx &lt;- f(dX, ...)\n    iter &lt;- iter + 1\n    cat(\"At iteration \", iter, \"value of x is: \", dX, \"\\n\")\n  }\n  if (abs(fx) &gt; dTol) {\n    cat(\"Algorithm failed to converge\\n\")\n    return(NULL)\n  } else {\n    cat(\"Algorithm converged\\n\")\n    return(dX)\n  }\n}\n\nNR(f, f_prime, dX0 = 0.15, dA = 10, dP = 100, iN = 20)\n#&gt; At iteration  1 value of x is:  0.08241672 \n#&gt; At iteration  2 value of x is:  0.07758319 \n#&gt; At iteration  3 value of x is:  0.0775469 \n#&gt; At iteration  4 value of x is:  0.0775469 \n#&gt; Algorithm converged\n#&gt; [1] 0.0775469\n\n\n#&gt; [1] -3.534942e-09",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Root-finding</span>"
    ]
  },
  {
    "objectID": "rootfinding.html#the-secant-method",
    "href": "rootfinding.html#the-secant-method",
    "title": "\n7  Root-finding\n",
    "section": "\n7.4 The Secant method",
    "text": "7.4 The Secant method\nIf the derivative \\(f'\\) does not exist (or we don’t want to compute it), then we can use the secant method, which only requires that the function \\(f\\) is continuous.\nSuppose \\(f\\) has a root \\(a\\). For this method, we assume that we have two current ‘guesses’, \\(x_0\\) and \\(x_1\\), for the value of \\(a\\).\nWe will think of \\(x_0\\) as an older guess and we want to replace the pair \\(x_0,x_1\\) by the pair \\(x_1,x_2\\), where \\(x_2\\) is a new guess.\nTo find a new guess \\(x_2\\), we first draw the straight line from \\((x_0,f(x_0))\\) to \\((x_1,f(x_1))\\), i.e., the secant of the curve \\(y=f(x)\\).\nThe equation of the secant is:\n\\[\ny=f(x_1) + \\frac{f(x_0)-f(x_1)}{x_0-x_1}(x-x_1)\n\\]\nAs the new guess, we will use the \\(x\\)-coordinate \\(x_2\\) of the point at which the secant crosses the \\(x\\)-axis, and so \\(x_2\\) can be found from:\n\\[\n0 = f(x_1)+\\frac{f(x_0)-f(x_1)}{x_0-x_1}(x-x_1)\n\\]\nWhich implies:\n\\[\nx_2=x_1-f(x_1) \\frac{x_0-x_1}{f(x_0)-f(x_1)}\n\\]\nRepeating this, we get a second-order recurrence relation (each new value depends on the previous two):\n\\[\nx_{n+1}=x_n-f(x_n) \\frac{x_{n-1}-x_n}{f(x_{n-1})-f(x_n)}\n\\]\nNote that if \\(x_n\\) and \\(x_{n-1}\\) are cloes together, then:\n\\[\n\\frac{1}{f'(x_n)} \\approx \\frac{x_{n-1}-x_n}{f(x_{n-1})-f(x_n)}\n\\]\nAnd we can thus view the secant method as an approximation of the Newton-Raphson method.\nSimilar convergence properties to the Newton-Raphson method:\n\nIf \\(f\\) is well behaved at \\(a\\) and you start with \\(x_0\\) and \\(x_1\\) sufficiently close to \\(a\\), then \\(x_n\\) will converge quickly to \\(a\\), though not quite as fast as the NR method.\nWe cannot guarantee convergence.\nTrade-off compared to NR: we no longer need to know \\(f'\\) but in return we give up some speed and have to provide two initial points, \\(x_0\\) and \\(x_1\\).",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Root-finding</span>"
    ]
  },
  {
    "objectID": "rootfinding.html#the-bisection-method",
    "href": "rootfinding.html#the-bisection-method",
    "title": "\n7  Root-finding\n",
    "section": "\n7.5 The bisection method",
    "text": "7.5 The bisection method\nA more reliable but slower approach is root-bracketing, which works by first isolating an interval in which the root must lie, and then successively refining the bounding interval in such a way that the root is guaranteed to always lie inside the interval.\nThe canonical example is the bisection method, in which the width of the bounding interval is successively halved.\nSuppose that \\(f\\) is a continuous function, then it is easy to see that \\(f\\) has at least one root in the interval \\((x_l,x_r)\\) if \\(f(x_l)f(x_r)&lt;0\\) (i.e., if either \\(f(x_l)&lt;0\\) and \\(f(x_r)&gt;0\\) or \\(f(x_l)&gt;0\\) and \\(f(x_r)&lt;0\\)).\nThe bisection method works by taking an interval \\((x_l,x_r)\\) that contains a root, the successively refining \\(x_l\\) and \\(x_r\\) until \\(x_r-x_l \\le \\varepsilon\\), where \\(\\varepsilon\\) is some predefined tolerance.\nThe algorithm proceeds as follows:\n\nStart with \\(x_l&lt;x_r\\) such that \\(f(x_l)f(x_r)&lt;0\\)\nIf \\(x_r-x_l \\le \\varepsilon\\) then stop; set \\(a=x_m=\\frac{x_r-x_l}{2}\\)\nPut \\(x_m=(x_l+x_r)/2\\); if \\(f(x_m)=0\\) then stop.\nIf \\(f(x_l)f(x_m)&lt;0\\) then put \\(x_r=x_m\\); otherwise put \\(x_l=x_m\\).\nGo back to step 1.\n\n[] (source: https://upload.wikimedia.org/wikipedia/commons/f/f0/Bisektion_Ani.gif)\nNote that at every iteration of the algorithm, we know that there is a root in the interval \\((x_l,x_r)\\). Hence, provided we start with \\(f(x_l)f(x_r)&lt;0\\), the algorithm is guaranteed to converge and we do not need to put a bound on the maximum number of iterations.\nIf we stop when \\(x_r-x_l \\le \\varepsilon\\), then we know that both \\(x_l\\) and \\(x_r\\) are within distance \\(\\varepsilon\\) of a root.\nThe bisection method converges very slowly…\nThe most popular current root-finding methods use root-bracketing to get close to a root, then switch over to the Newton-Raphson or secant method when it seems safe to do so. This strategy combines the safety of bisection with the speed of the secant method.\n\n7.5.1 The bisection method: Implementation in R\n\nbisection &lt;- function(f, dX.l, dX.r, dTol = 1e-9, max.iter = 1000, ...) {\n  \n  #check inputs\n  if (dX.l &gt;= dX.r) {\n    cat(\"error: x.l &gt;= x.r \\n\")\n    return(NULL)\n  }\n  f.l &lt;- f(dX.l, ...)\n  f.r &lt;- f(dX.r, ...)\n  if (f.l == 0) {\n    return(dX.l)\n  } else if (f.r == 0) {\n    return(dX.r)\n  } else if (f.l*f.r &gt; 0) {\n    cat(\"error: f(x.l)*f(x.r) &gt; 0 \\n\")\n    return(NULL)\n  }\n  \n  # successively refine x.l and x.r\n  iter &lt;- 0\n  while ((dX.r - dX.l) &gt; dTol && (iter &lt; max.iter)) {\n    dX.m &lt;- (dX.l + dX.r)/2\n    f.m &lt;- f(dX.m, ...)\n    if (f.m == 0) {\n      return(dX.m)\n    } else if (f.l*f.m &lt; 0) {\n      dX.r &lt;- dX.m\n      f.r &lt;- f.m\n    } else {\n      dX.l &lt;- dX.m\n      f.l &lt;- f.m\n    }\n    iter &lt;- iter + 1\n    cat(\"at iteration\", iter, \"the root lies between\", dX.l, \"and\", dX.r, \"\\n\")\n  }\n  \n  # return approximate root\n  return((dX.l + dX.r)/2)\n}\n\nLet again \\(f\\) be the loan repayment function.\n\nbisection(f, dX.l = 1e-6 , dX.r = 0.2, dTol = 1e-5, dA = 10, dP = 100, iN = 20)\n#&gt; at iteration 1 the root lies between 1e-06 and 0.1000005 \n#&gt; at iteration 2 the root lies between 0.05000075 and 0.1000005 \n#&gt; at iteration 3 the root lies between 0.07500063 and 0.1000005 \n#&gt; at iteration 4 the root lies between 0.07500063 and 0.08750056 \n#&gt; at iteration 5 the root lies between 0.07500063 and 0.08125059 \n#&gt; at iteration 6 the root lies between 0.07500063 and 0.07812561 \n#&gt; at iteration 7 the root lies between 0.07656312 and 0.07812561 \n#&gt; at iteration 8 the root lies between 0.07734436 and 0.07812561 \n#&gt; at iteration 9 the root lies between 0.07734436 and 0.07773499 \n#&gt; at iteration 10 the root lies between 0.07753967 and 0.07773499 \n#&gt; at iteration 11 the root lies between 0.07753967 and 0.07763733 \n#&gt; at iteration 12 the root lies between 0.07753967 and 0.0775885 \n#&gt; at iteration 13 the root lies between 0.07753967 and 0.07756409 \n#&gt; at iteration 14 the root lies between 0.07753967 and 0.07755188 \n#&gt; at iteration 15 the root lies between 0.07754578 and 0.07755188\n#&gt; [1] 0.07754883",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Root-finding</span>"
    ]
  },
  {
    "objectID": "rootfinding.html#the-unirootfunction",
    "href": "rootfinding.html#the-unirootfunction",
    "title": "\n7  Root-finding\n",
    "section": "\n7.6 The unirootfunction",
    "text": "7.6 The unirootfunction\nR implements a general purposed unit root finder with the function uniroot.\nuniroot is based on the Brent’s method, which is a root-finding algorithm that combines the bisection method, the secant method and a third method called the inverse quadratic interpolation method.1\nThe function uniroot searches the interval from lower to upper for a root (i.e., zero) of the function \\(f\\) with respect to its first argument. The function is given by:\n\nuniroot(f, interval, ..., lower = min(interval), upper = max(interval), tol = .Machine$double.eps^0.25, maxiter = 1000, trace = 0)\n\n\n7.6.1 Example for the loan repayment function\n\nuniroot(f, lower = 1e-6, upper = 0.2, dA = 10, dP = 100, iN = 20)\n#&gt; $root\n#&gt; [1] 0.07752483\n#&gt; \n#&gt; $f.root\n#&gt; [1] 1.659286e-05\n#&gt; \n#&gt; $iter\n#&gt; [1] 4\n#&gt; \n#&gt; $init.it\n#&gt; [1] NA\n#&gt; \n#&gt; $estim.prec\n#&gt; [1] 6.103516e-05",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Root-finding</span>"
    ]
  },
  {
    "objectID": "rootfinding.html#numerical-derivatives-in-r",
    "href": "rootfinding.html#numerical-derivatives-in-r",
    "title": "\n7  Root-finding\n",
    "section": "\n7.7 Numerical Derivatives in R",
    "text": "7.7 Numerical Derivatives in R\nIn order to implement the Newton-Raphson method as well as other numerical algorithms, the use of the derivative of the objective function is required.\nSometimes, we are not able to analytically derive the derivative or we are just too lazy to do that. In these cases, the derivative can be computed numerically.\nIn R, there are several routines to evaluate numerical derivatives. One of the most used is the grad function in the numDeriv package of Gilbert and Varadhan (2016).2\ngrad calculates a numerical approximation of the first derivative of a function at the point x. Its formulation is:\n\ngrad(func, x, method = \"Richardson\", side = NULL, method.args=list(), ...)\n\nWhere:\n\nfunc is a function with a scalar real result.\nx is a real scalar or vector argument to func, indicating the point(s) at which the gradient is to be calculated.\nmethod is one of Richardson, simple or complex indicating the method to use for the approximation.\n\nIn order to check that our implementation of f_prime in the loans problem is correct, we can test it using grad:\n\ngrad(f, 0.2, dA = 10, dP = 100, iN = 20)\n#&gt; [1] -0.9351161\n\nf_prime(0.2, dA = 10, dP = 100, iN = 20)\n#&gt; [1] -0.9351161\n\nabs(grad(f, 0.2, dA = 10, dP = 100, iN = 20) - f_prime(0.2, dA = 10, dP = 100, iN = 20))\n#&gt; [1] 1.055467e-11\n\n\n7.7.1 Warnings\nBeware that:\n\nNumerical derivatives are not always precise.\nThe computational cost generally increases considerably.\n\n\nlibrary(microbenchmark)\n#&gt; Warning: pakke 'microbenchmark' blev bygget under R version 4.3.3\nmicrobenchmark(\n  grad(f, 0.2, dA = 10, dP = 100, iN = 20),\n  f_prime(0.2, dA = 10, dP = 100, iN = 20)\n)\n#&gt; Unit: microseconds\n#&gt;                                      expr  min    lq   mean median    uq   max\n#&gt;  grad(f, 0.2, dA = 10, dP = 100, iN = 20) 35.8 37.15 45.771   43.8 45.45 193.6\n#&gt;  f_prime(0.2, dA = 10, dP = 100, iN = 20)  1.1  1.40  1.825    1.5  1.95  14.3\n#&gt;  neval\n#&gt;    100\n#&gt;    100",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Root-finding</span>"
    ]
  },
  {
    "objectID": "rootfinding.html#footnotes",
    "href": "rootfinding.html#footnotes",
    "title": "\n7  Root-finding\n",
    "section": "",
    "text": "Brent, R. (1973) Algorithms for Minimization without Derivatives. Englewood Cliffs, NJ: Prentice-Hall.↩︎\nPaul Gilbert and Ravi Varadhan (2016), numDeriv: Accurate Numerical Derivatives. R package version 2016.8-1. https://CRAN.R-project.org/package=numDeriv↩︎",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Root-finding</span>"
    ]
  },
  {
    "objectID": "numericaloptimization1.html",
    "href": "numericaloptimization1.html",
    "title": "\n8  Numerical Optimization 1\n",
    "section": "",
    "text": "8.1 Numerical vs Analytical Optimization\nFirst, recall that any maximization problem can be converted into a minimization problem by multiplying the function by -1.\nMost economic, econometric, and statistical problems can be solved by maximizing/minimizing an objective function.\nIn some cases, the optimal points can be found analytically, but in most cases a closed form solution is not available. In these cases, we need numerical optimization methods.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Numerical Optimization 1</span>"
    ]
  },
  {
    "objectID": "numericaloptimization1.html#numerical-vs-analytical-optimization",
    "href": "numericaloptimization1.html#numerical-vs-analytical-optimization",
    "title": "\n8  Numerical Optimization 1\n",
    "section": "",
    "text": "Maximum likelihood estimation\nGMM\nPortfolio allocation\n…\n\n\n\n8.1.1 Optimization and Root-finding\nRoot-finding and optimization are very closely related subjects which often occur in practical applications.\nIn fact, as you all know, optimizing a function, e.g., \\(f(x)=\\frac{x^3}{3}-x\\) is equivalent to finding the roots of its derivative, \\(f'(x)=x^2-1\\).\n\n\n\n\n\n\n\n\nTherefore, the methods used for numerical optimization are closely related to the numerical root-finding methods that were seen in the last lecture.\n\n8.1.2 Maximum: Definition\nIn one dimension, we suppose that we have a function \\(f : \\mathbb{R} \\rightarrow \\mathbb{R}\\) with continuous first and second derivatives.\nThe function \\(f\\) has a global maximum at \\(x^*\\) if \\(f(x) \\le f(x^*)\\) for all \\(x\\).\nThe function \\(f\\) has a local maximum at \\(x^*\\) if \\(f(x) \\le f(x^*)\\) for all \\(x\\) in a neighborhood of \\(x^*\\),\nA necessary condition for \\(x^*\\) to be a local maximum of \\(f\\) is \\(f'(x^*)=0\\) and \\(f''(x^*) \\le 0\\), a sufficient condition is \\(f'(x^*)=0\\) and \\(f''(x^*)&lt;0\\).\n\n8.1.3 Numerical optimization: local search techniques\nClearly, finding a local maximum is much easier than finding a global maximum.\nAll of the algorithms we consider for numerical optimization are local search techniques.\nThese algorithms work by generating a series of points, \\(x_0\\), \\(x_1\\), \\(x_2\\), \\(\\cdots\\), which (hopefully, but not necessarily) converge to a local maximum of \\(f\\).\nGiven a prospective solution \\(x_n\\), we look for the next prospective solution \\(x_{n+1}\\) in some neighborhood of \\(x_n\\).\nBecause they never consider the whole space of possible solutions, local search techniques can only ever be guaranteed to find local maxima.\n\n8.1.4 Numerical optimization: general structure\nThese search techniques are iterative procedures with the following general structure.\n\nChose a starting point, \\(x_0\\);\nCheck for optimality in \\(x_0\\);\nA series of instructions to select another candidate of an optimal point;\nBack to point 2 until an optimality criterion is satisfied;\nFinal point, \\(x_N\\), is an approximation of the local optimal point, \\(x^*\\), for the function \\(f(\\cdot)\\).\n\nA serach method is said to be globally convergent if it can reach a stationary point starting from any initial point. Otherwise, the method is said to be locally convergent.\n\n8.1.5 Stopping criteria\nAll algorithms require a stopping ceriteria to avoid that the routines keep running forever. The purpose is:\n\nLimit the number of iterations.\nObtain an accurate solution.\n\nTypical stopping criteria are:\n\n\\(|x_n-x_{n-1}| \\le \\varepsilon\\), with \\(\\varepsilon &gt;0\\);\n\\(|f(x_n)-f(x_{n-1})| \\le \\varepsilon\\), with \\(\\varepsilon &gt;0\\);\n\\(|f(x_n)| \\le \\varepsilon\\), with \\(\\varepsilon &gt;0\\);\nMaximum number of iterations reached, \\(n=n_{max}\\).\n\nIf the sequence \\(\\{x_n\\}_{n=1}^{\\infty}\\) converges to a local maximum, the criteria 1-3 will all be satisfied, but the converse is not necessarily true. Thus, even when a local search technique appears to converge, we may still need to check that the final solution really is a local maximum.\nWe may find that the algorithm does not converge at all; e.g., if \\(f\\) is unbounded then possibly \\(x_n \\rightarrow \\infty\\). This is the reason that we need to specify a maximum number of iterations \\(n_{max}\\).",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Numerical Optimization 1</span>"
    ]
  },
  {
    "objectID": "numericaloptimization1.html#the-newton-raphson-method---and-method-for-optimization.",
    "href": "numericaloptimization1.html#the-newton-raphson-method---and-method-for-optimization.",
    "title": "\n8  Numerical Optimization 1\n",
    "section": "\n8.2 The Newton-Raphson Method - and Method for Optimization.",
    "text": "8.2 The Newton-Raphson Method - and Method for Optimization.\nAs we saw last time, the Newton-Raphson method uses information on the function \\(f\\) and its derivative \\(f'\\). The original formulation of the NR method is intended to find the roots of a function, and it is based on the following recursive algorithm:\n\nDefine inputs: \\(f(x), x_0, \\varepsilon, n_{max}\\). For \\(n=0,1,...\\) until stopping condition reached, do:\nCompute \\(f(x_n)\\) and \\(f'(x_n)\\).\nCompute \\(x_{n+1}=x_n-\\frac{f(x_n)}{f'(x_n)}\\).\n\nStopping conditions:\n\nIf \\(|f(x_n)| \\le \\varepsilon\\) then set \\(x_n=a\\) and stop; algorithm converged.\nIf \\(n=m_{max}\\) then stop; maximum number of iterations has been reached, algorithm failed to converge.\n\n\n\n\n8.2.1 Newton’s Method for Optimization\nSince we deal with a problem of optimization, i.e., \\(f'(x)=0\\), Newton’s method for optimization replaces \\(f(x)\\) with \\(f'(x)\\) in the original Newton-Raphson method.\nHence, the \\(n\\)-th step in Newton’s method for optimization (for univariate functions) is:\n\\[\nx_n = x_{n-1} - \\frac{f'(x_{n-1})}{f''(x_{n-1})}\n\\]\nThe Newton’s method is such that the convergence is local and quadratic, meaning that near the solution, the convergence is very fast!\nThe Newton’s method is really suitable when the first- and second-order information are readily and easily calculated.\n\n8.2.2 Newton’s algorithm convergence\nWhen the Newton algorithm converges, we can end up with a minimum, or indeed a ‘flat spot’, just as easily as a maximum.\nThe reason is that all such stationary points satisfy \\(f'(x^*)=0\\).\nIt can be shown that if:\n\n\\(x^*\\) is a local maximum.\n\\(f'(x^*)=0\\)\n\\(f''(x^*)&lt;0\\)\n\\(\\exists k \\in \\mathbb{R} : |f''(x) - f''(y)| \\le k|x-y| \\forall (x,y) \\in \\mathbb{R}^2\\) (that is, \\(f''\\) is Lipschitz-continuous) in a neighborhood of \\(x^*\\) then, provided that \\(x_0\\) is close enough to \\(x^*\\), \\(x_n \\rightarrow x^*\\) quickly as \\(n \\rightarrow \\infty\\).\n\nWe will revisit Newton’s method later in higher dimensions.\n\n8.2.3 Newton’s method: Example\nConsider the function \\(f(x)=2x(x-1)^2(x+2)\\)\n\nf &lt;- function(dX) {\n  dOut &lt;- 2 * dX * (dX - 1)^2 * (dX + 2)\n  return(dOut)\n}\n\nwith first derivative \\(f'(x)=4-12x+8x^3\\)\n\nf_prime &lt;- function(dX) {\n  dOut &lt;- 4 - 12 * dX + 8 * dX^3\n  return(dOut)\n}\n\nand second derivative \\(f''(x)=24x^2-12\\)\n\nf_second &lt;- function(dX) {\n  dOut &lt;- 24 * dX^2 - 12\n  return(dOut)\n}\nvX &lt;- seq(-2, 2, 0.1)\nplot(vX, f(vX), type = \"l\")\n\n\n\n\n\n\n\n\n8.2.4 Newton’s method: implementation in R\n\nNM &lt;- function(f, f_prime, f_sec, dX0, dTol = 1e-9, n.max = 1000){\n  dX &lt;- dX0\n  fx &lt;- f(dX)\n  fpx &lt;- f_prime(dX)\n  fsx &lt;- f_sec(dX)\n  n &lt;- 0\n  while ((abs(fpx) &gt; dTol) && (n &lt; n.max)) {\n    dX &lt;- dX - fpx/fsx\n    fx &lt;- f(dX)\n    fpx &lt;- f_prime(dX)\n    fsx &lt;- f_sec(dX)\n    n &lt;- n + 1\n    cat(\"At iteration\", n, \"the value of x is:\", dX, \"\\n\")\n  }\n  if (n == n.max) {\n    cat('newton failed to converge\\n')\n  } else {\n    return(dX)\n  }\n}\n\nExample: starting point \\(x_0 = -2\\):\n\nx1 &lt;- NM(f, f_prime, f_second, dX0 = -2)\n#&gt; At iteration 1 the value of x is: -1.571429 \n#&gt; At iteration 2 the value of x is: -1.398224 \n#&gt; At iteration 3 the value of x is: -1.367014 \n#&gt; At iteration 4 the value of x is: -1.366026 \n#&gt; At iteration 5 the value of x is: -1.366025\nvX &lt;- seq(-2, 2, 0.1)\nplot(vX, f(vX), type = \"l\")\nabline(v=-2, col=\"blue\") # initial guess\nabline(v=-1.571429, lty=2) # first iteration\nabline(v=-1.398224, lty=2) # second iteration\nabline(v=-1.367014 , lty=2) # third iteration\nabline(v=x1, col=\"red\") # solution\n\n\n\n\n\n\n\nExample: starting point \\(x_0 = 1.5\\):\n\n\n\n\n\n\n\n\nExample: starting point \\(x_0 = -0.4\\):\n\n\n\n\n\n\n\n\n\n8.2.5 Exercise\nFind the local maximum and the local minimum of the function \\(f(x)=x^3+(6-x)^2\\) on the interval \\([-5,5]\\) using Newton’s method and the optimize function.\nPlot the function to pick starting points for Newton’s method.\n\n# Functions\nfe1 &lt;- function(dX) {\n  dOut &lt;- dX^3 + (6-dX)^2\n  return(dOut)\n}\nfeprime &lt;- function(dX) {\n  dOut &lt;- 3 * dX^2 - 2*(6-dX) \n  return(dOut)\n}\nfesecond &lt;- function(dX) {\n  dOut &lt;- 6 * dX + 2\n  return(dOut)\n}\n#Solution\nxe1 &lt;- NM(fe1, feprime, fesecond, dX0 = -1) # minimum\nvX &lt;- seq(-5, 5, 0.1)\nplot(vX, fe1(vX), type = \"l\")\nabline(v=-1, col=\"blue\") # initial guess\nabline(v=xe1, col=\"red\") # solution\n\nxe2 &lt;- NM(fe1, feprime, fesecond, dX0 = 4) # maximum\nvX &lt;- seq(-5, 5, 0.1)\nplot(vX, fe1(vX), type = \"l\")\nabline(v=4, col=\"blue\") # initial guess\nabline(v=xe2, col=\"red\") # solution",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Numerical Optimization 1</span>"
    ]
  },
  {
    "objectID": "numericaloptimization1.html#the-golden-section-method",
    "href": "numericaloptimization1.html#the-golden-section-method",
    "title": "\n8  Numerical Optimization 1\n",
    "section": "\n8.3 The Golden-section method",
    "text": "8.3 The Golden-section method\nThe golden-section method works in one dimension only, but does not need \\(f'\\).\nThis method is similar to the root-bracketing technique for root-finding.\nLet \\(f : \\mathbb{R} \\rightarrow \\mathbb{R}\\) be a continuous function (note, that we do not assume that we have a derivative).\nIf we have two points \\(x_l &lt; x_r\\) such that \\(f(x_l)f(x_r) \\le 0\\) then we know that there is a root in the interval \\([x_l, x_r]\\).\nTo determine if we have a local maximum, we need three points: if \\(x_l &lt; x_m &lt; x_r\\) and \\(f(x_l) \\le f(x_m)\\) and \\(f(x_r) \\le f(x_m)\\) then there must be a local maximum in the interval \\([x_l, x_r]\\).\n\n8.3.1 The idea of the Golden-section method\n\nStart with \\(x_l &lt; x_m &lt; x_r\\) such that \\(f(x_l) \\le f(x_m)\\) and \\(f(x_r) \\le f(x_m)\\).\nIf \\(x_r - x_l \\le \\varepsilon\\) then stop; set \\(x^*=x_m\\).\nIf \\(x_r - x_m &gt; x_m - x_l\\) then 2a.; otherwise do 2b.\n\n2a) Choose a point \\(y \\in (x_m, x_r)\\)\n\nIf \\(f(y) \\ge f(x_m)\\) then the maximum is \\((x_m,x_r)\\) and we can redefine the interval: put \\(x_l=x_m\\) and \\(x_m = y\\);\nOtherwise the maximum is in the interval \\((x_l,y)\\) and we can redefine the interval: \\(x_r=y\\).\n\n2b) Choose a point \\(y \\in (x_l, x_m)\\)\n\nIf \\(f(y) \\ge f(x_m)\\) then the maximum is \\((x_l,x_m)\\) and we can redefine the interval: put \\(x_r=x_m\\) and \\(x_m = y\\);\nOtherwise the maximum is in the interval \\((y,x_r)\\) and we can redefine the interval: \\(x_l=y\\).\n\n\nGo back to step 1.\n\n8.3.2 The Golden ratio\nOther than saying that \\(y\\) should be in the larger of the two intervals, \\((x_l,x_m)\\) and \\((x_m,x_r)\\), how do we specify \\(y\\)?\nSuppose \\((x_m,x_r)\\) is the larger interval (as in the figure below), and let \\(a=x_m-x_l\\), \\(b=x_r-x_m\\), and \\(c=y-x_m\\).\n[]\nThe golden-section algortihm chooses \\(y\\) such that the ratio of the lengths of the larger to the smaller interval stays the same at each iteration.\nThat is, if the new breacketing interval is \\([x_l,y]\\) then:\n\\[\n\\frac{a}{c} = \\frac{b}{a}\n\\]\nWhile if the new bracketing interval is \\([x_m,x_r]\\) then:\n\\[\n\\frac{b-c}{c} = \\frac{b}{a}\n\\]\nSetting \\(\\phi = \\frac{b}{a}\\) and using the two equations above to solve for \\(\\phi\\) and we can find the famous golden ratio:\n\\[\n\\phi^2 - \\phi - 1 = 0 \\Rightarrow \\phi = \\frac{1+\\sqrt{5}}{2} \\approx 1.618\n\\]\nWe then solve for \\(c\\) using the second equation:\n\\[\n\\frac{b-c}{c} = \\phi \\Rightarrow c=b/(1+\\phi)\n\\]\nChoose \\(y\\):\n\\[\ny = x_m + \\underbrace{c}_{y-x_m} + x_m + \\underbrace{b}_{x_r-x_m} / (1+\\phi) = x_m + (x_r - x_m)/(1+\\phi)\n\\]\nAn analogous argument applies if \\((x_l,x_m)\\) is the larger interval.\nUsing this method for choosing \\(y\\)gives the following version of the algorithm:",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Numerical Optimization 1</span>"
    ]
  },
  {
    "objectID": "numericaloptimization1.html#new-algorithm",
    "href": "numericaloptimization1.html#new-algorithm",
    "title": "\n8  Numerical Optimization 1\n",
    "section": "\n8.4 New algorithm",
    "text": "8.4 New algorithm\n\nStart with \\(x_l &lt; x_m &lt; x_r\\) such that \\(f(x_l) \\le f(x_m)\\) and \\(f(x_r) \\le f(x_m)\\).\nIf \\(x_r - x_l \\le \\varepsilon\\) then stop; set \\(x^*=x_m\\).\nIf \\(x_r - x_m &gt; x_m - x_l\\) then 2.i; otherwise do 2.ii.\n\n\nSet \\(y = x_m + (x_r-x_m)/(1+\\phi)\\). If \\(f(y) \\ge f(x_m)\\) then put \\(x_l=x_m\\) and \\(x_m = y\\); otherwise put \\(x_r = y\\);\nSet \\(y = x_m + (x_m-x_l)/(1+\\phi)\\). If \\(f(y) \\ge f(x_m)\\) then put \\(x_r=x_m\\) and \\(x_m = y\\); otherwise put \\(x_l = y\\);\n\n\nGo back to step 1.\n\nWith \\(\\phi = \\frac{1+\\sqrt{5}}{2}\\) being the famous golden ratio.\n\n8.4.1 Notes\nThe length ratio of the new interval to the old is either \\(b/(a+b)\\) or \\((a+c)/(a+b)\\) (see figure to verify graphically), which both work out as \\(\\phi / (1+\\phi)\\).\nThat implies that if we start with \\(x_m\\) chosen so that the ratio \\(\\underbrace{(x_r-x_m)}_b / \\underbrace{(x_m-x_l)}_a=\\phi\\) or \\(1/\\phi\\) then at each iteration the width of the bracketing interval is reduced by a constant factor of \\(\\phi / (1+\\phi)\\) and so must eventually go to zero.\nIf you do not start with the ratio \\((x_r-x_m) / (x_m-x_l)=\\phi\\) or \\(1/\\phi\\) it is not a problem because as soon as you have an iteration that puts \\(x_m=y\\), this will be the case (by definition of how \\(y\\) is chosen).\nSince \\((x_r-x_l) \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\), to stop the golden-section algorithm it is sufficient to specify a tolerance \\(\\varepsilon &gt; 0\\) then stop when \\(x_r-x_l \\le \\varepsilon\\).\n\n8.4.2 The golden-section method: R implementation\n\ngsection &lt;- function(f, dX.l, dX.r, dX.m, dTol = 1e-9) {\n  \n  # golden ratio plus one\n  dGR1 &lt;- 1 + (1 + sqrt(5))/2\n  \n  # successively refine x.l, x.r, and x.m\n  f.l &lt;- f(dX.l)\n  f.r &lt;- f(dX.r)\n  f.m &lt;- f(dX.m)\n  while ((dX.r - dX.l) &gt; dTol) { \n    if ((dX.r - dX.m) &gt; (dX.m - dX.l)) { # if the right segment is wider than the left \n      dY &lt;- dX.m + (dX.r - dX.m)/dGR1 # put Y into the right segment according to the golden ratio\n      f.y &lt;- f(dY)\n      if (f.y &gt;= f.m) {\n        dX.l &lt;- dX.m\n        f.l &lt;- f.m\n        dX.m &lt;- dY\n        f.m &lt;- f.y\n      } else {\n        dX.r &lt;- dY\n        f.r &lt;- f.y\n      }\n    } else { #if the left segment is wider than the right\n      dY &lt;- dX.m - (dX.m - dX.l)/dGR1 # put Y into the left segment according to the golden ratio\n      f.y &lt;- f(dY)\n      if (f.y &gt;= f.m) {\n        dX.r &lt;- dX.m\n        f.r &lt;- f.m\n        dX.m &lt;- dY\n        f.m &lt;- f.y\n      } else {\n        dX.l &lt;- dY\n        f.l &lt;- f.y\n      }\n    }\n  }\n  return(dX.m)\n}\ngsection(f, dX.l = -1, dX.r = 1, dX.m = 0.5)\n#&gt; [1] 0.3660254",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Numerical Optimization 1</span>"
    ]
  },
  {
    "objectID": "numericaloptimization1.html#optimization-in-r-univariate",
    "href": "numericaloptimization1.html#optimization-in-r-univariate",
    "title": "\n8  Numerical Optimization 1\n",
    "section": "\n8.5 Optimization in R: univariate",
    "text": "8.5 Optimization in R: univariate\nIn one dimension, R provides the function optimize, which uses a combination of the golden-section algorithm with a technique called parabolic interpolation.\n\noptimize(f, interval, ..., lower = min(interval), upper = max(interval), maximum = FALSE, tol = .Machine$double.eps^0.25)\n\nNote that since maximum = FALSE, by default, optimize minimizes \\(f\\).\n\noptimize(f, lower = -0.4, upper = 0.8, maximum = TRUE)\n#&gt; $maximum\n#&gt; [1] 0.3660215\n#&gt; \n#&gt; $objective\n#&gt; [1] 0.6961524\n\nFor the example:\n\n# Using built-in optimizer\noptimize(fe1, lower = -5, upper = 5, maximum = TRUE) # maximum\n#&gt; $maximum\n#&gt; [1] -2.360919\n#&gt; \n#&gt; $objective\n#&gt; [1] 56.74535\n\noptimize(fe1, lower = -5, upper = 5) # minimum\n#&gt; $minimum\n#&gt; [1] 1.694255\n#&gt; \n#&gt; $objective\n#&gt; [1] 23.4028\n\nf2 &lt;- function(x){-1*fe1(x)}\noptimize(f2, lower = -5, upper = 5)\n#&gt; $minimum\n#&gt; [1] -2.360919\n#&gt; \n#&gt; $objective\n#&gt; [1] -56.74535",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Numerical Optimization 1</span>"
    ]
  },
  {
    "objectID": "numericaloptimization2.html",
    "href": "numericaloptimization2.html",
    "title": "\n9  Numerical Optimization 2\n",
    "section": "",
    "text": "9.1 Multivariate Optimization\nWe now turn to the more useful but more difficult problem of finding local minima or maxima of a function of several variables, i.e., multivariate optimization.\nLet \\(f : \\mathbb{R}^d \\rightarrow \\mathbb{R}\\) and suppose that all of the first- and second-order partial derivatives of \\(f\\) exist and are continuous everywhere.\nDenote the i-th partial derivative at \\(\\mathbf{x}=(x_1,...,x_d)^\\intercal\\) as \\(f_i(\\mathbf{x})=\\frac{\\partial f(\\mathbf{x})}{\\partial x_i}\\) and define the gradient:\n\\[\n\\nabla f(\\mathbf{x}) = (f_1(\\mathbf{x}), f_2(\\mathbf{x}), \\dots, f_d(\\mathbf{x}))^{\\top},\n\\]\nAnd the Hessian:\n\\[\n\\mathbf{H}(\\mathbf{x}) =\n\\begin{pmatrix}\n\\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_1 \\partial x_1} & \\cdots & \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_1 \\partial x_d} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_d \\partial x_1} & \\cdots & \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_d \\partial x_d}\n\\end{pmatrix}\n\\]\nA necessary (but not sufficient) condition for a local maximum at \\(\\mathbf{x}\\) is \\(\\nabla f(\\mathbf{x})=\\mathbf{0}\\) and the Hessian is negative semi-definite.\nA sufficient (but not necessary) condition for a local maximum at \\(\\mathbf{x}\\) is \\(\\nabla f(\\mathbf{x})=\\mathbf{0}\\) and the Hessian is negative definite.\nAs in one dimension, we will use iterative local search techniques to find local maxima.\nDefine \\(||\\mathbf{x}||_\\infty = \\max_i{|x_i|}\\), In higher dimensions, we use stopping conditions that are combinations of the following:",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Numerical Optimization 2</span>"
    ]
  },
  {
    "objectID": "numericaloptimization2.html#multivariate-optimization",
    "href": "numericaloptimization2.html#multivariate-optimization",
    "title": "\n9  Numerical Optimization 2\n",
    "section": "",
    "text": "\\(\\|\\mathbf{x}_n - \\mathbf{x}_{n-1}\\|_{\\infty} \\leq \\varepsilon;\\)\n\\(|f(\\mathbf{x}_n) - f(\\mathbf{x}_{n-1})| \\leq \\varepsilon;\\)\n\\(\\|\\nabla f(\\mathbf{x}_n)\\|_{\\infty} \\leq \\varepsilon;\\)\nAnd to guard against non-convergence, we also specify a maximum number of iterations \\(n_{max}\\), then stop when \\(n=n_{max}\\).",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Numerical Optimization 2</span>"
    ]
  },
  {
    "objectID": "numericaloptimization2.html#steepest-ascent-method",
    "href": "numericaloptimization2.html#steepest-ascent-method",
    "title": "\n9  Numerical Optimization 2\n",
    "section": "\n9.2 Steepest ascent method",
    "text": "9.2 Steepest ascent method\nLet \\(f : \\mathbb{R}^d \\rightarrow \\mathbb{R}\\) be a function with continuous partial derivatives everywhere. We wish to find a local maximum of \\(f\\) in the neighborhood of some point \\(x_0\\).\nIn the steepest ascent method, we seek a local maximum by moving in the direction of the gradient, which is the direction with the largest slope, i.e., the steepest ascent. (The direction with the smallest slope at the point \\(\\mathbf{x}\\) is \\(- \\nabla f(\\mathbf{x})\\), which you use if you are searching for a local minimum).\nThus, the steepest ascent method has the form:\n\\[\n\\mathbf{x}_{n+1} = \\mathbf{x}_n + \\alpha \\nabla f(\\mathbf{x}_n)\n\\]\nWith some \\(\\alpha \\ge 0\\) being the step size while \\(\\nabla f(\\mathbf{x}_n)\\) is the direction of the step.\nGiven this form, we choose \\(\\alpha \\ge 0\\) to maximize.\n\\[\ng(\\alpha) = f(\\mathbf{x}_n + \\alpha \\nabla f(\\mathbf{x}_n)) = f(\\mathbf{x}_{n+1}).\n\\]\nUnless we’re already at a local maximum, we will always choose \\(\\alpha &gt; 0\\) with \\(f(\\mathbf{x}_{n+1}) &gt; f(\\mathbf{x}_n)\\).\nIf \\(f\\) is bounded from above then, because \\(f(\\mathbf{x}_{n+1}) \\ge f(\\mathbf{x}_n)\\), the sequence \\(\\{f(\\mathbf{x}_n)\\}_{n=1}^{\\infty}\\) must converge, which suggests that we can use the stopping condition \\(f(\\mathbf{x}_{n+1}) - f(\\mathbf{x}_n) \\le \\varepsilon\\) for some small tolerance \\(\\varepsilon &gt; 0\\).\nDefine the function line.search that takes arguments \\(f\\), \\(\\mathbf{x}_n\\), and \\(\\nabla f(\\mathbf{x}_n)\\) and returns \\(\\mathbf{x}_n + \\alpha^* \\nabla f(\\mathbf{x}_n)\\), where \\(\\alpha^*\\) is the value that maximizes \\(g(\\alpha)\\)), we can sketch the implementation of the steepest ascent method.\n\n9.2.1 Steepest ascent method: Implementation in R\n\nascent &lt;- function(f, grad.f, vX0, dTol = 1e-9, n.max = 100) {\n  vX.old &lt;- vX0\n  vX &lt;- line.search(f, vX0, grad.f(vX0))\n  n &lt;- 1\n  while ((f(vX) - f(vX.old) &gt; dTol) & (n &lt; n.max)) {\n    vX.old &lt;- vX\n    vX &lt;- line.search(f, vX, grad.f(vX))\n    cat(\"at iteration\", n, \"the coordinates of x are\", vX, \"\\n\")\n    n &lt;- n + 1\n  }\n  return(vX)\n}\n\n\n9.2.2 Line search\nTo complete the steepest ascent algorithm, at each step \\(n\\), given the optimal direction \\(\\mathbf{\\nabla} f(\\mathbf{x}_n)\\), we need to find the optimal step size, \\(\\alpha^*\\), that is, we need to maximize \\(g(\\alpha) = f(\\mathbf{x}_n + \\alpha \\mathbf{\\nabla} f(\\mathbf{x}_n))\\) over \\(\\alpha &gt; 0\\).\nHence, we need to find\n\\[\\alpha^* = \\underset{\\alpha}{\\operatorname{arg max}} f(\\mathbf{x}_n + \\alpha \\mathbf{\\nabla} f(\\mathbf{x}_n)).\\]\nAs we know, it is very hard to find a global maximum, so we will instead look for a local maximum, for which we will use the golden-section algorithm.\nThe golden-section method requires three initial points \\(\\alpha_l &lt; \\alpha_m &lt; \\alpha_r\\) such that \\(g(\\alpha_m) \\ge g(\\alpha_l)\\) and \\(g(\\alpha_m) \\ge g(\\alpha_r)\\).\nPut \\(\\alpha_l = 0\\). Theoretically, if \\(||\\nabla f(\\mathbf{x}_n)|| &gt; 0\\) then \\(g'(0) &gt; 0\\) and there must exist some \\(\\varepsilon &gt; 0\\) such that \\(g(\\varepsilon) &gt; g(0)\\), so we can put \\(\\alpha_m = \\varepsilon\\).\nIn practice, however, if \\(g'(0)\\) is very small, it might not be possible to numerically distinguish \\(g(0)\\) from \\(g(\\varepsilon)\\), and in that case we set \\(\\alpha^* = 0\\).\nThere is no theoretical guarantee that a suitable \\(\\alpha_r\\) exists, because we might have \\(g\\) increasing over the whole interval \\([0, \\infty)\\). To deal with this problem, we specify a maximum step size, \\(\\alpha_{max}\\), such that if we cannot find \\(\\alpha_r \\le \\alpha_{max}\\) such that \\(g(\\alpha_m) \\ge g(\\alpha_r)\\), we set \\(\\alpha^* = \\alpha_{max}\\).\n\n9.2.3 Line-search: Implementation in R\n\nline.search &lt;- function(f, vX, vG, dTol = 1e-9, dA.max = 2^5) {\n  # f is a real function that takes a vector of length d\n  # x and y are vectors of length d\n  # line.search uses gsection to find a &gt;= 0 such that\n  # g(a) = f(x + a*y) has a local maximum at a,\n  # within a tolerance of tol\n  # if no local max is found then we use 0 or a.max for a\n  # the value returned is x + a*y\n  if (sum(abs(vG)) == 0){\n    return(vX) # +0*vG\n  } # g(a) constant\n  g &lt;- function(dA){\n    return(f(vX + dA*vG)) \n  }\n  # find a triple a.l &lt; a.m &lt; a.r such that\n  # g(a.l) &lt;= g(a.m) and g(a.m) &gt;= g(a.r)\n  \n  # choose a.l\n  dA.l &lt;- 0\n  g.l &lt;- g(dA.l)\n  # find a.m\n  dA.m &lt;- 1\n  g.m &lt;- g(dA.m)\n  while ((g.m &lt; g.l) & (dA.m &gt; dTol)) {\n    dA.m &lt;- dA.m/2\n    g.m &lt;- g(dA.m)\n  }\n  # if a suitable a.m was not found then use 0 for a, so just return vX as the next step\n  if ((dA.m &lt;= dTol) & (g.m &lt; g.l)){\n    return(vX)\n  } \n  # find a.r\n  dA.r &lt;- 2*dA.m\n  g.r &lt;- g(dA.r)\n  while ((g.m &lt; g.r) & (dA.r &lt; dA.max)) {\n    dA.m &lt;- dA.r\n    g.m &lt;- g.r\n    dA.r &lt;- 2*dA.m\n    g.r &lt;- g(dA.r)\n  }\n  # if a suitable a.r was not found then use a.max for a\n  if ((dA.r &gt;= dA.max) & (g.m &lt; g.r)){\n    return(vX + dA.max*vG)\n  } \n  # apply golden-section algorithm to g to find a\n  dA &lt;- gsection(g, dA.l, dA.r, dA.m)\n  return(vX + dA*vG)\n}\n\n\n\n\n\n\n\nNote: the code depends on the golden section algorithm. Click here to view it.\n\n\n\n\n\n\n# golden section algorithm from last week\ngsection &lt;- function(f, dX.l, dX.r, dX.m, dTol = 1e-9) {\n  \n  # golden ratio plus one\n  dGR1 &lt;- 1 + (1 + sqrt(5))/2\n  \n  # successively refine x.l, x.r, and x.m\n  f.l &lt;- f(dX.l)\n  f.r &lt;- f(dX.r)\n  f.m &lt;- f(dX.m)\n  while ((dX.r - dX.l) &gt; dTol) { \n    if ((dX.r - dX.m) &gt; (dX.m - dX.l)) { # if the right segment is wider than the left \n      dY &lt;- dX.m + (dX.r - dX.m)/dGR1 # put Y into the right segment according to the golden ratio\n      f.y &lt;- f(dY)\n      if (f.y &gt;= f.m) {\n        dX.l &lt;- dX.m\n        f.l &lt;- f.m\n        dX.m &lt;- dY\n        f.m &lt;- f.y\n      } else {\n        dX.r &lt;- dY\n        f.r &lt;- f.y\n      }\n    } else { #if the left segment is wider than the right\n      dY &lt;- dX.m - (dX.m - dX.l)/dGR1 # put Y into the left segment according to the golden ratio\n      f.y &lt;- f(dY)\n      if (f.y &gt;= f.m) {\n        dX.r &lt;- dX.m\n        f.r &lt;- f.m\n        dX.m &lt;- dY\n        f.m &lt;- f.y\n      } else {\n        dX.l &lt;- dY\n        f.l &lt;- f.y\n      }\n    }\n  }\n  return(dX.m)\n}\n\n\n\n\n\n9.2.4 Example\nConsider the function \\(f(x,y)=\\sin(x^2/2-y^2/4) \\cos(2x-\\exp(y))\\)\n\n# function\nf &lt;- function(vX) {\n  dOut = sin(vX[1]^2/2 - vX[2]^2/4) * cos(2*vX[1] - exp(vX[2]))\n  return(dOut)\n}\n\n# gradient\ngrad.f &lt;- function(vX) {\n  dX_prime.1 = cos(vX[1]^2/2 - vX[2]^2/4)*vX[1] * cos(2*vX[1] - exp(vX[2]))\n  dX_prime.2 = sin(vX[1]^2/2 - vX[2]^2/4) * (-sin(2*vX[1] - exp(vX[2]))*2)\n  dY_prime.1 = cos(vX[1]^2/2 - vX[2]^2/4)*(-vX[2]/2) * cos(2*vX[1] - exp(vX[2]))\n  dY_prime.2 = sin(vX[1]^2/2 - vX[2]^2/4) * sin(2*vX[1] - exp(vX[2]))*exp(vX[2])\n  vOut = c(dX_prime.1 + dX_prime.2, dY_prime.1 + dY_prime.2)\n  return(vOut)\n}\n\nascent(f, grad.f, vX0 = c(0.1, 0.3))\n#&gt; [1] 2.030674 1.401513\nascent(f, grad.f, vX0 = c(0, 0.5))\n#&gt; [1] 0.3424608 1.4271493\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA small difference in where you start can make a big difference to where you end up; we find different local maxima with the two different starting points.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Numerical Optimization 2</span>"
    ]
  },
  {
    "objectID": "numericaloptimization2.html#newtons-method-in-higher-dimensions",
    "href": "numericaloptimization2.html#newtons-method-in-higher-dimensions",
    "title": "\n9  Numerical Optimization 2\n",
    "section": "\n9.3 Newton’s method in higher dimensions",
    "text": "9.3 Newton’s method in higher dimensions\nThe steepest ascent method uses information about the gradient, i.e, the first-order derivatives of \\(f\\).\nBy makign use of the Hessian, i.e., the second-order derivatives of \\(f\\), we can construct methods that converge in fewer steps.\nThe simplest second-order technique is Newton’s method, which can be generalized from one dimension to higher dimensions relatively easily.\nNewton’s method looks for a point \\(\\mathbf{x}\\) such that \\(\\nabla f(\\mathbf{x})=\\mathbf{0}\\).\nThe basis of the method is a second-order Taylor expansion of \\(f\\). For any \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) close together we have:\n\\[f(\\mathbf{y}) \\approx f(\\mathbf{x}) + (\\mathbf{y}-\\mathbf{x})' \\nabla f(\\mathbf{x}) + \\frac{1}{2}(\\mathbf{y}-\\mathbf{x})' \\mathbf{H}(\\mathbf{x}) (\\mathbf{y}-\\mathbf{x}). \\qquad (1)\\]\nTaking first-order partial derivatives on both sides of (1) with respect to the components of \\(\\mathbf{y}\\), we get:\n\\[\\nabla f(\\mathbf{y}) \\approx \\nabla f(\\mathbf{x}) + \\mathbf{H}(\\mathbf{x}) (\\mathbf{y}-\\mathbf{x}).\\]\nIf \\(\\mathbf{y}\\) is a local maximum then \\(\\nabla f(\\mathbf{y}) = 0\\) and, solving the equation above, we get \\(\\mathbf{y} = \\mathbf{x} - \\mathbf{H}(\\mathbf{x})^{-1} \\nabla f(\\mathbf{x})\\).\nPutting \\(\\mathbf{x} = \\mathbf{x}_n\\) and \\(\\mathbf{y} = \\mathbf{x}_{n+1}\\), our Newton’s step in higher dimensions becomes:\n\\[\\mathbf{x}_{n+1} = \\mathbf{x}_n - \\mathbf{H}(\\mathbf{x}_n)^{-1} \\nabla f(\\mathbf{x}_n).\\]\nClearly, if \\(\\mathbf{H}(\\mathbf{x}_n)\\) is singular (has no inverse), then Newton’s method does not work.\nHowever, as in the one-dimensional case, even if \\(\\mathbf{H}(\\mathbf{x}_n)\\) is non-singular at each step, Newton’s method may not converge.\nBut if \\(f\\) has a local maximum at \\(\\mathbf{x}^*\\), \\(f\\) is ‘nicely behaved’ near \\(\\mathbf{x}^*\\), and if our initial point \\(\\mathbf{x}_0\\) is ‘close enough’ to \\(\\mathbf{x}^*\\), then Newton’s method will converge to \\(\\mathbf{x}^*\\) quickly.\nIn implementing Newton’s method, we assume that we have some function f3 that takes argument \\(\\mathbf{x}\\) and returns a list containing \\(f(\\mathbf{x})\\), \\(\\nabla f(\\mathbf{x})\\) and \\(H(\\mathbf{x})\\), and for the stopping condition we use \\(||\\nabla f(\\mathbf{x}_n)||_{\\infty} \\le \\varepsilon\\).\nFurthermore, we make use of the fact that for an invertible matrix \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times d}\\) and a vector \\(\\mathbf{b} \\in \\mathbb{R}^d\\), \\(\\mathbf{A}^{-1}\\mathbf{b}\\) is the solution to the system of equations \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\), which can be found by the R command solve(A, b).\n\n9.3.1 Newton’s method: Implementation in R\n\nnewton &lt;- function(f3, vX0, dTol = 1e-9, n.max = 100) {\n  # Newton's method for optimisation, starting at x0\n  # f3 is a function that given x returns the list\n  # {f(x), grad f(x), Hessian f(x)}, for some f\n  vX &lt;- vX0\n  f3.x &lt;- f3(vX)\n  n &lt;- 0\n  while ((max(abs(f3.x[[2]])) &gt; dTol) & (n &lt; n.max)) {\n    vX &lt;- vX - solve(f3.x[[3]], f3.x[[2]])\n    #vX &lt;- vX - solve(f3.x[[3]])%*%f3.x[[2]]\n    f3.x &lt;- f3(vX)\n    cat(\"At iteration\", n, \"the coordinates of x are\", vX, \"\\n\")\n    n &lt;- n + 1\n  }\n  if (n == n.max) {\n    cat('newton failed to converge\\n')\n  } else {\n    return(vX)\n  }\n}\n\n\n9.3.2 Example\nConsider again the function \\(f(x,y)=\\sin(x^2/2-y^2/4) \\cos(2x-\\exp(y))\\).\n\nnewton(f3, vX0 = c(1.6, 1.2)) # maximum\n#&gt; [1] 2.030697 1.401526\nnewton(f3, vX0 = c(1.6, 0.5)) # saddle point\n#&gt; [1] -0.2902213 -0.2304799\nnewton(f3, vX0 = c(1.75, 0.25)) # minimum\n#&gt; [1]  1.8313777 -0.6516928\n\n\n\n\n\n\n\n\n\nRemember to check whether it is in fact a maximum with the eigen function, e.g.,\n\neigen(f3(n1)[[3]]) # maximum\n#&gt; eigen() decomposition\n#&gt; $values\n#&gt; [1]  -2.030712 -23.079010\n#&gt; \n#&gt; $vectors\n#&gt;            [,1]       [,2]\n#&gt; [1,] -0.8429247 -0.5380316\n#&gt; [2,] -0.5380316  0.8429247\n\neigen(f3(n2)[[3]]) # saddle point\n#&gt; eigen() decomposition\n#&gt; $values\n#&gt; [1] -0.06666478 -1.20252229\n#&gt; \n#&gt; $vectors\n#&gt;           [,1]      [,2]\n#&gt; [1,] -0.456103 -0.889927\n#&gt; [2,] -0.889927  0.456103\n\neigen(f3(n3)[[3]]) # minimum\n#&gt; eigen() decomposition\n#&gt; $values\n#&gt; [1] 7.382289 0.349442\n#&gt; \n#&gt; $vectors\n#&gt;             [,1]        [,2]\n#&gt; [1,] -0.99798281 -0.06348473\n#&gt; [2,]  0.06348473 -0.99798281\n\n\n\n\n\n\n\nClick to view the function f3 and the code for the plot\n\n\n\n\n\n\n#function, gradient, and hessian in list\nf3 &lt;- function(vX) {\n  dA &lt;- vX[1]^2/2 - vX[2]^2/4\n  dB &lt;- 2*vX[1] - exp(vX[2])\n  f &lt;- sin(dA)*cos(dB)\n  f1 &lt;- cos(dA)*cos(dB)*vX[1] - sin(dA)*sin(dB)*2\n  f2 &lt;- -cos(dA)*cos(dB)*vX[2]/2 + sin(dA)*sin(dB)*exp(vX[2])\n  f11 &lt;- -sin(dA)*cos(dB)*(4 + vX[1]^2) + cos(dA)*cos(dB) -\n    cos(dA)*sin(dB)*4*vX[1]\n  f12 &lt;- sin(dA)*cos(dB)*(vX[1]*vX[2]/2 + 2*exp(vX[2])) +\n    cos(dA)*sin(dB)*(vX[1]*exp(vX[2]) + vX[2])\n  f22 &lt;- -sin(dA)*cos(dB)*(vX[2]^2/4 + exp(2*vX[2])) - cos(dA)*cos(dB)/2 -\n    cos(dA)*sin(dB)*vX[2]*exp(vX[2]) + sin(dA)*sin(dB)*exp(vX[2])\n  return(list(f, c(f1, f2), matrix(c(f11, f12, f12, f22), 2, 2)))\n}\n\nn1&lt;-newton(f3, vX0 = c(1.6, 1.2))\nn2&lt;-newton(f3, vX0 = c(1.6, 0.5))\nn3&lt;-newton(f3, vX0 = c(1.75, 0.25))\n\n{\n  plot(NA,xlim=range(vx),\n       ylim=range(vy),xlab=\"x\",ylab=\"y\",\n       frame=FALSE)\n  levels = pretty(range(mf), 50)\n  color.palette = function(n) hcl.colors(n, \"YlOrRd\", rev = TRUE)\n  .filled.contour(x=vx, y=vy, z=mf,\n                  levels=levels,\n                  col=color.palette(length(levels) - 1))\n  points(1.6, 1.2, pch=16, col=\"blue\")\n  arrows(1.6, 1.2, n1[1], n1[2], length=0.1, col=\"blue\")\n  \n  points(1.6, 0.5, pch=16, col=\"green\")\n  arrows(1.6, 0.5, n2[1], n2[2], length=0.1, col=\"green\")\n  \n  points(1.75, 0.25, pch=16, col=\"black\")\n  arrows(1.75, 0.25, n3[1], n3[2], length=0.1, col=\"black\")\n}\n\n\n\n\nThe above example has illustrated that:\n\nNewton’s method can converge to minima or saddle points as well as maxima.\nNewton’s method is faser than the steepest ascent method.\nUnless you are close to a minimum or maximum, you can move in unexpected directions.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Numerical Optimization 2</span>"
    ]
  },
  {
    "objectID": "numericaloptimization2.html#disadvantages-of-steepest-ascent-and-newtons-method",
    "href": "numericaloptimization2.html#disadvantages-of-steepest-ascent-and-newtons-method",
    "title": "\n9  Numerical Optimization 2\n",
    "section": "\n9.4 Disadvantages of steepest ascent and Newton’s method",
    "text": "9.4 Disadvantages of steepest ascent and Newton’s method\nPotential disadvantages of the steepest ascent method and Newton’s method is the need to calculate the gradient and Hessian.\nFor functions that can be expressed in terms of polynomials and the simple transcendental functions (sin, cos, exp, log etc.) the process of calculating the gradient and Hessian should pose no problems (can use the R function deriv or just D to find the gradient and Hessian of a simple expression - check it out yourselves.)\nThere are, however, plenty of situations where \\(f\\) is available but \\(\\nabla f\\) is not; e.g. \\(f\\) might be the result of some numerical procedure or an approximation obtained by simulation.\nNext time you will see how to deal with such situations as well as see how to do constrained optimization.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Numerical Optimization 2</span>"
    ]
  },
  {
    "objectID": "numericaloptimization2.html#gradient-and-hessian-in-r",
    "href": "numericaloptimization2.html#gradient-and-hessian-in-r",
    "title": "\n9  Numerical Optimization 2\n",
    "section": "\n9.5 Gradient and Hessian in R",
    "text": "9.5 Gradient and Hessian in R\nWe saw that the gradfunction in the numDeriv package can help us to compute the gradient of f:\ngrad(f, x, ...)\nThe numDeriv package also provides us the hessian function:\nhessian(f, x, ...)\nSee help(hessian). (Remember to load numDeriv with library(numDeriv)).",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Numerical Optimization 2</span>"
    ]
  },
  {
    "objectID": "numericaloptimization2.html#optimization-in-r-multivariate",
    "href": "numericaloptimization2.html#optimization-in-r-multivariate",
    "title": "\n9  Numerical Optimization 2\n",
    "section": "\n9.6 Optimization in R: multivariate",
    "text": "9.6 Optimization in R: multivariate\nIn higher dimensions there are a variety of optimization methods in current use. The R function optim provides functionalities to maximize a multivariate function:\noptim(par, fn, gr = NULL, ..., method = c(\"Nelder-Mead\", \"BFGS\", \"CG\", \"L-BFGS-B\", \"SANN\", \"Brent\"), lower = -Inf, upper = Inf, control = list(), hessian = FALSE)\nWhere the method argument specifies the type of optimizer we want to employ. The most used are:\n\nmethod = \"Nelder-Mead\" the Nelder and Mead1, method.\nmethod = \"BFGS\" is a quasi-Newton method.\nmethod = \"L-BFGS-B\" of Byrd et. al. (1995)2 which allows box constraints.\n\n\n9.6.1 Example\n\n# fnscale multiplies the function by -1, such that a maximum is found. \n# the default is a minimum\noptim(c(1.6, 1.2), f, gr = grad.f, method = \"BFGS\", control = list(fnscale = -1))\n#&gt; $par\n#&gt; [1] 2.030697 1.401526\n#&gt; \n#&gt; $value\n#&gt; [1] 1\n#&gt; \n#&gt; $counts\n#&gt; function gradient \n#&gt;       22        9 \n#&gt; \n#&gt; $convergence\n#&gt; [1] 0\n#&gt; \n#&gt; $message\n#&gt; NULL",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Numerical Optimization 2</span>"
    ]
  },
  {
    "objectID": "numericaloptimization2.html#footnotes",
    "href": "numericaloptimization2.html#footnotes",
    "title": "\n9  Numerical Optimization 2\n",
    "section": "",
    "text": "Nelder, J. A. and Mead, R. (1965) A simplex algorithm for function minimization. Computer Journal 7, 308-313.↩︎\nByrd, R. H., Lu, P., Nocedal, J. and Zhu, C. (1995) A limited memory algorithm for bound constrained optimization. SIAM J. Scientific Computing, 16, 1190-1208.↩︎",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Numerical Optimization 2</span>"
    ]
  },
  {
    "objectID": "constrainedandpitfalls.html",
    "href": "constrainedandpitfalls.html",
    "title": "\n10  Constrained Optimization and Pitfalls in Optimization\n",
    "section": "",
    "text": "10.1 Constrained Optimization\nVery often we have some restrictions on the input space used for optimization. Typical examples include optimization under\nExample I: Likelihood of a stationary autoregressive AR(1) process:\n\\[y_t = \\alpha y_{t-1} + \\varepsilon_t, \\qquad \\varepsilon_t \\sim N(0, \\sigma^2)\\]\nFor stationarity we need that \\(|\\alpha| &lt; 1\\) or \\(-1 &lt; \\alpha &lt; 1\\).\nExample II: Stationary GARCH Process\n\\[\\sigma_t^2 = \\omega + \\alpha \\varepsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2\\]\nFor stationarity and nonnegativity we need that \\(\\omega &gt; 0\\) and \\(\\alpha + \\beta &lt; 1\\)\nIn general the constrained optimization problem can be defined as\n\\[\\begin{cases} \\min_x f(x) \\\\ \\text{s.t.} \\quad g(x) \\le 0 \\end{cases} \\qquad (1)\\]\nE.g.\nThe most common techniques to transform a constrained problem into an unconstrained problem are:",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Constrained Optimization and Pitfalls in Optimization</span>"
    ]
  },
  {
    "objectID": "constrainedandpitfalls.html#constrained-optimization",
    "href": "constrainedandpitfalls.html#constrained-optimization",
    "title": "\n10  Constrained Optimization and Pitfalls in Optimization\n",
    "section": "",
    "text": "nonnegativity constraints \\(\\mathbf{x} \\ge \\mathbf{0}\\)\n\n\n\\(m\\) linear restrictions \\(\\mathbf{R}\\mathbf{x} - \\mathbf{q} = \\mathbf{0}\\) for some \\(\\mathbf{R} \\in \\mathbb{R}^{m}\\times \\mathbb{R}^{d}\\), \\(\\mathbf{q} \\in \\mathbb{R}^d\\)\n\nupper and lower bounds \\(\\mathbf{a} \\le \\mathbf{x} \\le \\mathbf{b}\\) for some \\(\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^d\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathbf{g(x)} = -\\mathbf{x} \\le \\mathbf{0}\\)\n\\(\\mathbf{g(x)} = \\begin{pmatrix} \\mathbf{Rx - q} \\\\ -\\mathbf{(Rx - q)} \\end{pmatrix} \\le \\mathbf{0}\\)\n\\(\\mathbf{g(x)} = \\begin{pmatrix} \\mathbf{x - b} \\\\ \\mathbf{a - x} \\end{pmatrix} \\le \\mathbf{0}\\)\n\n\n\nPenalization functions\nBarrier functions",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Constrained Optimization and Pitfalls in Optimization</span>"
    ]
  },
  {
    "objectID": "constrainedandpitfalls.html#penalization-functions",
    "href": "constrainedandpitfalls.html#penalization-functions",
    "title": "\n10  Constrained Optimization and Pitfalls in Optimization\n",
    "section": "\n10.2 Penalization functions",
    "text": "10.2 Penalization functions\nIf we could, we’d like to optimize\n\\[\\min_{\\mathbf{x}} (f(\\mathbf{x}) + p(\\mathbf{x})), \\quad p(\\mathbf{x}) = \\begin{cases} 0 & \\mathbf{g(x) \\le 0} \\\\ \\infty & \\text{otherwise} \\end{cases} \\qquad (2)\\]\nNot a continuous function! Instead we can do\n\\[\\min_{\\mathbf{x}} \\underbrace{(f(\\mathbf{x}) + \\gamma \\cdot ||\\max\\{\\mathbf{g(x), 0\\}}||^2)}_{f_p(\\mathbf{x}, \\gamma)} \\quad \\gamma &gt; 0 \\qquad (3)\\]\nsuch that every time the function exits from the boundary, its value is increased by an amount that is proportional to the distance from the admissible set. We can then solve a sequence of optimization problems\n\\[\\mathbf{x}^{*(h)} = \\min_\\mathbf{x} f_p(\\mathbf{x}, \\gamma_h)\\]\nfor an increasing sequence of positive values of \\(\\gamma_h\\). The algorithm stops when\n\\[||\\mathbf{x}^{*(h)} - \\mathbf{x}^{*(h-1)}|| &lt; \\epsilon_\\gamma\\]",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Constrained Optimization and Pitfalls in Optimization</span>"
    ]
  },
  {
    "objectID": "constrainedandpitfalls.html#barrier-functions---interior-point",
    "href": "constrainedandpitfalls.html#barrier-functions---interior-point",
    "title": "\n10  Constrained Optimization and Pitfalls in Optimization\n",
    "section": "\n10.3 Barrier functions - (Interior-point)",
    "text": "10.3 Barrier functions - (Interior-point)\nThe objective function in this case is transformed as\n\\[\\min_\\mathbf{x} \\underbrace{(f(\\mathbf{x}) + \\gamma \\cdot b(\\mathbf{x}))}_{f_b(\\mathbf{x}, \\gamma)} \\quad \\gamma &gt; 0 \\qquad (4)\\]\nwhere\n\\[b(\\mathbf{x}) = \\begin{cases} -\\sum_{i=1}^m \\log(-g_i(\\mathbf{x})) & \\mathbf{g}(\\mathbf{x}) \\le \\mathbf{0} \\\\ \\infty & \\text{otherwise} \\end{cases}\\]\nWe impose a very large cost on feasible points that lie close to the boundary since \\(-\\log(-g_i(\\mathbf{x})) \\to \\infty\\) as \\(g_i(\\mathbf{x}) \\to 0^-\\). We can then solve a sequence of optimization problems\n\\[\\mathbf{x}^{*(h)} = \\min_\\mathbf{x} f_b(\\mathbf{x}, \\gamma_h)\\]\nfor a decreasing sequence of positive values of \\(\\gamma_h\\). The algorithm stops when\n\\[||\\mathbf{x}^{*(h)} - \\mathbf{x}^{*(h-1)}|| &lt; \\epsilon_\\gamma\\]",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Constrained Optimization and Pitfalls in Optimization</span>"
    ]
  },
  {
    "objectID": "constrainedandpitfalls.html#parameter-constraints",
    "href": "constrainedandpitfalls.html#parameter-constraints",
    "title": "\n10  Constrained Optimization and Pitfalls in Optimization\n",
    "section": "\n10.4 Parameter constraints",
    "text": "10.4 Parameter constraints\nSo far we have considered the case where \\(\\mathbf{x} \\in \\mathbb{R}^p\\), however, the most common scenario is when \\(\\mathbf{x} \\in \\mathcal{X} \\subset \\mathbb{R}^p\\).\n\nNewton-based method (method = \"BFGS\") doesn’t know about ranges\nAlternative optimization (method = \"L-BFGS-B\") does but: slower/worse convergence\n\nConsider for example the problem of estimating the location \\(\\mu\\), scale \\(\\psi\\) and degree of freedom parameters of a Student’s \\(t\\) distribution. Formally let \\(\\mathbf{y} = (y_1, ..., y_T)'\\) a sample of \\(T\\) iid observations from a Student’s \\(t\\) distribution. The ML estimator for \\(\\mathbf{\\theta} = (\\mu, \\psi, \\nu)\\) is:\n\\[\\mathbf{\\theta}^{ML} = \\underset{\\mathbf{\\theta} \\in \\Theta}{\\operatorname{arg max}} \\sum_{t=1}^T \\log p_T(y_t; \\mathbf{\\theta}),\\]\nwhere \\(\\Theta\\) is the restricted space \\(\\mathbb{R} \\times (0, \\infty) \\times (0, \\infty)\\).\nWe have two options:\n\nUse a constrained optimizer like optim with method = \"L-BFGS-B\".\nReparameterize our problem and use an unrestricted optimizer.\n\nThe second option is usually preferred and provides better results.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Constrained Optimization and Pitfalls in Optimization</span>"
    ]
  },
  {
    "objectID": "constrainedandpitfalls.html#reparameterization",
    "href": "constrainedandpitfalls.html#reparameterization",
    "title": "\n10  Constrained Optimization and Pitfalls in Optimization\n",
    "section": "\n10.5 Reparameterization",
    "text": "10.5 Reparameterization\nLet \\(\\mathbf{\\lambda} : \\mathbb{R}^p \\rightarrow \\Omega\\) a vector-valued differentiable mapping function such that \\(\\mathbf{\\lambda(\\tilde{\\theta})} = \\mathbf{\\theta}\\) where \\(\\mathbf{\\tilde{\\theta}}\\) is a reparametrization of \\(\\mathbf{\\theta}\\). In the Student’s \\(t\\) case \\(\\mathbf{\\lambda(\\cdot)}\\) can be chosen as:\n\\[\\mathbf{\\lambda(\\tilde{\\theta})} = \\begin{cases} \\mu = \\tilde{\\mu} \\\\ \\psi = \\exp(\\tilde{\\psi}) \\\\ \\nu = \\exp(\\tilde{\\nu}) \\end{cases}\\]\nwhere \\(\\tilde{\\psi}\\) and \\(\\tilde{\\nu}\\) are reparameterization of \\(\\psi\\) and \\(\\nu\\), respectively.\nSince \\(\\tilde{\\theta} \\in \\mathbb{R}^3\\), the original optimization problem can now be reformulated as:\n\\[\\mathbf{\\tilde{\\theta}}^{ML} = \\underset{\\tilde{\\theta} \\in \\mathbb{R}^3}{\\operatorname{arg max}} \\sum_{t=1}^T \\log p_T(y_t; \\mathbf{\\lambda(\\tilde{\\theta})}),\\]\nand then \\(\\mathbf{\\theta}^{ML}=\\lambda(\\tilde{\\mathbf{\\theta}^{ML}})\\). Which can be solved with an unconstrained optimizer like optim with method = \"BFGS\".",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Constrained Optimization and Pitfalls in Optimization</span>"
    ]
  },
  {
    "objectID": "constrainedandpitfalls.html#differentiation-after-reparameterization-delta-method",
    "href": "constrainedandpitfalls.html#differentiation-after-reparameterization-delta-method",
    "title": "\n10  Constrained Optimization and Pitfalls in Optimization\n",
    "section": "\n10.6 Differentiation after reparameterization: Delta method",
    "text": "10.6 Differentiation after reparameterization: Delta method\nNote however that the evaluation of the gradient and the hessian matrix needs to be modified after the reparameterization of the problem. We have the following identities:\n\\[\\widetilde{\\nabla} f(\\tilde{\\mathbf{\\theta}}) = \\mathcal{J}(\\tilde{\\mathbf{\\theta}})' \\nabla f(\\mathbf{\\theta}) = \\mathcal{J}(\\tilde{\\mathbf{\\theta}})' \\nabla f(\\lambda(\\tilde{\\mathbf{\\theta}}))\\] \\[\\widetilde{\\mathbf{H}} (\\tilde{\\theta}) = \\mathcal{J}(\\tilde{\\mathbf{\\theta}})' \\mathbf{H}(\\mathbf{\\theta}) \\mathcal{J}(\\tilde{\\mathbf{\\theta}}) = \\mathcal{J}(\\tilde{\\mathbf{\\theta}})' \\mathbf{H}(\\lambda(\\tilde{\\mathbf{\\theta}})) \\mathcal{J}(\\tilde{\\mathbf{\\theta}}),\\]\nwhere \\(\\mathcal{J}(\\tilde{\\mathbf{\\theta}})\\) is the Jacobian associated to the mapping \\(\\mathbf{\\lambda}(\\cdot)\\). In the previous example:\n\\[\\mathcal{J}(\\tilde{\\mathbf{\\theta}}) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\exp(\\tilde{\\psi}) & 0 \\\\ 0 & 0 & \\exp(\\tilde{\\nu}) \\end{pmatrix}\\]",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Constrained Optimization and Pitfalls in Optimization</span>"
    ]
  },
  {
    "objectID": "constrainedandpitfalls.html#transforming-parameters",
    "href": "constrainedandpitfalls.html#transforming-parameters",
    "title": "\n10  Constrained Optimization and Pitfalls in Optimization\n",
    "section": "\n10.7 Transforming parameters",
    "text": "10.7 Transforming parameters\nVariance parameter positive? Solutions:\n\nUse \\(\\sigma \\equiv |\\theta|\\) as parameter, ie forget the sign altogether (doesn’t matter for optimisation, interpret negative \\(\\sigma\\) in outcome as positive value)\nTransform, optimise \\(\\theta = \\log \\sigma \\in (-\\infty, \\infty)\\), no trouble for optimisation\n\nLast option most common, most robust, neatest.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Constrained Optimization and Pitfalls in Optimization</span>"
    ]
  },
  {
    "objectID": "constrainedandpitfalls.html#transform-common-transformations",
    "href": "constrainedandpitfalls.html#transform-common-transformations",
    "title": "\n10  Constrained Optimization and Pitfalls in Optimization\n",
    "section": "\n10.8 Transform: Common transformations",
    "text": "10.8 Transform: Common transformations\n\n\n\n\n\n\n\nConstraint\n\\(\\theta\\)\n\\(\\tilde{\\theta}\\)\n\n\n\n\\([0, \\infty)\\)\n\\(\\exp(\\tilde{\\theta})\\)\n\\(\\log(\\theta)\\)\n\n\n\\([0, 1]\\)\n\\(\\frac{\\exp(\\tilde{\\theta})}{1 + \\exp(\\tilde{\\theta})}\\)\n\\(\\log\\left(\\frac{\\theta}{1-\\theta}\\right)\\)\n\n\n\nOf course, to get a range of \\([L, U]\\), use a rescaled \\([0, 1]\\) transformation.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Constrained Optimization and Pitfalls in Optimization</span>"
    ]
  },
  {
    "objectID": "constrainedandpitfalls.html#code-for-constrained-optimization",
    "href": "constrainedandpitfalls.html#code-for-constrained-optimization",
    "title": "\n10  Constrained Optimization and Pitfalls in Optimization\n",
    "section": "\n10.9 Code for constrained optimization",
    "text": "10.9 Code for constrained optimization\n\n\n\n\n\n\nClick to view full code associated with the first part\n\n\n\n\n\n\n# adapted code from last lecture ------------------------------------------\n\n#golden section \ngsection &lt;- function(f, dX.l, dX.r, dX.m, dTol = 1e-9, ...) {\n  \n  # golden ratio plus one\n  dGR1 &lt;- 1 + (1 + sqrt(5))/2\n  \n  # successively refine x.l, x.r, and x.m\n  f.l &lt;- f(dX.l, ...)\n  f.r &lt;- f(dX.r, ...)\n  f.m &lt;- f(dX.m, ...)\n  while ((dX.r - dX.l) &gt; dTol) { \n    if ((dX.r - dX.m) &gt; (dX.m - dX.l)) { # if the right segment is wider than the left \n      dY &lt;- dX.m + (dX.r - dX.m)/dGR1 # put Y into the right segment according to the golden ratio\n      f.y &lt;- f(dY, ...)\n      if (f.y &gt;= f.m) {\n        dX.l &lt;- dX.m\n        f.l &lt;- f.m\n        dX.m &lt;- dY\n        f.m &lt;- f.y\n      } else {\n        dX.r &lt;- dY\n        f.r &lt;- f.y\n      }\n    } else { #if the left segment is wider than the right\n      dY &lt;- dX.m - (dX.m - dX.l)/dGR1 # put Y into the left segment according to the golden ratio\n      f.y &lt;- f(dY, ...)\n      if (f.y &gt;= f.m) {\n        dX.r &lt;- dX.m\n        f.r &lt;- f.m\n        dX.m &lt;- dY\n        f.m &lt;- f.y\n      } else {\n        dX.l &lt;- dY\n        f.l &lt;- f.y\n      }\n    }\n  }\n  return(dX.m)\n}\n\n#line search\nline.search &lt;- function(f, vX, vG, dTol = 1e-9, dA.max = 2^5, ...) {\n  # f is a real function that takes a vector of length d\n  # x and y are vectors of length d\n  # line.search uses gsection to find a &gt;= 0 such that\n  # g(a) = f(x + a*y) has a local maximum at a,\n  # within a tolerance of tol\n  # if no local max is found then we use 0 or a.max for a\n  # the value returned is x + a*y\n  if (sum(abs(vG)) == 0){\n    return(vX) # +0*vG\n  } # g(a) constant\n  g &lt;- function(dA, ...){\n    return(f(vX + dA*vG, ...)) \n  }\n  # find a triple a.l &lt; a.m &lt; a.r such that\n  # g(a.l) &lt;= g(a.m) and g(a.m) &gt;= g(a.r)\n  \n  # choose a.l\n  dA.l &lt;- 0\n  g.l &lt;- g(dA.l, ...)\n  # find a.m\n  dA.m &lt;- 1\n  g.m &lt;- g(dA.m, ...)\n  while ((g.m &lt; g.l) & (dA.m &gt; dTol)) {\n    dA.m &lt;- dA.m/2\n    g.m &lt;- g(dA.m, ...)\n  }\n  # if a suitable a.m was not found then use 0 for a, so just return vX as the next step\n  if ((dA.m &lt;= dTol) & (g.m &lt; g.l)){\n    return(vX)\n  } \n  # find a.r\n  dA.r &lt;- 2*dA.m\n  g.r &lt;- g(dA.r, ...)\n  while ((g.m &lt; g.r) & (dA.r &lt; dA.max)) {\n    dA.m &lt;- dA.r\n    g.m &lt;- g.r\n    dA.r &lt;- 2*dA.m\n    g.r &lt;- g(dA.r, ...)\n  }\n  # if a suitable a.r was not found then use a.max for a\n  if ((dA.r &gt;= dA.max) & (g.m &lt; g.r)){\n    return(vX + dA.max*vG)\n  } \n  # apply golden-section algorithm to g to find a\n  dA &lt;- gsection(g, dA.l, dA.r, dA.m, ...)\n  return(vX + dA*vG)\n}\n\n#ascent function\nascent &lt;- function(f, grad.f, vX0, dTol = 1e-9, n.max = 100, verbose = TRUE, ...) {\n  vX.old &lt;- vX0\n  vG0&lt;-grad.f(vX0, ...)\n  vX &lt;- line.search(f, vX0, vG0, ...)\n  n &lt;- 1\n  while ((f(vX, ...) - f(vX.old, ...) &gt; dTol) & (n &lt; n.max)) {\n    vX.old &lt;- vX\n    vG &lt;- grad.f(vX, ...)\n    vX &lt;- line.search(f, vX, vG, ...)\n    if(verbose){\n      cat(\"at iteration\", n, \"the coordinates of x are\", vX, \"\\n\")\n    }\n    n &lt;- n + 1\n  }\n  return(vX)\n}\n\n#function\nf &lt;- function(vX) {\n  dOut = sin(vX[1]^2/2 - vX[2]^2/4) * cos(2*vX[1] - exp(vX[2]))\n  return(dOut)\n}\n\n\n# constrained with penalization -------------------------------------------\n\n#penalizion function\np&lt;-function(vX, va, vb){\n  gX&lt;-c(vX-vb, va-vX)\n  return(sum(max(gX,0))^2)\n}\n\n#new penalized objective\nf_p&lt;-function(vX, dGamma, ...){\n  return(f(vX)-dGamma*p(vX, ...)) #subtract the penalty since we're maximizing\n}\n\nlibrary(numDeriv)\ngrad.f_p&lt;-function(vX, dGamma, ...){\n  return(grad(func=f_p, x=vX, dGamma=dGamma, ...)) # using numerical derivatives here because I'm lazy\n}\n\nvx1 &lt;- seq(-0.5, 3, 0.05)\nvx2 &lt;- seq(-0.5, 2, 0.05)\nmf &lt;- matrix(0, length(vx1), length(vx2))\nfor(i in 1:length(vx1)){\n  for(j in 1:length(vx2)){\n    mf[i,j]&lt;-f(c(vx1[i], vx2[j]))\n  }\n}\n{\n  plot(NA,xlim=range(vx1),\n       ylim=range(vx2),xlab=expression(\"x\"[1]),ylab=expression(\"x\"[2]),\n       frame=FALSE)\n  levels = pretty(range(mf), 50)\n  color.palette = function(n) hcl.colors(n, \"YlOrRd\", rev = TRUE)\n  .filled.contour(x=vx1, y=vx2, z=mf,\n                  levels=levels,\n                  col=color.palette(length(levels) - 1))\n}\n\n\n\n#choose upper and lower bounds va and vb, and draw the corresponding box constraint\n#this box doesn't contain any local minimum, so any solution will be on the border\nva=c(1,0.5)\nvb=c(2.5,1)\nrect(xleft=va[1], ybottom=va[2], xright=vb[1], ytop=vb[2])\n\n#choose a point inside the box - penalty=0\nvX=c(1.5,0.75)\npoints(vX[1],vX[2], col=\"blue\", pch=16)\np(vX, va, vb)\n#&gt; [1] 0\n\n#point outside -  penalty&gt;0\nvX=c(0.5,0.75)\npoints(vX[1],vX[2], col=\"green\", pch=16)\n\n\n\n\n\n\np(vX, va, vb)\n#&gt; [1] 0.25\n\n\n\n#plotting the solutions for different gamma_h\n{\n  plot(NA,xlim=range(vx1),\n       ylim=range(vx2),xlab=expression(\"x\"[1]),ylab=expression(\"x\"[2]),\n       frame=FALSE)\n  levels = pretty(range(mf), 50)\n  color.palette = function(n) hcl.colors(n, \"YlOrRd\", rev = TRUE)\n  .filled.contour(x=vx1, y=vx2, z=mf,\n                  levels=levels,\n                  col=color.palette(length(levels) - 1))\n  rect(xleft=va[1], ybottom=va[2], xright=vb[1], ytop=vb[2])\n}\n\n\nvX0&lt;-c(1.5,1.5)\npoints(vX0[1],vX0[2], col=\"blue\", pch=16)\n\n\n#gamma_h=0, no penalization\nvX_star&lt;-ascent(f_p, grad.f_p, vX0, verbose=FALSE, dGamma=0, va=va, vb=vb)\narrows(x0=vX0[1], y0=vX0[2], x1=vX_star[1], y1=vX_star[2], length=0.05, col=\"blue\")\nvX_old&lt;-vX_star\n\n#gamma_h=1 \nvX_star&lt;-ascent(f_p, grad.f_p, vX0=vX_old, verbose=FALSE, dGamma=1, va=va, vb=vb) #hot start: take the initial value equal to the last best guess\narrows(x0=vX_old[1], y0=vX_old[2], x1=vX_star[1], y1=vX_star[2], length=0.05, col=\"blue\") \nvX_old&lt;-vX_star\n\n#gamma_h=10 \nvX_star&lt;-ascent(f_p, grad.f_p, vX0=vX_old, verbose=FALSE, dGamma=10, va=va, vb=vb)\narrows(x0=vX_old[1], y0=vX_old[2], x1=vX_star[1], y1=vX_star[2], length=0.05, col=\"blue\") \nvX_old&lt;-vX_star\n\n#gamma_h=100\nvX_star&lt;-ascent(f_p, grad.f_p, vX0, verbose=FALSE, dGamma=100, va=va, vb=vb)\narrows(x0=vX_old[1], y0=vX_old[2], x1=vX_star[1], y1=vX_star[2], length=0.05, col=\"blue\")\n\n\n\n\n\n\n\n\n\n#making a function that iterates like above\npenalized_ascent&lt;-function(f_p, grad.f_p, vX0, epsilon_h = 1e-9, h.max = 100, verbose_ = TRUE, ...){\n  #first iteration\n  vXh &lt;- ascent(f_p, grad.f_p, vX0, verbose=FALSE, dGamma=0, ...)\n  vXh_old &lt;- vX0\n  h &lt;- 1\n  while( sum(abs(vXh - vXh_old)) &gt; epsilon_h && h &lt; h.max){\n    vXh_old &lt;- vXh\n    vXh &lt;- ascent(f_p, grad.f_p, vXh_old, verbose=FALSE, dGamma=10^(h-1), ...) #gamma_h gets 10x bigger with every interation\n    if(verbose_){\n      cat(\"at iteration\", h, \"the coordinates of x are\", vXh, \"\\n\")\n    }\n    h &lt;- h + 1\n  }\n  return(vXh)\n}\n\n#test out how it works for different starting values!\n{\n  plot(NA,xlim=range(vx1),\n       ylim=range(vx2),xlab=expression(\"x\"[1]),ylab=expression(\"x\"[2]),\n       frame=FALSE)\n  levels = pretty(range(mf), 50)\n  color.palette = function(n) hcl.colors(n, \"YlOrRd\", rev = TRUE)\n  .filled.contour(x=vx1, y=vx2, z=mf,\n                  levels=levels,\n                  col=color.palette(length(levels) - 1))\n  rect(xleft=va[1], ybottom=va[2], xright=vb[1], ytop=vb[2])\n}\n\n\nvX0&lt;-c(2.5,0)\npoints(vX0[1],vX0[2], col=\"blue\", pch=16)\nvX&lt;-penalized_ascent(f_p, grad.f_p, vX0, va=va, vb=vb)\n#&gt; at iteration 1 the coordinates of x are 3.024968 1.039979 \n#&gt; at iteration 2 the coordinates of x are 1.629637 1.039826 \n#&gt; at iteration 3 the coordinates of x are 1.593251 1.003943 \n#&gt; at iteration 4 the coordinates of x are 1.589693 1.000394 \n#&gt; at iteration 5 the coordinates of x are 1.589684 1.000039 \n#&gt; at iteration 6 the coordinates of x are 1.589684 1.000004 \n#&gt; at iteration 7 the coordinates of x are 1.589684 1 \n#&gt; at iteration 8 the coordinates of x are 1.589684 1 \n#&gt; at iteration 9 the coordinates of x are 1.589684 1\narrows(x0=vX0[1], y0=vX0[2], x1=vX[1], y1=vX[2], length=0.05, col=\"blue\")\n\n\n\n\n\n\n\n# constrained with barrier ------------------------------------------------\n#barrier function\nb&lt;-function(vX, va, vb){\n  gX&lt;-c(vX-vb, va-vX)\n  if(all(gX&lt;=0)){\n    return(-sum(log(-gX)))\n  }else{\n    return(Inf)\n  }\n}\n#new objective with a barrier\nf_b&lt;-function(vX, dGamma, ...){\n  return(f(vX)-dGamma*b(vX, ...)) #subtract the penalty since we're maximizing\n}\n\ngrad.f_b&lt;-function(vX, dGamma, ...){\n  return(grad(func=f_p, x=vX, dGamma=dGamma, ...)) # using numerical derivatives here because I'm lazy\n}\n\n\n#plotting the solutions for different gamma_h\n{\n  plot(NA,xlim=range(vx1),\n       ylim=range(vx2),xlab=expression(\"x\"[1]),ylab=expression(\"x\"[2]),\n       frame=FALSE)\n  levels = pretty(range(mf), 50)\n  color.palette = function(n) hcl.colors(n, \"YlOrRd\", rev = TRUE)\n  .filled.contour(x=vx1, y=vx2, z=mf,\n                  levels=levels,\n                  col=color.palette(length(levels) - 1))\n  rect(xleft=va[1], ybottom=va[2], xright=vb[1], ytop=vb[2])\n}\n\n\nvX0&lt;-c(1.2,0.7) #start inside the box\npoints(vX0[1],vX0[2], col=\"blue\", pch=16)\n\n\n#gamma_h=1\nvX_star&lt;-ascent(f_b, grad.f_b, vX0, verbose=FALSE, dGamma=1, va=va, vb=vb)\narrows(x0=vX0[1], y0=vX0[2], x1=vX_star[1], y1=vX_star[2], length=0.05, col=\"blue\")\nvX_old&lt;-vX_star\n\n#gamma_h=1/10\nvX_star&lt;-ascent(f_b, grad.f_b, vX0=vX_old, verbose=FALSE, dGamma=1/10, va=va, vb=vb) #hot start: take the initial value equal to the last best guess\narrows(x0=vX_old[1], y0=vX_old[2], x1=vX_star[1], y1=vX_star[2], length=0.05, col=\"blue\") \nvX_old&lt;-vX_star\n\n#gamma_h=1/100\nvX_star&lt;-ascent(f_b, grad.f_b, vX0=vX_old, verbose=FALSE, dGamma=1/100, va=va, vb=vb)\narrows(x0=vX_old[1], y0=vX_old[2], x1=vX_star[1], y1=vX_star[2], length=0.05, col=\"blue\") \nvX_old&lt;-vX_star\n\n#gamma_h=1/1000\nvX_star&lt;-ascent(f_b, grad.f_b, vX0, verbose=FALSE, dGamma=1/1000, va=va, vb=vb)\narrows(x0=vX_old[1], y0=vX_old[2], x1=vX_star[1], y1=vX_star[2], length=0.05, col=\"blue\")\n\n\n\n\n\n\n\nbarrier_ascent&lt;-function(f_b, grad.f_b, vX0, epsilon_h = 1e-9, h.max = 100, verbose_ = TRUE, ...){\n  #first iteration\n  vXb &lt;- ascent(f_b, grad.f_b, vX0, verbose=FALSE, dGamma=1, ...)\n  vXb_old &lt;- vX0\n  h &lt;- 1\n  while( sum(abs(vXb - vXb_old)) &gt; epsilon_h && h &lt; h.max){\n    vXb_old &lt;- vXb\n    vXb &lt;- ascent(f_b, grad.f_b, vXb_old, verbose=FALSE, dGamma=10^(-h), ...) #gamma_h gets 10x smaller with every iteration\n    if(verbose_){\n      cat(\"at iteration\", h, \"the coordinates of x are\", vXb, \"\\n\")\n    }\n    h &lt;- h + 1\n  }\n  return(vXb)\n}\n\n#test out how it works for different starting values!\n{\n  plot(NA,xlim=range(vx1),\n       ylim=range(vx2),xlab=expression(\"x\"[1]),ylab=expression(\"x\"[2]),\n       frame=FALSE)\n  levels = pretty(range(mf), 50)\n  color.palette = function(n) hcl.colors(n, \"YlOrRd\", rev = TRUE)\n  .filled.contour(x=vx1, y=vx2, z=mf,\n                  levels=levels,\n                  col=color.palette(length(levels) - 1))\n  rect(xleft=va[1], ybottom=va[2], xright=vb[1], ytop=vb[2])\n}\n\n\nvX0&lt;-c(1.1,0.55)\npoints(vX0[1],vX0[2], col=\"blue\", pch=16)\nvX&lt;-barrier_ascent(f_b, grad.f_b, vX0, va=va, vb=vb)\n#&gt; at iteration 1 the coordinates of x are 1.490837 0.8984811 \n#&gt; at iteration 2 the coordinates of x are 1.506099 0.9881654 \n#&gt; at iteration 3 the coordinates of x are 1.517451 0.9987611 \n#&gt; at iteration 4 the coordinates of x are 1.518635 0.9998752 \n#&gt; at iteration 5 the coordinates of x are 1.518754 0.9999875 \n#&gt; at iteration 6 the coordinates of x are 1.518766 0.9999988 \n#&gt; at iteration 7 the coordinates of x are 1.518767 0.9999999 \n#&gt; at iteration 8 the coordinates of x are 1.518767 1 \n#&gt; at iteration 9 the coordinates of x are 1.518767 1 \n#&gt; at iteration 10 the coordinates of x are 1.518767 1 \n#&gt; at iteration 11 the coordinates of x are 1.518767 1\narrows(x0=vX0[1], y0=vX0[2], x1=vX[1], y1=vX[2], length=0.05, col=\"blue\")",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Constrained Optimization and Pitfalls in Optimization</span>"
    ]
  },
  {
    "objectID": "constrainedandpitfalls.html#pitfalls",
    "href": "constrainedandpitfalls.html#pitfalls",
    "title": "\n10  Constrained Optimization and Pitfalls in Optimization\n",
    "section": "\n10.10 Pitfalls",
    "text": "10.10 Pitfalls\nThere are some potential pitfalls with optimization routines in empirical practice. E.g, for local optima, different starting points may lead to different optima. A possible solution to this is simulated annealing which is a probabilistic technique for approximating the global optimum of a given function.\n\n\nPitfall 1\n\nAnother potential is related to flat surfaces: different starting points; transformation for parameter (\\(\\theta = \\log{\\sigma}\\) tends to optimise a lot better…)\n\n\nPitfall 2\n\nNoise on derivatives/function: Only smooth functions can be optimised using NR-approach. Use analytical derivatives. Robust programming of target function (optimise AVERAGE LOG likelihood, never likelihood itself).\n\n\nPitfall 3\n\nNon-uniqueness of optimum/indeterminacy in model specification: Optimize \\(\\sigma\\), both \\(\\sigma = -1\\), \\(\\sigma = 1\\) give same likelihood…\n\n\nPitfall 4\n\nBad Taylor-approximation: Sometimes transformation of parameters helps a bit, or it is just bad luck.\n\n\nPitfall 5",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Constrained Optimization and Pitfalls in Optimization</span>"
    ]
  },
  {
    "objectID": "constrainedandpitfalls.html#problematic-hessian",
    "href": "constrainedandpitfalls.html#problematic-hessian",
    "title": "\n10  Constrained Optimization and Pitfalls in Optimization\n",
    "section": "\n10.11 Problematic Hessian?",
    "text": "10.11 Problematic Hessian?\nAlgorithms based on NR need \\(\\mathbf{H}(x_n)\\).\nA problem with NR is that the Hessian matrix is not guaranteed to be non-singular. If it is (near) singular at any step, the search routine could either diverge or break down altogether. Problematic:\n\nTaking derivatives is not stable (…)\nNeeds many function evaluations\n\n\\(\\mathbf{H}\\) not guaranteed to be non-singular\n\nProblem is in step\n\\[\\mathbf{x}_{n+1} - \\mathbf{x}_n = -\\mathbf{H}^{-1}(\\mathbf{x}_n) \\nabla f(\\mathbf{x}_n)\\]\nReplace \\(\\mathbf{H}(\\mathbf{x}_n)\\) by some \\(\\mathbf{M}_n\\), invertible by construction?",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Constrained Optimization and Pitfalls in Optimization</span>"
    ]
  },
  {
    "objectID": "constrainedandpitfalls.html#recap-of-newtons-method-in-higher-dimensions",
    "href": "constrainedandpitfalls.html#recap-of-newtons-method-in-higher-dimensions",
    "title": "\n10  Constrained Optimization and Pitfalls in Optimization\n",
    "section": "\n10.12 Recap of Newton’s method in higher dimensions",
    "text": "10.12 Recap of Newton’s method in higher dimensions\nFrom last lecture we have the following quadratic approximation of the objective function \\(f\\):\n\\[f(\\mathbf{x}_{n+1}) \\approx f(\\mathbf{x}_n) + \\mathbf{s}_n' \\nabla f(\\mathbf{x}_n) + \\frac{1}{2} \\mathbf{s}_n' \\mathbf{H}(\\mathbf{x}_n) \\mathbf{s}_n,\\]\nwhere \\(\\mathbf{s}_n = \\mathbf{x}_{n+1} - \\mathbf{x}_n\\).\nTaking first-order partial derivatives on both sides with respect to \\(s_n\\), we get:\n\\[\\nabla f(\\mathbf{x}_{n+1}) \\approx \\nabla f(\\mathbf{x}_n) + \\mathbf{H}(\\mathbf{x}_n) \\mathbf{s}_n\\]\nTo solve for the next step, set \\(\\nabla f(\\mathbf{x}_{n+1}) = 0\\) and rearrange:\n\\[\\mathbf{s}_n = -\\mathbf{H}(\\mathbf{x}_n)^{-1} \\nabla f(\\mathbf{x}_n)\\]\nInstead of \\(\\mathbf{H}(\\mathbf{x}_n)\\), we could use some \\(\\mathbf{M}_n\\), which has nice properties:\n\\[\\mathbf{s}_n = -\\mathbf{M}_n^{-1} \\nabla f(\\mathbf{x}_n)\\]",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Constrained Optimization and Pitfalls in Optimization</span>"
    ]
  },
  {
    "objectID": "constrainedandpitfalls.html#broyden-fletcher-goldfarb-and-shanno-bfgs",
    "href": "constrainedandpitfalls.html#broyden-fletcher-goldfarb-and-shanno-bfgs",
    "title": "\n10  Constrained Optimization and Pitfalls in Optimization\n",
    "section": "\n10.13 Broyden, Fletcher, Goldfarb and Shanno (BFGS)",
    "text": "10.13 Broyden, Fletcher, Goldfarb and Shanno (BFGS)\nChoose \\(\\mathbf{M}_{n+1}\\) such that it satisfies the “secant equation”\n\\[\\mathbf{M}_{n+1}(\\mathbf{x}_{n+1}-\\mathbf{x}_{n}) = \\nabla f(\\mathbf{x}_{n+1}) - \\nabla f(\\mathbf{x}_{n}) \\iff \\mathbf{M}_{n+1} \\mathbf{s}_{n} = \\mathbf{y}_{n},\\]\nwhere \\(\\mathbf{y}_{n} = \\nabla f(\\mathbf{x}_{n+1}) - \\nabla f(\\mathbf{x}_{n})\\)\nWhen \\(\\mathbf{x}_{n+1}\\) is close to \\(\\mathbf{x}_{n}\\), then \\(\\mathbf{H}(\\mathbf{x}_{n+1}) \\approx \\mathbf{M}_{n+1}\\)\nTo determine \\(\\mathbf{M}_{n+1}\\) uniquely, then, we impose the additional condition that among all symmetric matrices satisfying the secant equation, \\(\\mathbf{M}_{n+1}\\) is, in some sense, closest to the current matrix \\(\\mathbf{M}_n\\)\n\\[\\mathbf{M}_{n+1} = \\min_\\mathbf{M} ||\\mathbf{M} - \\mathbf{M}_n||\\]\nsubject to \\(\\mathbf{M} = \\mathbf{M}'\\), \\(\\mathbf{M}\\mathbf{s}_n = \\mathbf{y}_n\\)\nThe solution:\n\\[\\mathbf{M}_{n+1} = \\mathbf{M}_n + \\frac{\\mathbf{y}_n \\mathbf{y}_n'}{\\mathbf{y}_n \\mathbf{s}_n'} - \\frac{\\mathbf{M}_n \\mathbf{s}_n \\mathbf{s}_n' \\mathbf{M}_n'}{\\mathbf{s}_n' \\mathbf{M}_n \\mathbf{s}_n}\\]\n\nStart with \\(n=0\\) and positive definite \\(\\mathbf{M}_0\\), e.g. \\(\\mathbf{M}_0 = I\\)\n\nCalculate \\(\\mathbf{s}_n = -\\mathbf{M}_n^{-1} \\nabla f(\\mathbf{x}_n)\\)\n\nFind new \\(\\mathbf{x}_{n+1} = \\mathbf{x}_n + \\mathbf{s}_n\\) (or with line search: \\(\\mathbf{x}_{n+1} = \\mathbf{x}_n + \\alpha_n \\mathbf{s}_n\\))\nCalculate, with \\(\\mathbf{y}_n = \\nabla f(\\mathbf{x}_{n+1}) - \\nabla f(\\mathbf{x}_n)\\) \\[\\mathbf{M}_{n+1} = \\mathbf{M}_n + \\frac{\\mathbf{y}_n \\mathbf{y}_n'}{\\mathbf{y}_n' \\mathbf{s}_n} - \\frac{\\mathbf{M}_n \\mathbf{s}_n \\mathbf{s}_n' \\mathbf{M}_n'}{\\mathbf{s}_n' \\mathbf{M}_n \\mathbf{s}_n}\\]\n\nThis produces a symmetric and positive definite matrix at each step if the initialization is a symmetric and positive definite matrix, like the identity matrix.\nWe can use the Sherman-Morrison formula to calculate \\(\\mathbf{M}_{n+1}^{-1}\\) efficiently.\n\nResult: * No Hessian needed * Still fast convergence * No problems with singular \\(H_n\\)",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Constrained Optimization and Pitfalls in Optimization</span>"
    ]
  },
  {
    "objectID": "constrainedandpitfalls.html#example-regression-likelihood",
    "href": "constrainedandpitfalls.html#example-regression-likelihood",
    "title": "\n10  Constrained Optimization and Pitfalls in Optimization\n",
    "section": "\n10.14 Example: Regression Likelihood",
    "text": "10.14 Example: Regression Likelihood\n\\[y_i = X_i \\beta + \\epsilon_i \\qquad \\epsilon_i \\sim N(0, \\sigma^2)\\]\nML maximises likelihood (other options: Minimise sum-of-squares, optimise utility etc):\n\\[L(y; \\theta) = \\prod_i \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left( -\\frac{(y_i - X_i \\beta)^2}{2\\sigma^2} \\right)\\] \\[= (2\\pi \\sigma^2)^{-\\frac{N}{2}} \\exp \\left( -\\frac{1}{2\\sigma^2} (y - X\\beta)' (y - X\\beta) \\right)\\]\nIn this case, \\(\\theta = (\\beta, \\sigma^2)\\)\nAnd in order to write the likelihood:\nTo remember:\n\\[L(y; \\theta) = (2\\pi \\sigma^2)^{-\\frac{N}{2}} \\exp \\left( -\\frac{1}{2\\sigma^2} (y - X\\beta)' (y - X\\beta) \\right)\\]\n\\[\\log L(y; \\theta) = -\\frac{1}{2} \\left( N \\log 2\\pi + N \\log \\sigma^2 + \\frac{e'e}{\\sigma^2} \\right)\\]\nIn this case, \\(\\theta = (\\beta, \\sigma)\\) or \\(\\theta = (\\beta, \\sigma^2)\\).\n\nExtract your parameters from the vector, use sensible names\nCheck if your parameters are valid, i.e. \\(\\sigma^2 &gt; 0\\)?\nAnd test…",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Constrained Optimization and Pitfalls in Optimization</span>"
    ]
  },
  {
    "objectID": "constrainedandpitfalls.html#optim-continued",
    "href": "constrainedandpitfalls.html#optim-continued",
    "title": "\n10  Constrained Optimization and Pitfalls in Optimization\n",
    "section": "\n10.15 optim continued",
    "text": "10.15 optim continued\nFunction to maximize should have format\noptim(par, fn, gr = NULL, ..., method = c(\"BFGS\", \"CG\"), lower = -Inf, upper = Inf, control = list(), hessian = FALSE)\nwhere\n\n\npar initial values for the parameters to be optimized over.\n\nfn a function to be minimized\n\ngr a function to return the gradient for the “BFGS” and “CG” methods. If it is NULL, a finite-difference approximation will be used.\n\nThe output of fn has to be the negative average log-likelihood value evaluated in vPar.\nWhy negative? Because by convention optimizers in R by default are minimizer. This option can be modified using arguments of the optimizer function.\nThe output from optim is a list with named elements:\n\n\npar: The estimated vector of parameters\n\nvalue: The value of fn corresponding to par.\n\n\ncounts: A two-element integer vector giving the number of calls to fn and gr respectively.\n\nconvergence: An integer code. 0 indicates successful completion, see help(optim) for other labels.\n\nmessage: A character string giving any additional information returned by the optimizer, or NULL.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Constrained Optimization and Pitfalls in Optimization</span>"
    ]
  },
  {
    "objectID": "constrainedandpitfalls.html#code-for-pitfalls-and-bfgs",
    "href": "constrainedandpitfalls.html#code-for-pitfalls-and-bfgs",
    "title": "\n10  Constrained Optimization and Pitfalls in Optimization\n",
    "section": "\n10.16 Code for pitfalls and BFGS",
    "text": "10.16 Code for pitfalls and BFGS\n\n\n\n\n\n\nClick to view full code associated with the first part\n\n\n\n\n\n\n##  Purpose: Constrained Optimization\n##    Example with Gaussian Log-Likelihood + Gradient Transform in unconstrained \n##    optimizer\n##\n\n##    Compute the negative average log likelihood\n\nNegAvgLL &lt;- function(vPar, vY, mX) {\n  \n  dSigma &lt;- vPar[1]       ## Extracting the first parameter from vPar\n  vBeta  &lt;- vPar[-1]      ## Extracting the second parameter from vPar\n  vMean  &lt;- mX %*% vBeta\n  vEps   &lt;- vY - vMean\n  \n  iN &lt;- length(vY)\n  dSumSquareRes &lt;- as.numeric(t(vEps) %*% vEps)\n  \n  dLLK &lt;- -0.5 * (iN * log(dSigma^2) + dSumSquareRes / dSigma^2) ## Log likelihood function\n \n  return(-dLLK / iN)\n}\n\n\n\n\n##  Simulate from the DGP\nset.seed(12)\niT &lt;- 100 # Number of observations\n\nmX     &lt;- matrix(rnorm(iT * 3), ncol = 3)  # A matrix of randomly drawn values from the normal distribution\nvBeta  &lt;- c(1, 2, 3)                       # True beta values\ndSigma &lt;- 3                                # True variance\nvY     &lt;- mX %*% vBeta + dSigma * rnorm(iT)     \n\n\n##  Estimate parameters using BFGS\nvPar0 &lt;- c(0.1, c(0, 1, 4)) #problem: flat gradient\nFit0 &lt;- optim(vPar0, NegAvgLL, vY = vY, mX = mX, method = \"BFGS\")\nprint(Fit0)\n#&gt; $par\n#&gt; [1] 422.3164913   2.2633722   5.4705809   0.3252097\n#&gt; \n#&gt; $value\n#&gt; [1] 6.045838\n#&gt; \n#&gt; $counts\n#&gt; function gradient \n#&gt;      102      100 \n#&gt; \n#&gt; $convergence\n#&gt; [1] 1\n#&gt; \n#&gt; $message\n#&gt; NULL\n\n\nvPar1 &lt;- c(-1, c(0, 1, 4))  #problem: negative sigma solution\nFit1 &lt;- optim(vPar1, NegAvgLL, vY = vY, mX = mX, method = \"BFGS\")\nprint(Fit1)\n#&gt; $par\n#&gt; [1] -2.8384475  0.8679416  2.0497271  3.0799090\n#&gt; \n#&gt; $value\n#&gt; [1] 1.543322\n#&gt; \n#&gt; $counts\n#&gt; function gradient \n#&gt;       70       65 \n#&gt; \n#&gt; $convergence\n#&gt; [1] 0\n#&gt; \n#&gt; $message\n#&gt; NULL\n\n\n\n\n## Now using reparameterized problem ##\n\n\n#reparameterized neg LL\nNegAvgLL.t &lt;- function(vPar, vY, mX) {\n  \n  dSigma.t &lt;- vPar[1]       ## Extracting the first parameter from vPar\n  vBeta    &lt;- vPar[-1]      ## Extracting the second parameter from vPar\n  vMean    &lt;- mX %*% vBeta\n  vEps     &lt;- vY - vMean\n  \n  iN &lt;- length(vY)\n  dSumSquareRes &lt;- as.numeric(t(vEps) %*% vEps)\n  \n  dLLK &lt;- -0.5 * (iN * dSigma.t + dSumSquareRes / exp(dSigma.t) ) ## Log likelihood function\n  \n  return(-dLLK / iN)\n}\n\n\n#unconstrained optimization\nFit.t &lt;- optim(vPar0, NegAvgLL.t, vY = vY, mX = mX, method = \"BFGS\") #flat gradient case\n#Fit.t &lt;- optim(vPar1, NegAvgLL.t, vY = vY, mX = mX, method = \"BFGS\") #neg sigma case\nprint(Fit.t)\n#&gt; $par\n#&gt; [1] 2.0865283 0.8670166 2.0497514 3.0794685\n#&gt; \n#&gt; $value\n#&gt; [1] 1.543322\n#&gt; \n#&gt; $counts\n#&gt; function gradient \n#&gt;       18       14 \n#&gt; \n#&gt; $convergence\n#&gt; [1] 0\n#&gt; \n#&gt; $message\n#&gt; NULL\n\n#re-transform estimates\nestimates &lt;- c( sqrt(exp(Fit.t$par[1])) , Fit.t$par[-1] )\nprint(estimates)\n#&gt; [1] 2.8384671 0.8670166 2.0497514 3.0794685\n\n#sanity check: compare -LL at corresponding solutions\nNegAvgLL.t(Fit.t$par, vY = vY, mX = mX)\n#&gt; [1] 1.543322\nNegAvgLL(estimates, vY = vY, mX = mX)\n#&gt; [1] 1.543322\n\n#compare to problematic solutions above: \nNegAvgLL(Fit0$par, vY = vY, mX = mX) #failed\n#&gt; [1] 6.045838\nNegAvgLL(Fit1$par, vY = vY, mX = mX) #worked but yielded negative sigma\n#&gt; [1] 1.543322\nNegAvgLL.t(Fit.t$par, vY = vY, mX = mX)\n#&gt; [1] 1.543322",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Constrained Optimization and Pitfalls in Optimization</span>"
    ]
  },
  {
    "objectID": "cpp.html",
    "href": "cpp.html",
    "title": "\n11  Integration of C++ Code with Rcpp\n",
    "section": "",
    "text": "11.1 Introduction\nR is both a powerful interactive environment for data analysis, visualization, and modeling and an expressive programming language designed and built to support these tasks. However, sometimes R code just isn’t fast enough. Generally, performance that relates to speed is improved by C++.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integration of C++ Code with Rcpp</span>"
    ]
  },
  {
    "objectID": "cpp.html#rcpp",
    "href": "cpp.html#rcpp",
    "title": "\n11  Integration of C++ Code with Rcpp\n",
    "section": "\n11.2 Rcpp",
    "text": "11.2 Rcpp\nRcpp is an R add-on package which makes it very simple to connect R with C++. While still relatively new (2008) as a project, Rcpp has already become widely deployed among users and developers in the R community. Rcpp continues to be under active development, and extensions are being added. It is now the most popular language extension for the R system and used by over 2700 CRAN Packages. This is about 14% of all R packages on CRAN, and 60% of all packages that use compiled code.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integration of C++ Code with Rcpp</span>"
    ]
  },
  {
    "objectID": "cpp.html#r-versus-c",
    "href": "cpp.html#r-versus-c",
    "title": "\n11  Integration of C++ Code with Rcpp\n",
    "section": "\n11.3 R versus C++",
    "text": "11.3 R versus C++\nOne of the key differences between R and C++ is that R is interpreted and C++ is compiled.\nWith a compiled language, code you enter is reduced or ‘compiled’ into the ‘target language’ (that is, the actual instructions in machine code). Once the translation is complete, the executable code is either run or set aside to run later.\nWith interpreted languages, the code is saved in the same format that you entered and is reduced/compiled to machine-specific instructions at runtime.\nThe interpreted languages thus allow for more flexibility because a program can be adaptively modified and there is no need for an additional ‘compilation stage’.\nThe flexibility, however, comes at a cost; interpreted languages are significantly slower, since the compilation of the code happens every time it is run (think of loops).\nTypical bottlenecks that C++ can address includes:\n\nLoops that can’t easily be vectorized because subsequent iterations depend on previous ones.\nRecursive functions, or problems which include calling functions many times. The overhead (resources required) of calling a function in C++ is much lower than that in R.\nProblems that require advanced data structures and algorithms that R doesn’t provide. C++ has efficient implementations of many advanced data structures that R does not provide, such as ordered maps or double-ended queues.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integration of C++ Code with Rcpp</span>"
    ]
  },
  {
    "objectID": "cpp.html#basic-c-in-r",
    "href": "cpp.html#basic-c-in-r",
    "title": "\n11  Integration of C++ Code with Rcpp\n",
    "section": "\n11.4 Basic C++ in R",
    "text": "11.4 Basic C++ in R\nIn the following, we will consider how to write basic C++ code by converting simple R functions to their C++ equivalents.\nAll examples need version 0.10.1 or above of the Rcpp package. This version includes the functions cppFunction() and sourceCpp(), which make it very easy to connect C++ to R.\nInstall the latest version of Rcpp from CRAN with:\n\ninstall.packages(\"Rcpp\")\n\nYou’ll also need a working compiler:\n\nOn Windows, install ‘Rtools’\nOn Mac, install ‘Xcode’ from the app store and some other things need to be sorted.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integration of C++ Code with Rcpp</span>"
    ]
  },
  {
    "objectID": "cpp.html#c-functions-in-r",
    "href": "cpp.html#c-functions-in-r",
    "title": "\n11  Integration of C++ Code with Rcpp\n",
    "section": "\n11.5 C++ functions in R",
    "text": "11.5 C++ functions in R\ncppFunction() allows you to write C++ functions in R:\n\nsuppressMessages(library(Rcpp))\n#&gt; Warning: pakke 'Rcpp' blev bygget under R version 4.3.3\n\ncppFunction('\nint add(int x, int y, int z) {\n  int sum = x + y + z;\n  return sum;\n}')\n\nWhen you run this code, Rcpp will compile the C++ code and construct an R function that connects to the compiled C++ function. You can then run the function in R:\n\nadd(1, 2, 3)\n#&gt; [1] 6\n\nThe following will teach you the basics of C++ by translating simple R functions to their C++ equivalents.\n\n11.5.1 Example: No inputs, scalar output\nWe will start with a very simple function that takes no arguments and always returns the integer 1:\n\noneR &lt;- function() 1L # R function\n\ncppFunction('\nint oneCpp() { # C++ function\n  return 1,\n}')\n\nThe syntax to create a function in C++ looks like the syntax to call a function in R.\nYou must declare the type of output the function returns before the function name; this function returns an int (a scalar integer).\nYou must use an explicit return statement to return a value from a function (which is not necessary in R).\nEvery statement is terminated by a semicolon, ;.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integration of C++ Code with Rcpp</span>"
    ]
  },
  {
    "objectID": "cpp.html#variables-declation",
    "href": "cpp.html#variables-declation",
    "title": "\n11  Integration of C++ Code with Rcpp\n",
    "section": "\n11.6 Variables declation",
    "text": "11.6 Variables declation\nA main difference between R and C++ is the declaration of variables. In R, we that that we can write commands like:\n\niN &lt;- 5\ndK &lt;- 0.156532\nbJ &lt;- TRUE\nsT &lt;- \"Hello World\"\n\nDifferently in C++, we need to specify the class of the variable before its definition:\n\nint    iN = 5;             // integer\ndouble dK = 0.156352;      // double\nbool   bJ = true;          // boolean\nstring st = \"Hello World\"; // string\n\nAlso note that the logical value TRUE is indicated as true (same for FALSE).\nComments are indicated with // instead of #;\nThe arrows that we use to assign values to variables in R do not make sense in C++.\n\n11.6.1 Example: scalar input, scalar output\n\nsignR &lt;- function(iX) {\n  if (iX &gt; 0) {\n    1\n  } else if (iX == 0) {\n    0\n  } else {\n    -1\n  }\n}\n\ncppFunction('int SignC(int x) {\n  if (x &gt; 0) {\n    return 1;\n  } else if (x == 0) {\n    return 0;\n  } else {\n    return -1;\n  }\n}')\n\nFrom the example above, it can be noted that it is needed to declare the function in the same way as the output. The if-statement syntax in C++ is identical to that in R. C++ also has a while-loop that works in the same way as R’s.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integration of C++ Code with Rcpp</span>"
    ]
  },
  {
    "objectID": "cpp.html#for-loops-in-c",
    "href": "cpp.html#for-loops-in-c",
    "title": "\n11  Integration of C++ Code with Rcpp\n",
    "section": "\n11.7 For loops in C++",
    "text": "11.7 For loops in C++\nOne of the main reasons for including C++ code in our R program is to speed up the evaluation of loops. Indeed, there are situations where, unfortunately, we cannot avoid making a loop, e.g., when subsequent iterations depend on previous ones.\nFor loops in C++ have a different syntax than those in R:\n\nfor (int i = 0 (init); i &lt; 10 (check); i++ (increment)) {\n  // some code here\n}\n\nNote that we declare the class of the loop index i (this has to be an integer), we indicate the start (i = 0), the end (i &lt; 10) and the increment at each iteration (i++).\nThe notation i++ indicates that the variable i needs to be augmented by 1 at each iteration. The notation i-- indicates that the value of i needs to be decreased by 1 at each iteration.\n\n11.7.1 Example: vector input, scalar output\n\nsumR &lt;- function(vX) {\n  dTotal &lt;- 0\n  for (i in 1:length(vX)) {\n    dTotal &lt;- dTotal + vX[i]\n  }\n  return(dTotal)\n}\n\ncppFunction('\ndouble sumC(NumericVector x) {\n  int n = x.size();\n  double total = 0;\n  for(int i = 0; i &lt; n; i++) {\n    total += x(i);\n  }\n  return total;\n}')\n\nThe classes for the most common types of R vectors are: NumericVector, IntegerVector, CharacterVector, and LogicalVector.\nTo find the length of the vector, we use the .size() method, which returns an integer. In general, C++ methods are called with ‘.’.\nC++ is zero based! The first element of a vector is indexed at 0 and not at 1.\nThe call total += x(i) is equivalent to total = total + x(i). Similar operators are -=, *=, /=.\nLoops are not at all as slow compared to vectorized programs in C++ as in R, so it is fine to use loops in C++. In R, on the other hand, we always want to use vector operations when possible, because looping is just too slow in comparison.\n\n11.7.2 Efficiency of loops: C++ vs. R\nThe above example is a situation where C++ is much more efficient than R:\n\nsuppressMessages(library(microbenchmark))\n#&gt; Warning: pakke 'microbenchmark' blev bygget under R version 4.3.3\n\nvX &lt;- runif(1e4)\nmicrobenchmark(\n  sum(vX),\n  sumC(vX),\n  sumR(vX)\n)\n#&gt; Unit: microseconds\n#&gt;      expr   min    lq    mean median     uq    max neval\n#&gt;   sum(vX)   7.4   7.6   7.997   7.70   8.10   14.1   100\n#&gt;  sumC(vX)  21.8  22.1  32.729  22.45  24.25  801.0   100\n#&gt;  sumR(vX) 253.3 256.3 290.191 257.80 261.55 2424.4   100\n\nThe sumC function even come somewhat close to beating the built-in (and highly optimized) R function sum(), while sumR() is significantly slower.\n\n11.7.3 Example: vector input, vector output\nNext, we’ll create a function that computes the Euclidean distance between a value and a vector of values:\n\npdistR &lt;- function(dX, vY) {\n  sqrt((dX - vY) ^ 2)\n}\n\ncppFunction('\nNumericVector pdistC(double x, NumericVector y) {\n  int n = y.size();\n  NumericVector out(n);\n  \n  for (int i = 0; i &lt; n; i++) {\n    out(i) = sqrt(pow(y(i) - x, 2.0));\n  }\n  return out;\n}')\n\nWe create a new numeric vector of length n with a constructor: NumericVector out(n). Another useful way of makin a vector is to copy an existing one: NumericVector zs = close(ys).\nC++ uses the pow() and not ^ for exponentation.\nEven though an R function might be fully vectorized, a C++ function can stil be faster. However, it also takes longer to write the C++ function.\nOther useful math functions in C++: min(), max(), abs(), sin(), cos(), tan(), exp(). log(), ceil(), floor(), round()…\n\n11.7.4 Example: matrix input, vector output\nConsider the C++ function that reproduces rowSums():\n\ncppFunction('\nNumericVector rowSumsC(NumericMatrix x) {\n  int nrow = x.nrow(), ncol = x.ncol();\n  NumericVector out(nrow);\n  \n  for (int i = 0; i &lt; nrow; i++) {\n    double total = 0;\n    for (int j = 0; j &lt; ncol; j++) {\n      total += x(i, j);\n    }\n    out(i) = total;\n  }\n  return out;\n}')\n\nCalling it, we get:\n\nset.seed(1014)\nmX &lt;- matrix(sample(100), 10)\nrowSums(mX)\n#&gt;  [1] 446 514 480 514 352 627 525 586 572 434\nrowSumsC(mX)\n#&gt;  [1] 446 514 480 514 352 627 525 586 572 434\n\nEach vector type has a matrix equivalent: NumericMatrix, IntegerMatrix, Charactermatrix, and LogicalMatrix.\nIn C++, you subset a matrix with () and not [].\nUse .nrow() and .ncol() methods to get the dimensions of a matrix.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integration of C++ Code with Rcpp</span>"
    ]
  },
  {
    "objectID": "cpp.html#using-sourcecpp",
    "href": "cpp.html#using-sourcecpp",
    "title": "\n11  Integration of C++ Code with Rcpp\n",
    "section": "\n11.8 Using sourceCpp",
    "text": "11.8 Using sourceCpp\nSo far, we have used inline C++ with cppFunction. This makes presentation simpler, but for real problems it is usually easier to use stand-alone C++ files and then source them into R using sourceCpp().\nYour stand-alone C++ files should have extension .cpp and needs to start with:\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\nAnd for each function that you want available within R, you need to prefix it with:\n// [[Rcpp::export]]\nYou can embed R code in special C++ comment blocks, which is really convenient if you want to run some test code. The blocks look as follows:\n\n/*** R\n# This is R code\n*/\n\nTo compile the C++ code, use sourceCpp(\"path/to/file.cpp\"). This will create the matching R functions and add them to your current session.\nNote that these C++ functions cannot be saved in a .Rdata file and be reloaded in a later sesions; they muse be recreated/recompiled each time you restart R.\nAs an example, running sourceCpp() on the following file implements mean in C++ and then compares it to the built-in R function mean().\n\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\ndouble meanC(NumericVector x) {\n  int n = x.size();\n  double total = 0;\n  for (int i = 0; i &lt; n; i++) {\n    total += x(i);\n  }\n  return total / n;\n}\n\n\n/*** R\nsuppressMessages(library(microbenchmark))\nvX &lt;- runif(1e5)\nmicrobenchmark(mean(vX), meanC(vX))\n*/\n\n\nsourceCpp(\"CppSource.cpp\")\n#&gt; \n#&gt; &gt; suppressMessages(library(microbenchmark))\n#&gt; \n#&gt; &gt; vX &lt;- runif(1e+05)\n#&gt; \n#&gt; &gt; microbenchmark(mean(vX), meanC(vX))\n#&gt; Unit: microseconds\n#&gt;       expr   min     lq    mean median     uq    max neval\n#&gt;   mean(vX) 152.3 154.35 160.467 156.15 161.75  227.2   100\n#&gt;  meanC(vX) 221.0 223.25 235.814 224.60 226.60 1013.8   100\nsuppressMessages(library(microbenchmark))\nvX &lt;- runif(1e5)\nmicrobenchmark(\n  mean(vX),\n  meanC(vX)\n)\n#&gt; Unit: microseconds\n#&gt;       expr   min     lq    mean median     uq   max neval\n#&gt;   mean(vX) 152.2 155.00 172.800 158.45 171.95 436.0   100\n#&gt;  meanC(vX) 221.1 224.25 242.466 225.95 234.90 608.6   100",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integration of C++ Code with Rcpp</span>"
    ]
  },
  {
    "objectID": "cpp.html#the-fibonacci-sequence",
    "href": "cpp.html#the-fibonacci-sequence",
    "title": "\n11  Integration of C++ Code with Rcpp\n",
    "section": "\n11.9 The Fibonacci sequence",
    "text": "11.9 The Fibonacci sequence\nConsider the problem of computing the Fibonacci sequence. The Fibonacci numbers \\(F_n\\) are defined as a recursive sum of the two preceding terms in the same sequence:\n\\[\nF_n = F_{n-1} + F_{n-2}\n\\]\nwith initial conditions \\(F_0 = 0\\) and \\(F_1 = 1\\). For example, the first ten numbers of the sequence \\(F_0\\) to \\(F_9\\) are seen to be:\n\\[\n0, 1, 1, 2, 3, 5, 8, 13, 21, 34\n\\]\n\n11.9.1 Fibonacci in R\nUsing recursive programming, we can compute the Fibonacci numbers in R:\n\nfibR &lt;- function(n) {\n  if (n == 0) return(0)\n  if (n == 1) return(1)\n  return(fibR(n - 1) + fibR(n - 2))\n}\n\nThis simple function has several key features:\n\nIt is very short.\nIt does not test for wrong input values less than zero.\nIt is easy to comprehenad.\nIt is a very faithful execution of the relationship \\(F_n = F_{n-1} + F_{n-2}\\).\n\nHowever, it is very insufficient; it can be shown that the algorithm is exponential in \\(n\\) - or in other words, its run-time increases at an exponential rate relative to the argument \\(n\\).\n\n11.9.2 Fibonacci in C++\nA simple solution to compute \\(F_n\\) much faster using the same simple and intuitive algorithm is to switch to C++. We can write a simple C++ script which contains a function for the Fibonacci number evaluation:\n\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nint fibC(int x) {\n  if (x == 0) return(0);\n  if (x == 1) return(1);\n  return(fibC(x - 1) + fibC(x - 2));\n}\n\n\n11.9.3 Comparing fibR and fibC\nTo load the functions inside fibonacci.cpp which we want to export (those with // [[Rcpp::export]]), we simply run:\n\nsuppressMessages(library(Rcpp))\nsourceCpp(\"fibonacci.cpp\")\nfibC(25)\n#&gt; [1] 75025\n\nComparing the two functions:\n\nsuppressMessages(library(microbenchmark))\nmicrobenchmark(fibR(25), fibC(25))\n#&gt; Unit: microseconds\n#&gt;      expr     min      lq      mean   median       uq      max neval\n#&gt;  fibR(25) 91158.3 93283.2 95861.965 94086.75 95356.65 140411.3   100\n#&gt;  fibC(25)   119.2   121.3   155.378   133.45   153.70   1697.3   100",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integration of C++ Code with Rcpp</span>"
    ]
  },
  {
    "objectID": "cpp.html#vector-matrices-and-linear-algebra-in-c",
    "href": "cpp.html#vector-matrices-and-linear-algebra-in-c",
    "title": "\n11  Integration of C++ Code with Rcpp\n",
    "section": "\n11.10 Vector, Matrices and Linear Algebra in C++",
    "text": "11.10 Vector, Matrices and Linear Algebra in C++\nC++ does not come with routines that implement linear algebra like in R. However, similar to R, we can include external libraries. One of the most used libraries for matrix calculus in C++ is armadillo. In order to link R with C++ code which exploits armadillo, we can use the RcppArmadillo package for R.\nInstall the RcppArmadillo package from CRAN:\n\ninstall.packages(\"RcppArmadillo\")\n\nFeatures of RcppArmadillo include:\n\nBetter vector and matrix types; arma::vec, arma::mat.\nVery fast routines for linear algebra operations, e.g., matrix multiplication, matrix inversion, linear system solving etc.\n\nhttps://arma.sourceforge.net/docs.html: documentation for all Armadillo functions. Great descriptions, examples, and links to related functions.\n\n11.10.1 Example\nThe following code is saved in the file armadilloexample.cpp:\n\n// [[Rcpp::depends(RcppArmadillo)]]\n#include &lt;RcppArmadillo.h&gt;\nusing namespace Rcpp;\n\n\n// [[Rcpp:export]]\narma::mat matprodC(arma::mat m1, arma::mat m2) {\n  if(m1.n_cols != m2.n_rows)\n    stop(\"Incompatible matrix dimensions\");\n\n  return m1 * m2;\n}\n\nAnd then used in R:\n\nsuppressMessages(library(RcppArmadillo))\n#&gt; Warning: pakke 'RcppArmadillo' blev bygget under R version 4.3.3\nsourceCpp(\"armadilloexample.cpp\")\nm1 &lt;- matrix(1:9, 3, 3)\nm2 &lt;- matrix(1:9, 3, 3)\n\nmatprodC(m1, m2)\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]   30   66  102\n#&gt; [2,]   36   81  126\n#&gt; [3,]   42   96  150\n\nm1 %*% m2\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]   30   66  102\n#&gt; [2,]   36   81  126\n#&gt; [3,]   42   96  150",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integration of C++ Code with Rcpp</span>"
    ]
  },
  {
    "objectID": "cpp.html#inverse-of-matrix-product",
    "href": "cpp.html#inverse-of-matrix-product",
    "title": "\n11  Integration of C++ Code with Rcpp\n",
    "section": "\n11.11 Inverse of Matrix Product",
    "text": "11.11 Inverse of Matrix Product\nConsider the R function:\n\nFunR &lt;- function(mX, mY) {\n  mZ &lt;- mX %*% mY\n  mZInv &lt;- solve(mZ)\n  return(mZInv)\n}\n\nThis function performs a matrix multiplication between mX and mY, and then returns the inverse of the product.\nConsider now the C++ implementation exploiting the RcppArmadillo package:\n\n// [[Rcpp:depends(RcppArmadillo)]]\n#include &lt;RcppArmadillo.h&gt;\nusing namespace arma;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nmat FunC(mat mx, mat mY) {\n  mat mZ = mx * mY;\n  mat mZInv = mZ.i();\n  return mZInv;\n}\n\nComparison:\n\nsuppressMessages(library(RcppArmadillo))\nsourceCpp(\"mycfunction.cpp\")\nM &lt;- 300\nmX &lt;- matrix(rnorm(M^2), M)\nmY &lt;- matrix(rnorm(M^2), M)\n\nmicrobenchmark(FunR(mX, mY), FunC(mX, mY))\n#&gt; Unit: milliseconds\n#&gt;          expr    min      lq     mean   median       uq     max neval\n#&gt;  FunR(mX, mY) 35.571 36.1687 36.73813 36.44685 36.74690 42.2427   100\n#&gt;  FunC(mX, mY) 28.627 29.0529 29.69111 29.66330 29.97065 34.9298   100",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integration of C++ Code with Rcpp</span>"
    ]
  },
  {
    "objectID": "cpp.html#calling-r-functions-from-c",
    "href": "cpp.html#calling-r-functions-from-c",
    "title": "\n11  Integration of C++ Code with Rcpp\n",
    "section": "\n11.12 Calling R Functions from C++",
    "text": "11.12 Calling R Functions from C++\nIt is very easy to load C++ functions in R using sourceCpp(). Using Rcpp, it is possible to load R functions inside C++ functions. See: https://gallery.rcpp.org/articles/r-function-from-c++/.\nRcpp permits easy access to native R objects at the C++ level. Calling an R function in C++ can be very useful. For example to pick up parameter initializations, maybe to access a custom data summary that would be tedious to recode, or maybe even calling a plotting routine.\nDefine in R the following function which returns the sum of squares of a vector:\n\nSumOfSquaresR &lt;- function(VX) {\n  dOut &lt;- sum(vX^2)\n  return(dOut)\n}\nvX &lt;- rnorm(10)\nSumOfSquaresR(vX)\n#&gt; [1] 15.98534\n\nConsider now the following C++ code:\n\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n\n// [[Rcpp::export]]\nSEXP callRFunction(NumericVector vX, Function f) {\n  SEXP res = f(vX);\n  return res;\n}\n\nNote the SEXP class for R objects in C++ and the Function class for general functions in C++. The code can be saved in a script callRFunction.cpp in the working directory and be loaded with the sourceCpp function as usual.\nAfter running:\n\nsourceCpp(\"callRFunction.cpp\")\n\nWe have the callRFunction available in R. Run:\n\ncallRFunction(vX, SumOfSquaresR)\n#&gt; [1] 15.98534\nSumOfSquaresR(vX)\n#&gt; [1] 15.98534\n\nDefine now the function sum of absolute values:\n\nSumOfAbsoluteValuesR &lt;- function(vX) {\n  dOut &lt;- sum(abs(vX))\n  return(dOut)\n}\n\nWe can use the same function callRFunction to execute SumOfAbsoluteValuesR in C++ as:\n\ncallRFunction(vX, SumOfAbsoluteValuesR)\n#&gt; [1] 10.25095\nSumOfAbsoluteValuesR(vX)\n#&gt; [1] 10.25095\n\nThat is, callRFunction(NumericVector vX, Function f) is independent from the definition of f.\n\n11.12.1 Warning\n\nsuppressMessages(library(microbenchmark))\nvX &lt;- rnorm(1e6)\nmicrobenchmark(\n  callRFunction(vX, SumOfSquaresR),\n  SumOfSquaresR(vX)\n)\n#&gt; Unit: milliseconds\n#&gt;                              expr    min      lq     mean  median      uq\n#&gt;  callRFunction(vX, SumOfSquaresR) 2.1440 2.28475 3.276364 2.35835 2.90405\n#&gt;                 SumOfSquaresR(vX) 2.0895 2.25540 2.979320 2.36685 2.65355\n#&gt;     max neval\n#&gt;  6.9373   100\n#&gt;  6.4855   100\n\nCalling R functions in C++ is slower than calling R functions in R.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integration of C++ Code with Rcpp</span>"
    ]
  },
  {
    "objectID": "cpp.html#the-rcpp-family",
    "href": "cpp.html#the-rcpp-family",
    "title": "\n11  Integration of C++ Code with Rcpp\n",
    "section": "\n11.13 The Rcpp Family",
    "text": "11.13 The Rcpp Family\nRcpp and RcppArmadillo are not the only two packages that belong to the Rcpp family. There also exist:\n\nThe RcppGSL package provides an easy-to-use interface between data structures from the GNU Scientific Library (https://www.gnu.org/software/gsl/).\nThe RcppEigen package provides an interface to the Eigen library with linear algebra: matrices, vectors, numerical solvers, and related algorithms.\nRcppNumerical: Rcpp integration for numerical computing libraries, including e.g., numerical integration, optimization.\n\nAnd a bunch more…",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integration of C++ Code with Rcpp</span>"
    ]
  },
  {
    "objectID": "cpp.html#a-simple-model-for-daily-financial-returns",
    "href": "cpp.html#a-simple-model-for-daily-financial-returns",
    "title": "\n11  Integration of C++ Code with Rcpp\n",
    "section": "\n11.14 A simple model for daily financial returns",
    "text": "11.14 A simple model for daily financial returns\nConsider the following conditional model for financial log-returns (GARCH(1,1)) \\[\n\\begin{aligned}\nr_t &= \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_t^2) \\\\\n\\sigma_t^2 &= \\omega + \\alpha \\varepsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2\n\\end{aligned}\n\\] The conditional variance is \\(\\sigma_t^2\\) and it can be shown that the unconditional variance, given stationarity, is given by \\[\n\\text{Var}(r_t) = \\frac{\\omega}{1-\\alpha-\\beta}\n\\] Condition for stationarity is \\(\\alpha + \\beta &lt; 1\\). Conditions for positivity are \\(\\omega &gt; 0\\), \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\).",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integration of C++ Code with Rcpp</span>"
    ]
  },
  {
    "objectID": "cpp.html#estimation-of-the-garch11-model",
    "href": "cpp.html#estimation-of-the-garch11-model",
    "title": "\n11  Integration of C++ Code with Rcpp\n",
    "section": "\n11.15 Estimation of the GARCH(1,1) model",
    "text": "11.15 Estimation of the GARCH(1,1) model\nGiven the stationarity and positivity conditions on the process \\(\\sigma_t^2\\), the average Gaussian log-likelihood function is defined as \\[\n\\ell(\\theta, r_t) = \\frac{1}{T} \\sum_{t=1}^{T} \\log(f(r_t, \\theta))\n\\] where \\[\n\\begin{aligned}\n\\log(f(r_t, \\theta)) &= \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma_t^2}} \\exp \\left( -\\frac{r_t^2}{2\\sigma_t^2} \\right) \\right) \\\\\n&= -\\frac{1}{2} \\log(2\\pi\\sigma_t^2) - \\frac{r_t^2}{2\\sigma_t^2} \\\\\n&\\propto -\\log(2\\pi) - \\log(\\sigma_t^2) - \\frac{\\varepsilon_t^2}{\\sigma_t^2}\n\\end{aligned}\n\\] so that \\[\n\\hat{\\theta} = \\underset{\\theta}{\\text{arg max}} \\, \\ell(\\theta, r_t)\n\\]\n\n11.15.1 GARCH practical issues\nAs evident from the likelihood formulation, the evaluation of the sequence of volatilities \\(\\sigma_1,...,\\sigma_T\\) is required for model estimation. Unfortunately, a for loop is required for this task. Also fors imulation from the GARCH(1,1) model, the computation of the aforementioned loop is required.\nSo let’s do it in C++.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integration of C++ Code with Rcpp</span>"
    ]
  },
  {
    "objectID": "cpp.html#simulation-of-garch11-in-c",
    "href": "cpp.html#simulation-of-garch11-in-c",
    "title": "\n11  Integration of C++ Code with Rcpp\n",
    "section": "\n11.16 Simulation of GARCH(1,1) in C++",
    "text": "11.16 Simulation of GARCH(1,1) in C++\n\n// [[Rcpp::depends(RcppArmadillo)]]\n#include &lt;RcppArmadillo.h&gt;\n\nusing namespace arma;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nRcpp::List GarchSim(int iT, double dOmega, double dAlpha, double dBeta) {\n  \n  vec vY(iT); //observations\n  vec vSigma2(iT); // conditional variances\n  \n  // initialize at the unconditional value\n  vSigma2(0) = dOmega/(1.0 - dAlpha - dBeta);\n\n  // sample the first observation\n  vY(0) = pow(vSigma2(0), 0.5) * Rf_rnorm(0.0, 1.0);\n\n  for (int t = 1; t &lt; iT; t++) {\n    vSigma2(t) = dOmega + dAlpha * pow(vY(t - 1), 2.0) + dBeta * vSigma2(t - 1);\n    vY(t) = pow(vSigma2(t), 0.5) * Rf_rnorm(0.0, 1.0);\n  }\n\n  List lOut;\n  lOut[\"vSigma2\"] = vSigma2;\n  lOut[\"vY\"] = vY;\n\n  return lOut;\n}\n\nNote:\n\nVector declaration with vec.\nThe use of pow(double, double) for powers.\nThe function Rf_rnorm(double, double) for simulation from a Gaussian distribution.\nThe list declaration.\nThe list elements assignment.\n\nAssuming the code is saved in the file SimGarch.cpp inside the working directory, a GARCH(1,1) process can be simulated by running:\n\nsourceCpp(\"SimGarch.cpp\")\n\niT &lt;- 1000\ndOmega &lt;- 0.1\ndAlpha &lt;- 0.05\ndBeta &lt;- 0.94\n\nlSim &lt;- GarchSim(iT, dOmega, dAlpha, dBeta)\n\nplot(1:1000, lSim[[\"vSigma2\"]], type = \"l\", lty = 1, xlab = \"Time\", ylab = \"Conditional variance\")\n\n\n\n\n\n\n\nplot(1:1000, lSim[[\"vY\"]], type = \"l\", lty = 1, xlab = \"Time\", ylab = \"Log Returns\")",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Integration of C++ Code with Rcpp</span>"
    ]
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "\n12  Creating R packages with RStudio\n",
    "section": "",
    "text": "12.1 Introduction\nBook reference: https://r-pkgs.had.co.nz\nIn R, the fundamental unit of shareable code is the package. A package bundles together code, data, documentation, and tests, and is easy to share with others.\nAs of now, there are over 2200 packages available on the Comprehensive R Archive Network, or CRAN, the public clearing house for R packages.\nThis huge variety of packages is one of the reasons that R is so successful: the chances are that someone has already solved a problem that you’re working on, and you can benefit form their work by downloading their package.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Creating R packages with RStudio</span>"
    ]
  },
  {
    "objectID": "packages.html#why-a-package",
    "href": "packages.html#why-a-package",
    "title": "\n12  Creating R packages with RStudio\n",
    "section": "\n12.2 Why a package?",
    "text": "12.2 Why a package?\nWhy write a package? One compelling reason is if you have code that you want to share with others. Bundling your code into a package makes it easy for other people to use it, because like you, they already know how to use packages. If your codei s in a package, any R user can easily download it, install it, and learn how to use it.\nFurthermore, organizing code in a package makes your life easier because packages come with conventions. These conventions are helpful because:\n\nThey save you time - you don’t need to think about the best way to organise ap roject, you can just follow a template.\nStandardised conventions lead to standardised tools - if you buy into R’s package conventions, you get many tools for free.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Creating R packages with RStudio</span>"
    ]
  },
  {
    "objectID": "packages.html#setting-up",
    "href": "packages.html#setting-up",
    "title": "\n12  Creating R packages with RStudio\n",
    "section": "\n12.3 Setting up",
    "text": "12.3 Setting up\nInstall the folowing packages:\n\ninstall.packages(c(\"Rcpp\", \"RcppArmadillO\", \"devtools\"))\n\nBefore we start, we need to install Rtools.exe from https://cran.r-project.org/bin/windows/Rtools/index.html.\nRtools includes all the programs we require in order to compile our code.\nNow, in RStudio, navigate to “File” -&gt; “New Project…” -&gt; “New Directory” -&gt; “R Package using RcppArmadillo”.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Creating R packages with RStudio</span>"
    ]
  },
  {
    "objectID": "packages.html#code-in-packages",
    "href": "packages.html#code-in-packages",
    "title": "\n12  Creating R packages with RStudio\n",
    "section": "\n12.4 Code in Packages",
    "text": "12.4 Code in Packages\nThe first principle of using a package is that all R code goes in the R folder of the source package on your computer. Here is where we put all the .R files containing our functions organized in a sensible way. Assume to have the following function:\n\nMySumFunction &lt;- function(a, b) {\n  c &lt;- a + b\n  return(c)\n}\n\nIn the file Utilities.R inside the R folder of your package. To load you package with your function you can press Ctrl + Shift + L.\n\nMySumFunction(5, 7)\n#&gt; [1] 12\n\nC++ code has to placed inside the src folder of your package.\nA new C++ file contains this by default:\n\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n\n// [[Rcpp::export]]\nNumericVector timesTwo(NumericVector x) {\n  return x * 2;\n}\n\n\n// You can include R code blocks in C++ files processed with sourceCpp\n// (useful for testing and development). The R code will be automatically\n// run after the compilation.\n//\n\n/*** R\ntimesTwo(42)\n*/\n\nNote that it includes Rcpp and uses the Rcpp namespace. IF we want ot use armadillo and all its functionalities we need to modify as follows.\n\n#include &lt;RcppArmadillo.h&gt;\n\nusing namespace arma;\nusing namespace Rcpp;\n\nNote that we have only included RcppArmadillo.h, this is because Rcpp.h is automatically loaded by #include &lt;RcppArmadillo.h&gt;. Indeed, we are still able to use the Rcpp namespace.\nLet’s define the following function after the namespace definition:\n\n#include &lt;RcppArmadillo.h&gt;\n\nusing namespace arma;\nusing namespace Rcpp;\n\n//[[Rcpp::export]]\narma::vec SumOfTwoVec_C(arma::vec vX, arma::vec vY) {\n  arma::vec vZ = vX + vY;\n  return vZ;\n}\n\nNow you can save the script as, say, utilities_c.cpp in the src folder of your package.\nRunning Ctrl+Shift+L, RStudio automatically compiles the C++ functions and export to R those that are signed with the //[[Rcpp::export]] flag.\nNow the package is loaded and its functions can be used.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Creating R packages with RStudio</span>"
    ]
  },
  {
    "objectID": "packages.html#object-documentation",
    "href": "packages.html#object-documentation",
    "title": "\n12  Creating R packages with RStudio\n",
    "section": "\n12.5 Object documentation",
    "text": "12.5 Object documentation\nDocumentation is one of the most important aspects of a good package. Without it, users will not know how to use your package.\n\nLearn about object documentation, as accessed by ? or help().\nWrite .Rd files in the man/ directory. These files use a custom syntax, loosely based on LaTeX, and are rendered to HTML, plain text and pdf for viewing.\nThe .Rd files can be used for documentation of each function and the package itself.\n\n\n12.5.1 Documentation for -package\nThe main MyName-package.Rd contains this code by default:\n\n\\name{packageexample-package}\n\\alias{packageexample-package}\n\\alias{packageexample}\n\\docType{package}\n\\title{\n\\packageTitle{packageexample}\n}\n\\description{\n\\packageDescription{packageexample}\n}\n\\details{\n\nThe DESCRIPTION file:\n\\packageDESCRIPTION{packageexample}\n\\packageIndices{packageexample}\n~~ An overview of how to use the package, including the most important functions ~~\n}\n\\author{\n\\packageAuthor{packageexample}\n\nMaintainer: \\packageMaintainer{packageexample}\n}\n\\references{\n~~ Literature or other references for background information ~~\n}\n~~ Optionally other standard keywords, one per line, from file KEYWORDS in the R documentation directory ~~\n\\keyword{ package }\n\\seealso{\n~~ Optional links to other man pages, e.g. ~~\n~~ \\code{\\link[&lt;pkg&gt;:&lt;pkg&gt;-package]{&lt;pkg&gt;}} ~~\n}\n\\examples{\n~~ simple examples of the most important functions ~~\n}\n\nThe function documentation contains this code by default:\n\n\\name{RcppArmadillo-Functions}\n\\alias{rcpparma_hello_world}\n\\alias{rcpparma_innerproduct}\n\\alias{rcpparma_outerproduct}\n\\alias{rcpparma_bothproducts}\n\\title{Set of functions in example RcppArmadillo package}\n\\description{\n  These four functions are created when\n  \\code{RcppArmadillo.package.skeleton()} is invoked to create a\n  skeleton packages.\n}\n\\usage{\nrcpparma_hello_world()\nrcpparma_outerproduct(x)\nrcpparma_innerproduct(x)\nrcpparma_bothproducts(x)\n}\n\\arguments{\n  \\item{x}{a numeric vector}\n}\n\\value{\n  \\code{rcpparma_hello_world()} does not return a value, but displays a\n  message to the console.\n\n  \\code{rcpparma_outerproduct()} returns a numeric matrix computed as the\n  outer (vector) product of \\code{x}.\n\n  \\code{rcpparma_innerproduct()} returns a double computer as the inner\n  (vector) product of \\code{x}.\n\n  \\code{rcpparma_bothproducts()} returns a list with both the outer and\n  inner products.\n  \n}\n\\details{\n  These are example functions which should be largely\n  self-explanatory. Their main benefit is to demonstrate how to write a\n  function using the Armadillo C++ classes, and to have to such a\n  function accessible from R.\n}\n\\references{\n  See the documentation for Armadillo, and RcppArmadillo, for more details.\n}\n\\examples{\n  x &lt;- sqrt(1:4)\n  rcpparma_innerproduct(x)\n  rcpparma_outerproduct(x)\n}\n\\author{Dirk Eddelbuettel}",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Creating R packages with RStudio</span>"
    ]
  },
  {
    "objectID": "packages.html#description",
    "href": "packages.html#description",
    "title": "\n12  Creating R packages with RStudio\n",
    "section": "\n12.6 Description",
    "text": "12.6 Description\nThe job of the description file is to store important metadata about your package. When you first start writing packages, you’ll mostly use these metadata to record what packages are needed to run your package.\nHowever, as time goes by and you start sharing your package with others, the metadata file becomes increasingly important because it specifies who can use it (the license) and whom to contact (you!) if there are any problems.\nDependencies: What does your package need? Packages that must be loaded along with your package need to be listed in the Imports section of DESCRIPTION.\n\nPackage: packageexample\nType: Package\nTitle: What the package does (short line)\nVersion: 1.0\nDate: 2025-05-16\nAuthor: Who wrote it\nMaintainer: Who to complain to &lt;yourfault@somewhere.net&gt;\nDescription: More about what it does (maybe more than one line)\nLicense: What license is it under?\nImports: Rcpp (&gt;= 1.0.13-1)\nLinkingTo: Rcpp, RcppArmadillo",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Creating R packages with RStudio</span>"
    ]
  },
  {
    "objectID": "packages.html#different-types-of-r-packages",
    "href": "packages.html#different-types-of-r-packages",
    "title": "\n12  Creating R packages with RStudio\n",
    "section": "\n12.7 Different types of R packages",
    "text": "12.7 Different types of R packages\nSource packages: This is the development version of a package that lives on your computer. A source package is just a directory with components subfolders like R, and files like DESCRIPTION, and so on.\nBundled packages: This is a package that’s been compressed into a single file. By convention (from Linux), package bundles in R use the extension .tar.gz. This means that multiple files have been reduced to a single file (.tar) and the ncompressed using (.gz). RStudio creates these files with Build -&gt; More -&gt; Build Source Package. (Yes, this is inconsistent with the previous point).\nBinary packages: This is the type of package you want to distribute to users who don’t have package development tools. Binary packages are platform specific: you can’t install a Windows binary package on a Mac or vice versa. Also, while Mac binary packages end in .tgz, Windows binary packages end in .zip.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Creating R packages with RStudio</span>"
    ]
  },
  {
    "objectID": "packages.html#beyond-the-basics",
    "href": "packages.html#beyond-the-basics",
    "title": "\n12  Creating R packages with RStudio\n",
    "section": "\n12.8 Beyond the basics",
    "text": "12.8 Beyond the basics\nGitHub: https://github.com/\n\nGreat place to share your packages (or other code) with others online.\nMultiple people working on a project.\nVersion control.\nDirectly connect RStudio to GitHub.\nPush/Pull versions between GitHub and your computer.\nhttps://happygitwithr.com/\n\nCRAN: https://cran.r-project.org/\n\nAs close as it gets to making an “official” R package.\nTeam of volunteers in Vienna do quality control.\nMany checks you should do before submitting a package.\nCorrect formatting, documentation, checking on many different OS’s.\n\ndevtools package helps a lot with this.\nSection 23 of Wickham’s book: https://r-pkgs.org/release.html",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Creating R packages with RStudio</span>"
    ]
  },
  {
    "objectID": "packages.html#coding-style-and-swirl",
    "href": "packages.html#coding-style-and-swirl",
    "title": "\n12  Creating R packages with RStudio\n",
    "section": "\n12.9 Coding Style and Swirl",
    "text": "12.9 Coding Style and Swirl\nStyle: You should get in the habit of writing R code with good and consistent style. Good style is important because while your code only has one author, it’ll usually have multiple readers. Therefore, use:\n\nWickham (https://adv-r.had.co.nz/Style.html) for a style guide.\nHungarian notation.\n\nSwirl: The swirl package is good for learning R. It asks you questions and gives you a prompt where you can type your answer.\n\ndevtools::load_all(\".\")\ninstall.packages(\"swirl\")\nlibrary(swirl)\nswirl()",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Creating R packages with RStudio</span>"
    ]
  },
  {
    "objectID": "problemset1.html",
    "href": "problemset1.html",
    "title": "\n13  Exercise set 1\n",
    "section": "",
    "text": "13.1 (1)\nDo some simple computations of your choice. This could be for example defining two vectors and then performing addition, subtraction, multiplication of their elements. At each computation use help to find out which function you must use.\nSolution:\nvX &lt;- 1:5 # Numbers 1, 2, 3, 4, 5\nvY &lt;- seq(2, 10, 2) # Numbers 2, 4, 6, 8, 10\n\n# Addition\nvX + 10\n#&gt; [1] 11 12 13 14 15\nvY + 1\n#&gt; [1]  3  5  7  9 11\n\n# Subtraction\nvX - 10\n#&gt; [1] -9 -8 -7 -6 -5\nvY - 1\n#&gt; [1] 1 3 5 7 9\n\n# Multiplication\nvX * 10\n#&gt; [1] 10 20 30 40 50\nvY * -1\n#&gt; [1]  -2  -4  -6  -8 -10\n\n# Powers\nvX ^ 2\n#&gt; [1]  1  4  9 16 25\nvY ^ -1\n#&gt; [1] 0.5000000 0.2500000 0.1666667 0.1250000 0.1000000\n\n# Other functions: division (/), exponentiation (^), square root (sqrt())...",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exercise set 1</span>"
    ]
  },
  {
    "objectID": "problemset1.html#section-1",
    "href": "problemset1.html#section-1",
    "title": "\n13  Exercise set 1\n",
    "section": "\n13.2 (2)\n",
    "text": "13.2 (2)\n\nDefine v1 = c(1, 2, 2, 1) and v2 = c(2, 3, 3, 2). Do element-wise addition, subtraction, multiplication etc. Perform concatenation between v1 and v2 into a new vector v3.\nSolution:\n\nv1 &lt;- c(1, 2, 2, 1)\nv2 &lt;- c(2, 3, 3, 2)\n\n# Element-wise addition\nv1 + v2\n#&gt; [1] 3 5 5 3\n\n# Element-wise subtraction\nv1 - v2\n#&gt; [1] -1 -1 -1 -1\n\n# Element-wise multiplication\nv1 * v2\n#&gt; [1] 2 6 6 2\n\n# Concatenation into new vector\nv3 &lt;- c(v1, v2)\nv3\n#&gt; [1] 1 2 2 1 2 3 3 2",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exercise set 1</span>"
    ]
  },
  {
    "objectID": "problemset1.html#section-2",
    "href": "problemset1.html#section-2",
    "title": "\n13  Exercise set 1\n",
    "section": "\n13.3 (3)\n",
    "text": "13.3 (3)\n\nDefine matrices of your choice. Use R and determine the rows or columns of each of these variables.\nSolution:\n\nmA &lt;- matrix(1:6, nrow = 2, ncol = 3, byrow = TRUE)\nmA\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    1    2    3\n#&gt; [2,]    4    5    6\n\n# Determining the dimensions (number of rows and columns)\ndim(mA)\n#&gt; [1] 2 3\n\n# Or retrieving either the number of rows or number of columns\nnrow(mA) # Rows\n#&gt; [1] 2\n\nncol(mA) # Columns\n#&gt; [1] 3",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exercise set 1</span>"
    ]
  },
  {
    "objectID": "problemset1.html#section-3",
    "href": "problemset1.html#section-3",
    "title": "\n13  Exercise set 1\n",
    "section": "\n13.4 (4)\n",
    "text": "13.4 (4)\n\nDefine a \\(3 \\times 4\\) matrix, mA, of your choice\n\nmA &lt;- matrix(seq(12, 1, -1), 3, 4, byrow = TRUE)\nmA\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]   12   11   10    9\n#&gt; [2,]    8    7    6    5\n#&gt; [3,]    4    3    2    1\n\n1. Print the minimum and the maximum of each row;\n\n# Minimum and maximum of each row\napply(mA, 1, min) # `apply` applies the function `min` to each row (indicated by 1)\n#&gt; [1] 9 5 1\n\napply(mA, 1, max)\n#&gt; [1] 12  8  4\n\n2. Compute the sum of each column and print it as a vector.\n\ncolSums(mA)\n#&gt; [1] 24 21 18 15\n\n3. Generate a matrix mB with columns equal to the cumulative sum of the rows in mA;\n\nmB &lt;- apply(mA, 1, cumsum) # See notes for `apply` above. `cumsum` returns the cumulative sum.\nmB # Column 1 in mB is the cumulative sum of row 1 in mA, and so on....\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]   12    8    4\n#&gt; [2,]   23   15    7\n#&gt; [3,]   33   21    9\n#&gt; [4,]   42   26   10\n\n4. Generate a matrix mC with columns equal to the cumulative product of the rows in mA.\n\nmC &lt;- apply(mA, 1, cumprod)\nmC # Column 1 in mC is the cumulative product of row 1 in mA, and so on....\n#&gt;       [,1] [,2] [,3]\n#&gt; [1,]    12    8    4\n#&gt; [2,]   132   56   12\n#&gt; [3,]  1320  336   24\n#&gt; [4,] 11880 1680   24\n\n5. Sort in ascending order the elements in the first column of mA.\n\nmA # Unsorted\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]   12   11   10    9\n#&gt; [2,]    8    7    6    5\n#&gt; [3,]    4    3    2    1\n\nsort(mA[, 1]) # Sort and return only the first column\n#&gt; [1]  4  8 12\n\nmA[order(mA[, 1]), ] # All rows sorted by the first column. `order` returns a permutation that rearranges its first argument into the correct order. Ascending order is the default.\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    4    3    2    1\n#&gt; [2,]    8    7    6    5\n#&gt; [3,]   12   11   10    9",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exercise set 1</span>"
    ]
  },
  {
    "objectID": "problemset1.html#section-4",
    "href": "problemset1.html#section-4",
    "title": "\n13  Exercise set 1\n",
    "section": "\n13.5 (5)\n",
    "text": "13.5 (5)\n\nGenerate a vector of random numbers extracted from a Uniform distribution (use runif ). The number samples is 250. Compute mean, variance, kurtosis and skewness parameters of the generated vector.\nSolution:\n\nset.seed(1) # Make the results reproducible by setting a seed\nvU &lt;- runif(n = 250, min = 0, max = 1) # The final two arguments are not necessary as they are the default values\n\nInstall the library for simple summary statistics (if needed):\n\nif (!require(\"e1071\")) install.packages(\"e1071\")\nsuppressMessages(library(e1071))\n\nCalculate the summary statistics:\n\ncat(\"The mean is: \", mean(vU), \"\\n\")\n#&gt; The mean is:  0.5098254\n\ncat(\"The variance is: \", var(vU), \"\\n\")\n#&gt; The variance is:  0.07249006\n\ncat(\"The skewness is: \", skewness(vU), \"\\n\")\n#&gt; The skewness is:  0.04188663\n\ncat(\"The kurtosis is: \", kurtosis(vU), \"\\n\")\n#&gt; The kurtosis is:  -1.121881\n\nWe could also have computed the kurtosis and skewness manually:\n\niN &lt;- length(vU) # Number of observations\ndStdU &lt;- sqrt(var(vU)) # Standard deviation of U\n\ndMoment3 &lt;- sum((vU - mean(vU)) ^ 3) / iN # 3rd central moment\ndSkewnews &lt;- dMoment3 / dStdU ^ 3\ndSkewnews\n#&gt; [1] 0.04188663\n\ndMoment4 &lt;- sum((vU - mean(vU)) ^ 4) / iN # 4th central moment\ndKurtosis &lt;- dMoment4 / dStdU ^ 4 - 3 # Excess kurtosis\ndKurtosis\n#&gt; [1] -1.121881",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exercise set 1</span>"
    ]
  },
  {
    "objectID": "problemset1.html#section-5",
    "href": "problemset1.html#section-5",
    "title": "\n13  Exercise set 1\n",
    "section": "\n13.6 (6)\n",
    "text": "13.6 (6)\n\nCompute the following:\n\n\nDefine the string \\(\\text{sR} = \\text{\"RisGreat\"}\\); Suppose that you want to replace the first \\(\\text{\"R\"}\\) with an \\(\\text{\"S\"}\\).\n\n\nSolution:\n\nsR &lt;- \"RisGreat\"\nsub(\"R\", \"S\", sR) # Note, `sub` only replaces the first occurence. `gsub` replaces all.\n#&gt; [1] \"SisGreat\"",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exercise set 1</span>"
    ]
  },
  {
    "objectID": "problemset2.html",
    "href": "problemset2.html",
    "title": "\n14  Exercise set 2\n",
    "section": "",
    "text": "14.1 (1)\nGive R assignment statements that set the variable \\(z\\) to.\n(a). \\(x^{a^b}\\)\nSolution:\nx &lt;- 2\na &lt;- 3\nb &lt;- 2\nz &lt;- x ^ (a ^ b)\nz\n#&gt; [1] 512\n(b). \\((x^a)^b\\)\nSolution:\nz &lt;- (x ^ a) ^ b\nz\n#&gt; [1] 64\n(c). \\(3x^3 + 2x^2 + 6x + 1\\)\nSolution:\nz &lt;- 3 * x ^ 3 + 2 * x ^ 2 + 6 * x + 1\nz\n#&gt; [1] 45\n(d). \\(z+1\\)\nSolution:\nz &lt;- z + 1\nz\n#&gt; [1] 46",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Exercise set 2</span>"
    ]
  },
  {
    "objectID": "problemset2.html#section-1",
    "href": "problemset2.html#section-1",
    "title": "\n14  Exercise set 2\n",
    "section": "\n14.2 (2)\n",
    "text": "14.2 (2)\n\nGive R expressions that return the following matrices and vectors:\n(a). \\((1,2,3,4,5,6,7,8,7,6,5,4,3,2,1)\\)\nSolution:\n\nc(1:8, 7:1) # Simple concatenation, but notice how 7:1 returns a decreasing vector of integers 1 to 7.\n#&gt;  [1] 1 2 3 4 5 6 7 8 7 6 5 4 3 2 1\n\n(b). \\((1,2,2,3,3,3,4,4,4,4,5,5,5,5,5)\\)\nSolution:\n\nrep.int(x = 1:5, times = 1:5) # `rep.int` is faster than `rep` and only requires two arguments. We're essentially pairing the n'th input in the vector given in the x argument to the n'th input in the vector given in the times argument.\n#&gt;  [1] 1 2 2 3 3 3 4 4 4 4 5 5 5 5 5\n\n(c).\n\\[\n  \\left(\n  \\begin{matrix}\n  0 & 1 & 1 \\\\\n  1 & 0 & 1 \\\\\n  1 & 1 & 0\n  \\end{matrix}\n  \\right)\n  \\]\nSolution:\n\n1 - diag(3) # We create a matrix of ones and subtract an identity matrix of size 3\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    0    1    1\n#&gt; [2,]    1    0    1\n#&gt; [3,]    1    1    0\n\n(d).\n\\[\n  \\left(\n  \\begin{matrix}\n  0 & 2 & 3 \\\\\n  0 & 4 & 0 \\\\\n  7 & 0 & 0\n  \\end{matrix}\n  \\right)\n  \\]\nSolution:\n\nmatrix(c(0, 0, 7, 2, 4, 0, 3, 0, 0), 3, 3) # Manually creating the matrix seems to be the easiest solution :-)\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    0    2    3\n#&gt; [2,]    0    4    0\n#&gt; [3,]    7    0    0",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Exercise set 2</span>"
    ]
  },
  {
    "objectID": "problemset2.html#section-2",
    "href": "problemset2.html#section-2",
    "title": "\n14  Exercise set 2\n",
    "section": "\n14.3 (3)\n",
    "text": "14.3 (3)\n\nUse R to produce vector containing all integers from 1 to 100 that are not divisible by 2, 3, or 7.\nSolution:\n\nvX &lt;- 1:100\nvX[(vX %% 2 != 0) & (vX %% 3 != 0) & (vX %% 7 != 0)] # We use the modulo operator for indexing since it will be equal to 0 when a value in the vector is divisible by one of the three values.\n#&gt;  [1]  1  5 11 13 17 19 23 25 29 31 37 41 43 47 53 55 59 61 65 67 71 73 79 83 85\n#&gt; [26] 89 95 97",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Exercise set 2</span>"
    ]
  },
  {
    "objectID": "problemset2.html#section-3",
    "href": "problemset2.html#section-3",
    "title": "\n14  Exercise set 2\n",
    "section": "\n14.4 (4)\n",
    "text": "14.4 (4)\n\nBuild a 10 x 10 identity matrix. Then make all the non-zero elements 5. Do this latter step in at least two different ways.\nSolution:\n\nmI &lt;- diag(10) # 10 x 10 identity matrix\nmI[mI != 0] &lt;- 5 # Making the non-zero elements 5 by using indexing\nmI\n#&gt;       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n#&gt;  [1,]    5    0    0    0    0    0    0    0    0     0\n#&gt;  [2,]    0    5    0    0    0    0    0    0    0     0\n#&gt;  [3,]    0    0    5    0    0    0    0    0    0     0\n#&gt;  [4,]    0    0    0    5    0    0    0    0    0     0\n#&gt;  [5,]    0    0    0    0    5    0    0    0    0     0\n#&gt;  [6,]    0    0    0    0    0    5    0    0    0     0\n#&gt;  [7,]    0    0    0    0    0    0    5    0    0     0\n#&gt;  [8,]    0    0    0    0    0    0    0    5    0     0\n#&gt;  [9,]    0    0    0    0    0    0    0    0    5     0\n#&gt; [10,]    0    0    0    0    0    0    0    0    0     5\n\n# Other solutions\nmA &lt;- diag(10)\nmA &lt;- ifelse(mA != 0, 5, mA) # Using if-else\nall.equal(mI, mA)\n#&gt; [1] TRUE\n\nmB &lt;- diag(10)\ndiag(mB) &lt;- 5 # Modify the diagonal directly\nall.equal(mI, mB)\n#&gt; [1] TRUE",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Exercise set 2</span>"
    ]
  },
  {
    "objectID": "problemset2.html#section-4",
    "href": "problemset2.html#section-4",
    "title": "\n14  Exercise set 2\n",
    "section": "\n14.5 (5)\n",
    "text": "14.5 (5)\n\nConsider the function \\(y=f(x)\\) defined by:\n\\[\n  f(x)=\n  \\begin{cases}\n  -x^3, \\text{ if } x \\le 0 \\\\\n  x^2, \\text{ if } x \\in (0,1] \\\\\n  \\sqrt{x}, \\text{ if } x &gt; 1\n  \\end{cases}\n  \\] Supposing that you are given \\(x\\), write an R expression for \\(y\\) using if statements.\nSolution:\n\nf &lt;- function(iX) {\n  if (iX &lt;= 0) {\n    return(- iX ^ 3)\n  } else if (iX &lt;= 1) {\n    return(iX ^ 2)\n  } else {\n    return(sqrt(iX))\n  }\n}\n\nset.seed(1)\nx &lt;- runif(n = 1, min = -1, max = 2) # Random variable with equal probability of being in the three categories.\n\ncat(\"x is: \", x, \" and y is: \", f(x))\n#&gt; x is:  -0.203474  and y is:  0.008424164\n\n# Bonus: the function in one-line: f &lt;- function(iX) ifelse(iX &lt;= 0, -iX ^ 3, ifelse(iX &lt;= 1, iX ^ 2, sqrt(iX)))",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Exercise set 2</span>"
    ]
  },
  {
    "objectID": "problemset2.html#section-5",
    "href": "problemset2.html#section-5",
    "title": "\n14  Exercise set 2\n",
    "section": "\n14.6 (6)\n",
    "text": "14.6 (6)\n\nLet \\(h(x,n)=1+x+x^2+x^3+\\cdots+x^n=\\sum_{i=0}^{n}x^i\\). Write an R program to calculate \\(h(x,n)\\) using a for loop.\nSolution:\n\n# Note, this is just a finite geometric series\niX &lt;- 5 # Example values\niN &lt;- 4\niGeoSum &lt;- 0\nfor (iIte in 0:iN) {\n  iGeoSum &lt;- iGeoSum + iX ^ iIte\n}\niGeoSum\n#&gt; [1] 781",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Exercise set 2</span>"
    ]
  },
  {
    "objectID": "problemset2.html#section-6",
    "href": "problemset2.html#section-6",
    "title": "\n14  Exercise set 2\n",
    "section": "\n14.7 (7)\n",
    "text": "14.7 (7)\n\nFirst, write a program that achieves the same result as in Exercise 6 but using a while loop.\nThen, write a program that does this using vector-operations (and no loops).\nSolution:\n\niX &lt;- 5 # Example values\niN &lt;- 4\niGeoSum &lt;- 0\niIte &lt;- 0\nwhile (iIte &lt;= iN) {\n  iGeoSum &lt;- iGeoSum + iX ^ iIte\n  iIte &lt;- iIte + 1\n}\niGeoSum\n#&gt; [1] 781\n\nsum(iX ^ (0:iN)) # Vector-operations. The one-liner uses element-wise exponentiation against a vector of increasing integers. This creates a vector of the individual x^i's that we then sum.\n#&gt; [1] 781",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Exercise set 2</span>"
    ]
  },
  {
    "objectID": "problemset2.html#section-7",
    "href": "problemset2.html#section-7",
    "title": "\n14  Exercise set 2\n",
    "section": "\n14.8 (8)\n",
    "text": "14.8 (8)\n\nHow would you find the sum of every third element of a vector \\(x\\)?\nSolution:\n\nvX &lt;- 1:100\nsum(vX[seq.int(3, length(vX), 3)]) # We use some smart indexing of our vector x. `seq.int` is faster than `seq` and creates the vector \"3, 6, 9, ...\" up until the final value in x. This vector can then be used for indexing x. Finally, the indexed values are summed.\n#&gt; [1] 1683",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Exercise set 2</span>"
    ]
  },
  {
    "objectID": "problemset2.html#section-8",
    "href": "problemset2.html#section-8",
    "title": "\n14  Exercise set 2\n",
    "section": "\n14.9 (9)\n",
    "text": "14.9 (9)\n\nWrite a program that uses a loop to find the minimum of a vector \\(x\\), without using any predefined functions like min or sort.\nSolution:\n\nset.seed(1)\nvX &lt;- runif(n = 250) # 250 random values\ndMin &lt;- vX[1]\nfor (iN in 1:length(vX)) {\n  if (vX[iN] &lt; dMin) dMin &lt;- vX[iN] # We loop over the vector vX and assign the value at the index iN if it is lower than our current lowest value\n}\ndMin # Print the minimum value\n#&gt; [1] 0.01307758\nmin(vX) # Verifying the result\n#&gt; [1] 0.01307758",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Exercise set 2</span>"
    ]
  },
  {
    "objectID": "problemset2.html#section-9",
    "href": "problemset2.html#section-9",
    "title": "\n14  Exercise set 2\n",
    "section": "\n14.10 (10)\n",
    "text": "14.10 (10)\n\nA room contains 100 toggle switches, originally all turned off. 100 people enter the room in turn. The first toggles every switch, the second toggles every second switch, the third every third switch, and so on, to the last person who toggles the last switch only. At the end of this process, which switches are turned on?\nSolution:\n\nvX &lt;- rep(0L, 100)\n\nfor (i in 1:100) {\n  \n  vX[seq_along(vX) %% i == 0] &lt;- bitwXor(vX[seq_along(vX) %% i == 0], 1) # Using bitwise logical operations is slightly overkill\n  \n  # This also works\n  # vX[seq_along(vX) %% i == 0] &lt;- !vX[seq_along(vX) %% i == 0]\n  \n}\n\nvX\n#&gt;   [1] 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n#&gt;  [38] 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n#&gt;  [75] 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n\nwhich(vX == 1) # Perfect squares! :-)\n#&gt;  [1]   1   4   9  16  25  36  49  64  81 100",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Exercise set 2</span>"
    ]
  },
  {
    "objectID": "problemset2.html#section-10",
    "href": "problemset2.html#section-10",
    "title": "\n14  Exercise set 2\n",
    "section": "\n14.11 (11)\n",
    "text": "14.11 (11)\n\nLoad the historical prices of MAERSK from the file MAERSK-B.CO.csv available in blackboard. Compute the percentage log-returns and plot the associated time-series. Add a red coloured horizontal line to the plot indicating the average value. Add one standard deviation confidence bands to the average value as dashed blue lines.\nSolution:\n\nmMaerskData &lt;- read.csv(\"MAERSK-B.CO.csv\", header = TRUE, sep = \",\", na.strings = \"null\")\n\nvY &lt;- diff(log(na.omit(mMaerskData[, \"Adj.Close\"]))) # Log-returns\n\n# Line plot\nplot(1:length(vY), vY, type = \"l\", ylim = c(-0.2, 0.2), ylab = \"Log returns\", xlab = paste0(\"Day(s) since \", min(mMaerskData$Date))) \n\n# Mean\nlines(1:length(vY), rep(mean(vY), length(vY)), col = \"red\", lwd = \"1\")\n\n# Confidence bands\nlines(1:length(vY), rep((mean(vY) + sqrt(var(vY))), length(vY)), col = \"blue\", lwd = \"3\", lty = 6)\nlines(1:length(vY), rep((mean(vY) - sqrt(var(vY))), length(vY)), col = \"blue\", lwd = \"3\", lty = 6)",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Exercise set 2</span>"
    ]
  },
  {
    "objectID": "problemset3.html",
    "href": "problemset3.html",
    "title": "\n15  Exercise set 3\n",
    "section": "",
    "text": "15.1 (1)\nThe (Euclidean) length of a vector \\(v=(a_0,...,a_k)\\) is the square root of the sum of squares of its coordinates, that is \\(\\sqrt{a_0^2+ \\cdots +a_k^2}\\). Write a function that returns the Euclidean length of a vector.\nSolution:\neuclidean_length &lt;- function(vInput) {\n  return(sqrt(sum(vInput^2)))\n}\n\nvX &lt;- 1:10\neuclidean_length(vX)\n#&gt; [1] 19.62142",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Exercise set 3</span>"
    ]
  },
  {
    "objectID": "problemset3.html#section-1",
    "href": "problemset3.html#section-1",
    "title": "\n15  Exercise set 3\n",
    "section": "\n15.2 (2)\n",
    "text": "15.2 (2)\n\nThe Game of Life is a cellular automaton and was devised by the mathematician J.H. Conway in 1970. It is played on a grid of cells, each of which is either alive or dead. The grid of cells evolves in time and each cell interacts with its eight neighbours, which are the cells directly adjacent horizontally, vertically, and diagonally.\nAt each time step cells change as follows:\n\nA live cell with fewer than two neighbours dies of loneliness.\nA live cell with more than three neighbours dies of overcrowding.\nA live cell with two or three neighbours lives on the the next generation.\nA dead cell with exactly three neighbours comes to life\n\nThe initial pattern constitutes the first generation of the system. The second generation is created by applying the above rules simultaneously to every cell in the first generation: births and deaths all happen simultaneously. The rules continue to be applied repeatedly to create further generations.\nTheoretically, the Game of Life is played on an infinite grid, but in practice we use a finite grid arranged as a torus. That is, if you are in the left-most column of the grid then your left-hand neighbours are in the right-most column, and if you are in the top row, then your neighbours above are in the bottom row.\nWrite a program that replicates The Game of Life for iT = 420 periods, then stop. Organize the cells in a \\(n \\times n\\) matrix A. Start by defining the initial population with A = matrix(round(runif(iN^2)), iN, iN), for iN = 10. At the beginning of the program, set the seed of the random number generator with set.seed(999) such that your results can be replicated.\nAfter running the algorithm, you should get the following population:\n\n#&gt;       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n#&gt;  [1,]    0    0    0    0    0    0    0    0    0     0\n#&gt;  [2,]    0    0    0    0    0    0    0    0    0     0\n#&gt;  [3,]    0    0    0    0    0    0    0    0    0     0\n#&gt;  [4,]    0    0    0    0    1    1    0    0    0     0\n#&gt;  [5,]    0    0    0    1    0    0    1    0    0     0\n#&gt;  [6,]    0    0    0    1    0    0    1    0    0     0\n#&gt;  [7,]    0    0    0    0    1    1    0    0    0     0\n#&gt;  [8,]    0    0    0    0    0    0    0    0    0     0\n#&gt;  [9,]    0    0    0    0    0    0    0    0    0     0\n#&gt; [10,]    0    0    0    0    0    0    0    0    0     0\n\nNotice that the distribution of the points is the same even if we let the program run for iT = 1620 iterations. Do you think we always observe this behaviour? Try to conjecture an answer.\nSolution(s):\n\nset.seed(999)\niT &lt;- 420\niN &lt;- 10\nA &lt;- matrix(round(runif(iN^2)), iN, iN)\n\n## First solution (using a triple loop)\ngame_of_life_1 &lt;- function(mInput, iT) {\n  iR &lt;- nrow(mInput)\n  iC &lt;- ncol(mInput)\n  mNew &lt;- matrix(0, iR, iC)\n  for (t in 1:iT) {\n    # Loop over rows and columns, i.e., each value in the matrix separately\n    for (i in 1:iR) {\n      for (j in 1:iC) {\n        # Use the modulus operator for handling the borders\n        # We need (i - 2) due to R indexing beginning at 1\n        iNeighbourCount &lt;- mInput[(i - 2) %% iR + 1, (j - 2) %% iC + 1] +\n                           mInput[(i - 2) %% iR + 1, (j - 1) %% iC + 1] +\n                           mInput[(i - 2) %% iR + 1, j %% iC + 1] +\n\n                           mInput[(i - 1) %% iR + 1, (j - 2) %% iC + 1] +\n                           mInput[(i - 1) %% iR + 1, j %% iC + 1] +\n\n                           mInput[i %% iR + 1, (j - 2) %% iC + 1] +\n                           mInput[i %% iR + 1, (j - 1) %% iC + 1] +\n                           mInput[i %% iR + 1, j %% iC + 1]\n        \n        if (mInput[i, j] == 1) { # If alive\n          if (iNeighbourCount &lt; 2 || iNeighbourCount &gt; 3) {\n            mNew[i, j] &lt;- 0\n          } else {\n            mNew[i, j] &lt;- 1\n          }\n        } else { # If dead\n          if (iNeighbourCount == 3) {\n            mNew[i, j] &lt;- 1\n          } else {\n            mNew[i, j] &lt;- 0\n          }\n        }\n      }\n    }\n    mInput &lt;- mNew\n  }\n  return(mInput)\n}\n\nprint(game_of_life_1(A, 420))\n#&gt;       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n#&gt;  [1,]    0    0    0    0    0    0    0    0    0     0\n#&gt;  [2,]    0    0    0    0    0    0    0    0    0     0\n#&gt;  [3,]    0    0    0    0    0    0    0    0    0     0\n#&gt;  [4,]    0    0    0    0    1    1    0    0    0     0\n#&gt;  [5,]    0    0    0    1    0    0    1    0    0     0\n#&gt;  [6,]    0    0    0    1    0    0    1    0    0     0\n#&gt;  [7,]    0    0    0    0    1    1    0    0    0     0\n#&gt;  [8,]    0    0    0    0    0    0    0    0    0     0\n#&gt;  [9,]    0    0    0    0    0    0    0    0    0     0\n#&gt; [10,]    0    0    0    0    0    0    0    0    0     0\n\n\n## Second solution (much faster)\ngame_of_life_2 &lt;- function(mInput, iT) {\n  iR &lt;- nrow(mInput)\n  iC &lt;- ncol(mInput)\n  for (t in 1:iT) {\n    mPadded &lt;- matrix(0, iR + 2, iC + 2) # Empty matrix with a border\n    mPadded[2:(iR + 1), 2:(iC + 1)] &lt;- mInput # Middle part\n    \n    mPadded[1, 2:(iC + 1)] &lt;- mInput[iR, ] # Top row\n    mPadded[iR + 2, 2:(iC + 1)] &lt;- mInput[1, ] # Bottom row\n    mPadded[2:(iR + 1), 1] &lt;- mInput[, iC] # Left column\n    mPadded[2:(iR + 1), iC + 2] &lt;- mInput[, 1] # Right column\n    \n    mPadded[1, 1] &lt;- mInput[iR, iC] # Top left value\n    mPadded[1, iC + 2] &lt;- mInput[iR, 1] # Top right value\n    mPadded[iR + 2, 1] &lt;- mInput[1, iC] # Bottom left value\n    mPadded[iR + 2, iC + 2] &lt;- mInput[1, 1] # Bottom right value\n    \n    # Now, we can create a neighbour count by shifting the padded matrix and adding the 1's\n    mNeighbourCount &lt;- mPadded[1:iR, 1:iC] + \n                       mPadded[2:(iR + 1), 1:iC] + \n                       mPadded[3:(iR + 2), 1:iC] + \n                       mPadded[1:iR, 2:(iC + 1)] + \n                       mPadded[3:(iR + 2), 2:(iC + 1)] + \n                       mPadded[1:iR, 3:(iC + 2)] + \n                       mPadded[2:(iR + 1), 3:(iC + 2)] + \n                       mPadded[3:(iR + 2), 3:(iC + 2)]\n    \n    # And we can use the neighbour count matrix for indexing\n    mInput &lt;- (mInput & (mNeighbourCount == 2 | mNeighbourCount == 3)) | (!mInput & mNeighbourCount == 3)\n    \n  }\n  return(mInput)\n}\n\n# Use the plus-sign to return the matrix as integer booleans\nprint(+game_of_life_2(A, 420))\n#&gt;       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n#&gt;  [1,]    0    0    0    0    0    0    0    0    0     0\n#&gt;  [2,]    0    0    0    0    0    0    0    0    0     0\n#&gt;  [3,]    0    0    0    0    0    0    0    0    0     0\n#&gt;  [4,]    0    0    0    0    1    1    0    0    0     0\n#&gt;  [5,]    0    0    0    1    0    0    1    0    0     0\n#&gt;  [6,]    0    0    0    1    0    0    1    0    0     0\n#&gt;  [7,]    0    0    0    0    1    1    0    0    0     0\n#&gt;  [8,]    0    0    0    0    0    0    0    0    0     0\n#&gt;  [9,]    0    0    0    0    0    0    0    0    0     0\n#&gt; [10,]    0    0    0    0    0    0    0    0    0     0\n\n\n## A quick speed comparison of the two solutions shows a huge increase in performance\nsuppressMessages(library(microbenchmark))\n#&gt; Warning: pakke 'microbenchmark' blev bygget under R version 4.3.3\nmicrobenchmark(game_of_life_1(A, 420), +game_of_life_2(A, 420))\n#&gt; Unit: milliseconds\n#&gt;                     expr     min       lq      mean   median       uq      max\n#&gt;   game_of_life_1(A, 420) 94.4793 97.03300 99.163319 97.95755 99.09065 127.8503\n#&gt;  +game_of_life_2(A, 420)  6.6153  6.94085  7.986745  7.65885  8.64525  18.4305\n#&gt;  neval\n#&gt;    100\n#&gt;    100\n\nRegarding the second part of the question, there are certain patterns in Conway’s Game of Life that do not change between generations - implying that further iterations are pointless.\nIn our case, every “live” cell has two neighbours and thus survive. And dead cells either have 0, 2 or 4 neighbours, so they never come alive.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Exercise set 3</span>"
    ]
  },
  {
    "objectID": "problemset3.html#section-2",
    "href": "problemset3.html#section-2",
    "title": "\n15  Exercise set 3\n",
    "section": "\n15.3 (3)\n",
    "text": "15.3 (3)\n\nThe number of ways you can choose \\(r\\) things from a set of \\(n\\), ignoring the order in which they are chosen is:\n\\[\n\\binom{n}{r} = \\frac{n!}{r!(n-r)!}\n\\]\nLet \\(x\\) be the first element of the set of \\(n\\) things. We can partition the collection of possible size \\(r\\) subsets into those that contain \\(x\\) and those that don’t: there must be \\(\\binom{n-1}{r-1}\\) subsets of the first type and \\(\\binom{n-1}{r}\\) subsets of the second type. Thus:\n\\[\n\\binom{n}{r} = \\binom{n-1}{r-1} + \\binom{n-1}{r}\n\\] Using this and the fact that \\(\\binom{n}{n} = \\binom{n}{0} = 1\\), write a recursive function to calculate \\(\\binom{n}{r}\\).\nSolution:\n\nfbinom &lt;- function(n, r) {\n  if (n == r | r == 0) {\n    return(1)\n  } else {\n    return(fbinom(n - 1, r - 1) + fbinom(n - 1, r))\n  }\n}",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Exercise set 3</span>"
    ]
  },
  {
    "objectID": "problemset3.html#section-3",
    "href": "problemset3.html#section-3",
    "title": "\n15  Exercise set 3\n",
    "section": "\n15.4 (4)\n",
    "text": "15.4 (4)\n\nWhat will be the output of the following code? Try to answer this without typing it up.\n\nfb &lt;- function(n) {\n  if (n == 1 || n == 2) {\n    return(1)\n  } else {\n    return(fb(n - 1) + fb(n - 2))\n  }\n}\nfb(8)\n\nSolution:\nThe output will be the 8th number in the Fibonacci series (1, 1, 2, 3, 5, 8, 13, 21), i.e., 21.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Exercise set 3</span>"
    ]
  },
  {
    "objectID": "problemset3.html#section-4",
    "href": "problemset3.html#section-4",
    "title": "\n15  Exercise set 3\n",
    "section": "\n15.5 (5)\n",
    "text": "15.5 (5)\n\nHorner’s algorithm for evaluating the polynomial \\(p(x)=a_0+a_1x+a_2x^2+\\cdots + a_nx^n\\) consists of re-expressing it as:\n\\[\na_0+x(a_1+x(a_2+\\cdots+x(a_{n-1}+xa_n)...))\n\\]\nHow many operations are required to evaluate \\(p(x)\\) in each form?\nSolution:\nFor the normal expression, we need \\(n - 1\\) additions, \\(n-1\\) multiplications and \\(\\sum_{i=0}^{n-1}=\\frac{n^2-n}{2}\\) multiplications. In total, this is \\(2*(n-1)+\\frac{n^2}{2}-\\frac{n}{2}\\) elementary operations, which is approximately \\(O(n^2)\\).\nFor Horner’s algorithm: There are still \\(n-1\\) additive terms, but only \\(n-1\\) multiplications. Thus, there are \\(2*(n-1)\\) elementary operations, which is approximately \\(O(n)\\).",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Exercise set 3</span>"
    ]
  },
  {
    "objectID": "problemset3.html#section-5",
    "href": "problemset3.html#section-5",
    "title": "\n15  Exercise set 3\n",
    "section": "\n15.6 (6)\n",
    "text": "15.6 (6)\n\nCreate a 10 by 10 matrix m1 and let its elements be drawn from a Gaussian distribution with mean value 20 and variance 100 and then rounded. Using the apply function, return a new matrix whose entries are those of m1 modulo 3.\nSolution:\n\nm1 &lt;- matrix(round(rnorm(100, mean = 20, sd = 10)), 10, 10)\nm1\n#&gt;       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n#&gt;  [1,]   29   35   18    9   21   29   24   17   12    28\n#&gt;  [2,]   18   24   23   11   27   37    2   32   13    13\n#&gt;  [3,]   18   25    1   16   25   42    2   19   10    13\n#&gt;  [4,]   11   29    1   31    5   29   20   19   28    15\n#&gt;  [5,]   25   28   26   -2   28   10    0    8   33    32\n#&gt;  [6,]   24   21    8   35    8   23   28   19   21    16\n#&gt;  [7,]   10   13   23   35   21   29   11    7   33    -2\n#&gt;  [8,]   12   24   13   10    5   12   13   25   35    11\n#&gt;  [9,]   21   13   24   32    0   -6   11   15   26    16\n#&gt; [10,]    5   36   27   28   14   22   10   28   12     2\nm2 &lt;- apply(m1, c(1, 2), function(x) x %% 3)\nm3 &lt;- matrix(m1 %% 3, 10, 10) # Not the apply() function, but it also works\nm2\n#&gt;       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n#&gt;  [1,]    2    2    0    0    0    2    0    2    0     1\n#&gt;  [2,]    0    0    2    2    0    1    2    2    1     1\n#&gt;  [3,]    0    1    1    1    1    0    2    1    1     1\n#&gt;  [4,]    2    2    1    1    2    2    2    1    1     0\n#&gt;  [5,]    1    1    2    1    1    1    0    2    0     2\n#&gt;  [6,]    0    0    2    2    2    2    1    1    0     1\n#&gt;  [7,]    1    1    2    2    0    2    2    1    0     1\n#&gt;  [8,]    0    0    1    1    2    0    1    1    2     2\n#&gt;  [9,]    0    1    0    2    0    0    2    0    2     1\n#&gt; [10,]    2    0    0    1    2    1    1    1    0     2",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Exercise set 3</span>"
    ]
  },
  {
    "objectID": "problemset4.html",
    "href": "problemset4.html",
    "title": "\n16  Exercise set 4\n",
    "section": "",
    "text": "16.1 (1)\nWrite an R function that finds the root of a scalar function using the secant method. The function should also have two starting value inputs, optional inputs for secondary arguments of the input function, and not run indefinitely. It should return a list with the following elements:\nYou can validate your results using uniroot() with function:\n\\[\nf(x)=-2.6(x-0.1)+2.5(x-0.1)^4\n\\]\nwhere the root is in the interval \\([-1,1]\\). Use appropriate starting values.\nSolution:\nf &lt;- function(dX) {\n  dOut &lt;- -2.6 * (dX - 0.1) + 2.5 * (dX - 0.1)^4\n  return(dOut)\n}\n\nfSecant &lt;- function(f, dX0, dX1, dTol = 1e-9, max.iter = 1000, ...) {\n  iter &lt;- 0\n  dX2 &lt;- dX1\n  while ((abs(f(dX2, ...)) &gt; dTol) && (iter &lt; max.iter)) {\n    dX2 &lt;- dX1 - f(dX1, ...) * ((dX0 - dX1) / (f(dX0, ...) - f(dX1, ...)))\n    dX0 &lt;- dX1\n    dX1 &lt;- dX2\n    iter &lt;- iter + 1\n    cat(\"At iteration \", iter, \"value of x is: \", dX1, \"\\n\")\n  }\n  if (abs(f(dX2, ...)) &gt; dTol) {\n    return(list(root = NULL, f.root = NULL, iter = iter, \"Algorithm failed to converge. Maximum iterations reached.\"))\n  } else {\n    return(list(root = dX2, f.root = f(dX2), iterations = iter, \"Convergence reached.\"))\n  }\n}\n\nroot &lt;- fSecant(f, dX0 = -0.5, dX1 = 0.5)\n#&gt; At iteration  1 value of x is:  0.1587413 \n#&gt; At iteration  2 value of x is:  0.09544817 \n#&gt; At iteration  3 value of x is:  0.1000008 \n#&gt; At iteration  4 value of x is:  0.1\nroot\n#&gt; $root\n#&gt; [1] 0.1\n#&gt; \n#&gt; $f.root\n#&gt; [1] -1.941947e-13\n#&gt; \n#&gt; $iterations\n#&gt; [1] 4\n#&gt; \n#&gt; [[4]]\n#&gt; [1] \"Convergence reached.\"\nuniroot(f, interval = c(-1,1))\n#&gt; $root\n#&gt; [1] 0.1000007\n#&gt; \n#&gt; $f.root\n#&gt; [1] -1.764589e-06\n#&gt; \n#&gt; $iter\n#&gt; [1] 6\n#&gt; \n#&gt; $init.it\n#&gt; [1] NA\n#&gt; \n#&gt; $estim.prec\n#&gt; [1] 6.103516e-05\n\nvX &lt;- seq(-1, 1, 0.01)\nplot(vX, f(vX), type = \"l\")\nabline(h = 0, col = \"red\")\nabline(v = root[\"root\"], col = \"blue\", lty = 2)",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exercise set 4</span>"
    ]
  },
  {
    "objectID": "problemset4.html#section",
    "href": "problemset4.html#section",
    "title": "\n16  Exercise set 4\n",
    "section": "",
    "text": "the root\nthe function value at the root\nthe number of iterations used\na short description which stopping criterion was used and whether convergence as achieved",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exercise set 4</span>"
    ]
  },
  {
    "objectID": "problemset4.html#section-1",
    "href": "problemset4.html#section-1",
    "title": "\n16  Exercise set 4\n",
    "section": "\n16.2 (2)\n",
    "text": "16.2 (2)\n\nYou would like to obtain information about the (in)equality of the income distribution in population of people living in Denmark. For that you would like to estimate the standard deviation \\(\\sigma\\) of the distribution of yearly income \\(y_i^*\\). You observe independent income data \\(y_i\\) for a sample \\(i = 1, ..., n\\). However, the observed data \\(y_i\\) has been modified to match data protection standards. In particular it has first been demeaned and then all units with income over \\(c = 1.250.000\\) DKK have been removed. This gives us the truncation mechanism:\n\\[y_i = \\begin{cases} y_i^* - E[y_i^*] & \\text{if } y_i^* \\le c \\\\ - & \\text{else} \\end{cases}\\]\nTo estimate the standard deviation, you assume that the true income \\(y_i^*\\) is normally distributed. This implies that the observed income \\(y_i\\) follows a truncated mean-zero normal distribution with density \\(f(y)\\):\n\\[f(y; \\sigma, c) = \\frac{1}{\\sigma} \\frac{\\phi(y/\\sigma)}{\\Phi(c/\\sigma)}, \\quad -\\infty &lt; y \\le c,\\]\nwhere\n\\[\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}z^2\\right)\\] \\[\\Phi(x) = \\int_{-\\infty}^x \\phi(z) dz,\\]\ni.e. \\(\\phi(\\cdot)\\) and \\(\\Phi(\\cdot)\\) are the standard normal PDF and CDF respectively. To estimate \\(\\sigma\\), we can setup the log-likelihood function\n\\[\\ln L(\\sigma; \\mathbf{y}, c) = \\sum_{i=1}^n \\ln(f(y_i; \\sigma, c)),\\]\nwhich has first derivative (“Score”)\n\\[S(\\sigma; \\mathbf{y}, c) = \\frac{-n}{\\sigma} + \\frac{1}{\\sigma^3} \\sum_{i=1}^n y_i^2 + \\frac{cn}{\\sigma} f(c; \\sigma, c)\\]\nand second derivative (“Hessian”)\n\\[H(\\sigma^2) = \\frac{n}{\\sigma^2} - \\frac{3}{\\sigma^4} \\sum_{i=1}^n y_i^2 + \\left(\\frac{c^3 n}{\\sigma^4} - \\frac{2cn}{\\sigma^2}\\right) f(c; \\sigma, c) + \\frac{c^2 n}{\\sigma^2} f(c; \\sigma, c)^2.\\]\nWe want to find the estimate \\(\\hat{\\sigma}\\) which maximizes the log-likelihood function.\nWrite an R function fTrunc() that finds the maximum likelihood estimator for \\(\\sigma\\) using a Gauss-Newton algorithm. It should take the following inputs:\n\n\nA numerical vector \\(Y\\) of length \\(n\\) containing the dependent variable.\n\n\nA vector of starting values for the maximization with default being the sample standard deviation of \\(Y\\).\nThe number of maximum iterations for the optimization with default set to 200.\nA tolerance level for the stopping criterion with default set to 1e-16.\n\nThe overall function should return a list with the following components:\n\n\nThe estimated parameter \\(\\hat{\\sigma}\\).\n\nThe log-likelihood, Score and Hessian at the optimum.\nA character containing information about convergence and stopping condition of the optimization.\n\nLoad the data QPE_income1.R into R using readRDS() and estimate \\(\\hat{\\sigma}\\). The solution should be 509699.8.\nTry out a range of different starting values above and below the default. Plot the Score and Hessian along a grid of different \\(\\sigma\\) values. What do you observe?\nSolution:\n\nf &lt;- function(vY, dSigma, dC) {\n  return((1 / dSigma) * ((dnorm(vY / dSigma)) / (pnorm(dC / dSigma))))\n}\n\nfScore &lt;- function(vY, dSigma, f, dC) {\n  return(((-1 * length(vY)) / dSigma) + (1 / dSigma^3) * (sum(vY^2)) + ((dC * length(vY)) / (dSigma)) * f(dC, dSigma, dC))\n}\n\nfHessian &lt;- function(vY, dSigma, f, dC) {\n  return(length(vY) / dSigma^2 - (3 / dSigma^4) * sum(vY^2) + (dC^3 * length(vY) / dSigma^4 - 2 * dC * length(vY) / dSigma^2) * f(dC, dSigma, dC) + (dC^2 * length(vY) / dSigma^2) * f(dC, dSigma, dC)^2)\n}\n\nfTrunc &lt;- function(f, fScore, fHessian, vY, start.val = sqrt(var(vY)), n.max = 200, dTol = 1e-16, ...) {\n  \n  n &lt;- 0\n  dParam &lt;- start.val\n  \n  # Keep updating until stopping criterion or max iterations reached\n  while ((abs(fScore(vY, dParam, f, ...)) &gt; dTol) && (n &lt; n.max)) {\n    # Newton-Raphson updating\n    dParam &lt;- dParam - fScore(vY, dParam, f, ...) / fHessian(vY, dParam, f, ...)\n    n &lt;- n + 1\n    #cat(\"At iteration\", n, \"the value of the parameter is:\", dParam, \"\\n\")\n  }\n  \n  if (n == n.max) {\n    return(list(\n      param = NULL, \n      log.likelihood = NULL, \n      score = NULL,\n      hessian = NULL,\n      iter = n, \n      msg = \"Algorithm failed to converge. Maximum iterations reached.\")\n    )\n  } else {\n    return(list(\n      param = dParam, \n      log.likelihood = sum(log(f(vY, dParam, ...))), \n      score = fScore(vY, dParam, f, ...),\n      hessian = fHessian(vY, dParam, f, ...),\n      iter = n, \n      msg = \"Algorithm converged\")\n    )\n  }\n  \n}\n\nvX &lt;- readRDS(\"QPE_income1.R\")\nresults &lt;- fTrunc(f, fScore, fHessian, vX, dC = 1250000)\nresults\n#&gt; $param\n#&gt; [1] 509699.8\n#&gt; \n#&gt; $log.likelihood\n#&gt; [1] -19280.04\n#&gt; \n#&gt; $score\n#&gt; [1] -5.692061e-19\n#&gt; \n#&gt; $hessian\n#&gt; [1] -8.458564e-09\n#&gt; \n#&gt; $iter\n#&gt; [1] 4\n#&gt; \n#&gt; $msg\n#&gt; [1] \"Algorithm converged\"\n\n# Plot Score and Hessian\nsigmaVals &lt;- seq(400000, 600000, length.out = 200)\nscoreVals &lt;- sapply(sigmaVals, function(sigma) fScore(vX, sigma, f, dC = 1250000))\nhessianVals &lt;- sapply(sigmaVals, function(sigma) fHessian(vX, sigma, f, dC = 1250000))\n\n# Plot Score\nplot(sigmaVals, scoreVals, type = \"l\", main = \"Score Function\", xlab = \"Sigma\", ylab = \"Score\")\nabline(h = 0, col = \"red\")\n\n\n\n\n\n\n\n# Plot Hessian\nplot(sigmaVals, hessianVals, type = \"l\", main = \"Hessian Function\", xlab = \"Sigma\", ylab = \"Hessian\")\nabline(h = 0, col = \"red\")",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exercise set 4</span>"
    ]
  },
  {
    "objectID": "problemset5.html",
    "href": "problemset5.html",
    "title": "\n17  Exercise set 5\n",
    "section": "",
    "text": "17.1 (1)\nAssume you observe independent data \\((\\mathbf{X}_i, Y_i)\\) for \\(i = 1, \\dots, n\\) where \\(\\mathbf{X}_i\\) have realizations in \\(\\mathbb{R}^p\\) and \\(Y_i\\) is binary. You would like to model the relationship between \\(\\mathbf{X}_i\\) and \\(Y_i\\). A logistic regression model is given by the following structure: \\[\nY_i = \\mathbb{I}(Y_i^* &gt; 0)\n\\] \\[\nY_i^* = \\mathbf{X}_i'\\mathbf{\\beta} + \\varepsilon_i\n\\] \\[\nP(\\varepsilon_i \\le z) = p(z) = \\frac{1}{1 + \\exp(-z)}\n\\] Thus the average log-likelihood function is given by \\[\n\\ln L(\\mathbf{\\beta}) = \\frac{1}{n} \\sum_{i=1}^n \\left[ Y_i \\ln(p(\\mathbf{X}_i'\\mathbf{\\beta})) + (1-Y_i) \\ln(1-p(\\mathbf{X}_i'\\mathbf{\\beta})) \\right]\n\\] with Score \\[\nS(\\mathbf{\\beta}) = \\frac{1}{n} \\sum_{i=1}^n (Y_i - p(\\mathbf{X}_i'\\mathbf{\\beta}))\\mathbf{X}_i\n\\] and Hessian \\[\nH(\\mathbf{\\beta}) = -\\frac{1}{n} \\sum_{i=1}^n p(\\mathbf{X}_i'\\mathbf{\\beta})(1-p(\\mathbf{X}_i'\\mathbf{\\beta}))\\mathbf{X}_i \\mathbf{X}_i'\n\\]\nWrite an R function fLogit() that finds the maximum likelihood estimator for \\(\\mathbf{\\beta}\\) using a Gauss-Newton algorithm. It should take the following inputs:\nBefore optimization, if \\(\\mathbf{X}\\) does not contain a constant, your function should create a new \\(\\mathbf{X}\\) with an additional column of ones automatically. The overall function should return a list with the following components:\nSolution:\nfLogit &lt;- function(vY, mX, add.constant = TRUE, init.vals = NULL, max.iter = 200, dTol = 1e-16) {\n  \n  # probability function\n  p &lt;- function(vZ) {\n    return(1 / (1 + exp(-vZ)))\n  }\n  \n  # avg log-likelihood function\n  f &lt;- function(vY, mX, vB) {\n    return(mean(vY * log(p(mX %*% vB)) + (1 - vY) * log(1 - p(mX %*% vB))))\n  }\n  \n  fScore &lt;- function(vY, mX, vB) {\n    return(t(mX) %*% (vY - as.numeric(p(mX %*% vB))) / nrow(mX))\n  }\n\n  fHessian &lt;- function(vY, mX, vB) {\n    return(-(t(mX) %*% diag(as.numeric(p(mX %*% vB) * (1 - as.numeric(p(mX %*% vB))))) %*% mX) / nrow(mX))\n  }\n  \n  if (add.constant == TRUE) {\n    mX &lt;- cbind(1, mX)\n  }\n  if (is.null(init.vals)) init.vals &lt;- rep(0, ncol(mX))\n  \n  i &lt;- 0\n  vB &lt;- init.vals\n  \n  # Keep updating until stopping criterion or max iterations reached\n  while ((max(abs(fScore(vY, mX, vB))) &gt; dTol) && (i &lt; max.iter)) {\n    # Newton-Raphson updating\n    vB &lt;- vB - solve(fHessian(vY, mX, vB), fScore(vY, mX, vB))\n    i &lt;- i + 1\n    #cat(\"At iteration\", n, \"the value of the parameter is:\", dParam, \"\\n\")\n  }\n  \n  if (i == max.iter) {\n    return(list(\n      beta_hat = NULL, \n      log_likelihood_opt = NULL, \n      score_opt = NULL,\n      hessian_opt = NULL,\n      log_likelihood_null = NULL,\n      predicted_probabilities = NULL,\n      iterations = i,  \n      msg = \"Algorithm failed to converge. Maximum iterations reached.\")\n    )\n  } else {\n    return(list(\n      beta_hat = vB, \n      log_likelihood_opt = f(vY, mX, vB), \n      score_opt = fScore(vY, mX, vB),\n      hessian_opt = fHessian(vY, mX, vB),\n      log_likelihood_null = f(vY, mX, rep(0, ncol(mX))),\n      predicted_probabilities = p(mX %*% vB),\n      iterations = i,  \n      msg = \"Algorithm converged\")\n    )\n  }\n}",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Exercise set 5</span>"
    ]
  },
  {
    "objectID": "problemset5.html#section",
    "href": "problemset5.html#section",
    "title": "\n17  Exercise set 5\n",
    "section": "",
    "text": "A numerical or logical input vector \\(Y\\) of length \\(n\\) containing the dependent variable.\n\n\nA \\([n \\times p]\\) dimensional matrix \\(\\mathbf{X}\\) containing the regressors.\n\n\nAn option whether a constant column should be added to \\(\\mathbf{X}\\) with default TRUE.\n\n\nA vector of starting values for the maximization with default being a zero vector of dimension \\(p\\).\n\nThe number of maximum iterations for the optimization with default set to 200.\n\nA tolerance level for the stopping criterion with default set to \\(1\\text{e-}16\\).\n\n\n\n\n\nThe estimated parameters \\(\\hat{\\mathbf{\\beta}}\\).\n\nThe average log-likelihood, Score and Hessian at the optimum.\n\nThe log-likelihood function at \\(\\mathbf{\\beta} = \\mathbf{0}\\).\n\n\nThe predicted probabilities \\(p(\\mathbf{X}_i'\\mathbf{\\beta})\\).\n\nA character containing information about convergence and stopping condition of the optimization.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Exercise set 5</span>"
    ]
  },
  {
    "objectID": "problemset5.html#section-1",
    "href": "problemset5.html#section-1",
    "title": "\n17  Exercise set 5\n",
    "section": "\n17.2 (2)\n",
    "text": "17.2 (2)\n\nSimulate a process like the one above for \\(p = 10\\) and \\(n = 2000\\) using random variable generators and estimate it using your function. Validate your results using the built-in glm() function with option family = binomial.\n\nset.seed(123)\nn &lt;- 2000\np &lt;- 10\nmX &lt;- matrix(rnorm(n*p), n, p)\nvB.actual &lt;- matrix(1:p, p, 1)\nvY.star &lt;- mX %*% vB.actual + rnorm(2000, 0, 1)\nvY.actual &lt;- vY.star &gt; 0\n\nvResults &lt;- fLogit(vY.actual, mX)\nglm_fit &lt;- glm(vY.actual ~ mX, family = binomial)\n#&gt; Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nglm_fit$coefficients\n#&gt; (Intercept)         mX1         mX2         mX3         mX4         mX5 \n#&gt;  -0.1927213   1.8925010   3.4933457   4.8288383   6.4561863   8.4108110 \n#&gt;         mX6         mX7         mX8         mX9        mX10 \n#&gt;  10.2085496  11.8431135  13.2432309  14.8829116  16.5606978\nvResults$beta_hat\n#&gt;             [,1]\n#&gt;  [1,] -0.1927213\n#&gt;  [2,]  1.8925010\n#&gt;  [3,]  3.4933457\n#&gt;  [4,]  4.8288384\n#&gt;  [5,]  6.4561864\n#&gt;  [6,]  8.4108112\n#&gt;  [7,] 10.2085499\n#&gt;  [8,] 11.8431138\n#&gt;  [9,] 13.2432312\n#&gt; [10,] 14.8829119\n#&gt; [11,] 16.5606982",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Exercise set 5</span>"
    ]
  },
  {
    "objectID": "problemset6.html",
    "href": "problemset6.html",
    "title": "\n18  Exercise set 6\n",
    "section": "",
    "text": "In this problem, you are supposed to debug a C++ version of the function used in Exercise Set 5 for logistic regression. It is supposed to take the same inputs, but with a mandatory starting value input vector for the optimization and only have outputs (a) - (d).\n\n\nOpen the function fLogitCpp() that can be sourced via sourceCpp()\n\n\nMake comments on where to debug the code. You may consider the following questions: - Were all objects declared? - Are all objects declared the correct type? - Are the correct functions used? - Are the input and outputs correct? - Is the algorithmic structure correct? - Hint: There are 8 bugs overall.\n\nDebug the code and compare the solutions to the fLogit() function from Exercise Set 5.\n\n\nSolution:\nThe original function including comments on where to debug has been added below:\n\n//[[Rcpp::depends(RcppArmadillo)]]\n#include &lt;RcppArmadillo.h&gt;\nusing namespace Rcpp;\nusing namespace arma;\n\n//initialize auxiliary functions: prb, LL, score, hessian\n\n// mX should be initialized with mat\nvec fP(vec vBeta, mX){\n  return(1 / (1 + exp( -mX * vBeta )));\n}\n// vP should be initialized with vec, and also it should return a double\nvec fLnL(vec vBeta, vec vY, mat mX){\n  vP = fP(vBeta,mX);\n  vec vLnL = mean( vY % log(vP) + (ones&lt;vec&gt;(vY.n_elem) - vY) * log(ones&lt;vec&gt;(vP.n_elem) - vP), 0); //should be a %, not a *\n  return vLnL;\n}\n// vScore is not divided by the number of rows in mX\nvec fScore(vec vBeta, vec vY, mat mX){\n  vec vP = fP(vBeta,mX);\n  vec vScore = trans(mX)*(vY - vP);\n  \n  return vScore;\n}\n//\nmat fHessian(vec vBeta, vec vY, mat mX){\n  int iN = vY.n_elem;\n  vec vP = fP(vBeta,mX);\n  mat mP = zeros&lt;mat&gt;(iN,iN);\n  \n  mP.diag(0) = vP % (ones&lt;vec&gt;(iN) - vP);\n  mat mHessian = -trans(mX)*mP*mX / iN;\n  \n  return mHessian;\n}\n\n//[[Rcpp::export]]\nList fLogitCpp(vec vY, mat mX, vec vBeta0, // vBeta should be initialized with default values\n               bool constant = true, \n               double dTol = 1e-9, \n               bool imax = 200){ // this should be an int\n  \n  List lOut;\n  int iN = vY.n_elem;\n  \n  if (constant == true){\n    vec vOnes = zeros&lt;vec&gt;(iN); // this should be ones\n    vBeta0 = join_cols(zeros&lt;vec&gt;(1),vBeta0);\n    mX = join_rows(vOnes,mX);\n    //Rcout &lt;&lt; \"Note: A constant has been added to data matrix mX\" &lt;&lt; std::endl;\n  }\n  \n  vec vBeta = vBeta0;\n  vec vScore = fScore(vBeta,vY,mX);\n  mat mHessian = fHessian(vBeta,vY,mX);\n  \n  int i = 0;\n  while ((max(abs(vScore)) &lt; dTol) & (i &lt; imax)) { // first sign should be flipped\n    //Rcout &lt;&lt; i &lt;&lt; \"\\n\";\n    vBeta = vBeta - solve(fHessian(vBeta,vY,mX),fScore(vBeta,vY,mX));\n    vScore = fScore(vBeta,vY,mX);\n    mHessian = fHessian(vBeta0,vY,mX); // this should be vBeta\n    \n    i++;\n  }\n  if (i == imax){\n    Rcout &lt;&lt; \"newton failed to converge\" &lt;&lt; std::endl;\n    return lOut;\n  } else {\n   // Rcout &lt;&lt; \"Convergence achieved after \" &lt;&lt; i &lt;&lt; \" iterations\" &lt;&lt; std::endl\n   //       &lt;&lt; \"due to max(abs(score)) &lt;\" &lt;&lt; dTol &lt;&lt; std::endl;\n    lOut[\"coefficients\"] = vBeta;\n    lOut[\"logLik0\"] = fLnL(zeros&lt;vec&gt;(vBeta.n_elem),vY,mX);\n    lOut[\"logLik\"] = fLnL(vBeta,vY,mX);\n    lOut[\"score\"] = vScore;\n    lOut[\"hessian\"] = mHessian;\n    lOut[\"phat\"] = fP(vBeta,mX);\n    \n    return lOut;\n  }\n}\n\nComparing the solutions:\n\nsuppressMessages(library(Rcpp))\n#&gt; Warning: pakke 'Rcpp' blev bygget under R version 4.3.3\nsuppressMessages(library(RcppArmadillo))\n#&gt; Warning: pakke 'RcppArmadillo' blev bygget under R version 4.3.3\nsourceCpp(\"fLogitCpp.cpp\")\n\nset.seed(123)\nn &lt;- 2000\np &lt;- 10\nmX &lt;- matrix(rnorm(n*p), n, p)\nvB.actual &lt;- matrix(1:p, p, 1)\nvY.star &lt;- mX %*% vB.actual + rnorm(2000, 0, 1)\nvY.actual &lt;- vY.star &gt; 0\n\nvResults &lt;- fLogitCpp(vY.actual, mX, vBeta = rep(0, ncol(mX)))\nvResults$coefficients\n#&gt;             [,1]\n#&gt;  [1,] -0.1927213\n#&gt;  [2,]  1.8925004\n#&gt;  [3,]  3.4933444\n#&gt;  [4,]  4.8288365\n#&gt;  [5,]  6.4561838\n#&gt;  [6,]  8.4108079\n#&gt;  [7,] 10.2085458\n#&gt;  [8,] 11.8431091\n#&gt;  [9,] 13.2432260\n#&gt; [10,] 14.8829059\n#&gt; [11,] 16.5606916",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exercise set 6</span>"
    ]
  },
  {
    "objectID": "exam2020.html",
    "href": "exam2020.html",
    "title": "\n19  Ordinary exam 2020\n",
    "section": "",
    "text": "19.1 Problem 1:\nIn this first part of the exam you are tasked with several independent problems. If you are unable to solve an exercise move on to the next. Set and use the seed 1337 for all exercises in Problem 1.\nSolution:\nset.seed(1337)\ndfData &lt;- data.frame(\n  rvnorm = rnorm(100, 10, sqrt(100)),\n  rvunif = runif(100)\n)\n\ncolMeans(dfData)\n#&gt;     rvnorm     rvunif \n#&gt; 12.3705575  0.4927623\napply(dfData, 2, sd)\n#&gt;     rvnorm     rvunif \n#&gt; 10.6541506  0.2865665\nmA &lt;- matrix(rnorm(10000), nrow = 100, ncol = 100)\nPrint your result to the console. Then define a new matrix equal to mA with every second row and every fifth column removed. Print to the console the second column of your new smaller matrix.\nSolution:\nset.seed(1337)\nmA &lt;- matrix(rnorm(10000), nrow = 100, ncol = 100)\nsum(mA &gt; 0)\n#&gt; [1] 4968\n\nindices &lt;- seq(1, nrow(mA))\nmB &lt;- mA[indices %% 2 != 0, indices %% 5 != 0]\nmB[, 2]\n#&gt;  [1]  0.20997680  1.37178507 -0.20485705 -0.80947218  0.06873847 -0.46916210\n#&gt;  [7] -0.73977783  0.37774034 -0.90396001 -0.23914384 -0.31475060 -0.54221576\n#&gt; [13] -0.13401439 -0.44703293 -0.58097800 -0.64590441 -0.43772050  1.70627166\n#&gt; [19]  1.28239811  3.10367380 -0.25455142  0.11397658  0.28331947 -0.25257320\n#&gt; [25] -0.10787613 -1.68283890  0.59339153  1.42397245 -0.16762034 -0.28949467\n#&gt; [31] -0.38718739 -0.86600469 -0.16801229 -0.05423047 -0.15457321  1.20115726\n#&gt; [37]  0.89901990 -0.81578612  0.56388549 -0.52282158  0.55286246 -0.05125182\n#&gt; [43] -0.64952770  1.54842553 -1.13708264 -0.21124735 -1.45882620 -0.13385242\n#&gt; [49]  0.31359063 -0.94320880\nSolution:\nset.seed(1337)\nMonteCarlo.Integration &lt;- function(f, n, a, b) {\n  \n  U &lt;- runif(n, min = a, max = b)\n  return( (b-a)*mean(f(U)) )\n  \n}\nMonteCarlo.Integration(function(x) (x^3 - x^2 + 9), 1e7, 3, 20)\n#&gt; [1] 37477.23\ndiag.covar &lt;- function(mX, mY) {\n      varcov &lt;- 0\n      for(j in 1:10) {\n        dsum &lt;- 0\n        for(i in 1:10) {\n          dsum &lt;- dsum + (mX[i,j] - mean(mX[,j]))*(mY[i,j] - mean(mY[,j]))\n        }\n        varcov[j] &lt;- dsum/(dim(mX)[1]-1)\n      }\n      return(varcov)\n    }\n(The above function is included in the diagcovar.R file in the exam hand-out.)\nCreate a new function, say diag.covarVec, that is fully vectorized (or as vectorized as possible). Compare the run time of the two functions using,\nmX &lt;- matrix(rnorm(100), nrow = 10, ncol = 10)\nmY &lt;- matrix(rnorm(100), nrow = 10, ncol = 10)\nYou may, but do not need to, use the functionalities of the microbenchmark package. How much faster is your new function? If you cannot implement the vectorization, explain potential causes for inefficiencies and poor programming practices in the diag.covar function via comments in your script. If your code is not faster, comment briefly in the script why that might be the case.\nSolution:\nset.seed(1337)\ndiag.covar &lt;- function(mX, mY) {\n  varcov &lt;- 0\n  for(j in 1:10) {\n    dsum &lt;- 0\n    for(i in 1:10) {\n      dsum &lt;- dsum + (mX[i,j] - mean(mX[,j]))*(mY[i,j] - mean(mY[,j]))\n    }\n    varcov[j] &lt;- dsum/(dim(mX)[1]-1)\n  }\n  return(varcov)\n}\n\ndiag.covarVec &lt;- function(mX, mY) {\n  mSum &lt;- (t(mX) - colMeans(mX)) * (t(mY) - colMeans(mY))\n  varcov &lt;- rowSums(mSum) / (dim(mX)[1]-1)\n  return(varcov)\n}\n\nmX &lt;- matrix(rnorm(100), nrow = 10, ncol = 10)\nmY &lt;- matrix(rnorm(100), nrow = 10, ncol = 10)\n\ndiag.covar(mX, mY)\n#&gt;  [1] -0.002280477  0.133113794  0.354419701 -0.881358047 -0.134682108\n#&gt;  [6] -0.436054671 -0.425000809 -0.119402997 -0.309367075  0.287553344\ndiag.covarVec(mX, mY)\n#&gt;  [1] -0.002280477  0.133113794  0.354419701 -0.881358047 -0.134682108\n#&gt;  [6] -0.436054671 -0.425000809 -0.119402997 -0.309367075  0.287553344\n\nsuppressMessages(library(microbenchmark))\n#&gt; Warning: pakke 'microbenchmark' blev bygget under R version 4.3.3\nmicrobenchmark(diag.covar(mX, mY), diag.covarVec(mX, mY))\n#&gt; Unit: microseconds\n#&gt;                   expr   min     lq     mean  median      uq    max neval\n#&gt;     diag.covar(mX, mY) 998.1 1024.8 1084.195 1051.05 1085.75 2555.7   100\n#&gt;  diag.covarVec(mX, mY)  18.9   20.0   73.218   22.00   26.70 4839.0   100\nSolution:\nset.seed(1337)\nGumbel &lt;- function(mu, beta, size) {\n  U &lt;- runif(size)\n  return(mu - beta * log(-log(U)))\n}\n\nvX &lt;- Gumbel(0.5, 2, 10000)\nhist(vX, \n     freq = FALSE,\n     breaks = 141,\n     col = \"cornflowerblue\", \n     xlab = \"\",\n     ylab = \"Density\",\n     main = \"\",\n     xlim = c(min(vX), max(vX)))\n\nvInput &lt;- seq(-5, 20, 0.05)\n\nfGumbelPdf &lt;- function(mu, beta, x) {\n  return(1/beta * exp(-((x-mu)/beta + exp(-(x-mu)/beta))))\n}\n\nlines(vInput, fGumbelPdf(0.5, 2, vInput), type = \"l\", col = \"red\", lwd = 2)\n\nlegend(\"topright\",\n       legend = c(\"simulated\", \"actual\"),\n       col = c(\"cornflowerblue\", \"red\"),\n       lwd = 2)",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Ordinary exam 2020</span>"
    ]
  },
  {
    "objectID": "exam2020.html#problem-1",
    "href": "exam2020.html#problem-1",
    "title": "\n19  Ordinary exam 2020\n",
    "section": "",
    "text": "Create a data structure of the type data.frame consisting of two equal length columns. The first column should contain 100 draws from the Gaussian (normal) distribution with mean 10 and variance 100, i.e. \\(N(10, 100)\\). The second column should contain 100 draws from the standard uniform distribution, i.e. \\(U(0,1)\\). Assign the two columns in the data.frame the names “rnorm” and “runif”. Calculate and print the mean and standard deviation of each column of this data.frame.\n\n\n\n\nCount the number of strictly positive (\\(&gt;0\\)) numerically valued elements in\n\n\n\n\n\n\nUsing \\(10^7\\) draws from the appropriate uniform distribution use Monte Carlo integration to approximate, \\[ I = \\int_3^{20} (x^3 - x^2 + 9)dx. \\] Print your approximation of \\(I\\) to the console.\n\n\n\n\nConsider the following function to calculate the diagonal of the sample covariance matrix between two matrices, mX and mY\n\n\n\n\n\n\n\n\n\n\nConstruct a function that uses the inversion method to generate random variables, \\(X \\sim \\text{Gumbel}(\\mu, \\beta)\\), where \\(\\mu \\in \\mathbb{R}\\) is the location parameter and \\(\\beta &gt; 0\\) the scale parameter of a Gumbel distribution. The Gumbel has the following cumulative distribution function, \\[ F_X(x) = \\exp\\left(-\\exp\\left(-\\frac{x-\\mu}{\\beta}\\right)\\right), \\quad \\beta &gt; 0, \\mu, x \\in \\mathbb{R} \\] with probability density function, \\[ f_X(x) = \\frac{1}{\\beta} \\exp\\left(-\\left(\\frac{x-\\mu}{\\beta} + \\exp\\left(-\\frac{x-\\mu}{\\beta}\\right)\\right)\\right), \\quad \\beta &gt; 0, \\mu, x \\in \\mathbb{R}. \\] Use your function to generate a sequence of 10,000 random variables from \\(\\text{Gumbel}(0.5, 2)\\). Produce a histogram of your generated random variable sequence using the built-in function hist(). In the histogram set breaks equal to 141 and limit the range for the x-axis of the plot from the smallest to the largest value in your generated sequence of random Gumbels. In addition superimpose the theoretical probability density function for comparison. In order to superimpose the theoretical probability density unto the histogram use the built-in lines() function with appropriate inputs (one input could be the seq() function that generates a sequence from -5 to 20 by step increments of 0.05).",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Ordinary exam 2020</span>"
    ]
  },
  {
    "objectID": "exam2020.html#problem-2",
    "href": "exam2020.html#problem-2",
    "title": "\n19  Ordinary exam 2020\n",
    "section": "\n19.2 Problem 2:",
    "text": "19.2 Problem 2:\nConsider the following function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) and its derivatives: \\[ f(x) = \\sin(x + \\pi - 1) \\quad (1.1) \\] \\[ f'(x) = \\cos(x + \\pi - 1) \\quad (1.2) \\] \\[ f''(x) = -\\sin(x + \\pi - 1) \\quad (1.3) \\]\n\nWrite three R functions with input \\(x\\) that return \\(f(x)\\), \\(f'(x)\\), and \\(f''(x)\\). Provide a line plot of the function and its first derivative on the interval [-1,1].\n\nSolution:\n\nf &lt;- function(x) {\n  return(sin(x + pi - 1))\n}\n\nfPrime &lt;- function(x) {\n  return(cos(x + pi - 1))\n}\n\nfDoublePrime &lt;- function(x) {\n  return(-sin(x + pi - 1))\n}\n\nvX &lt;- seq(-1, 1, 0.01)\nplot(vX, f(vX), type = \"l\", col = \"cornflowerblue\", lwd = 2, ylim = c(-1, 1))\nlines(vX, fPrime(vX), type = \"l\", col = \"red\", lwd = 2)\n\nlegend(\"topright\",\n       legend = c(\"f\", \"fprime\"),\n       col = c(\"cornflowerblue\", \"red\"),\n       lwd = 2)\n\n\n\n\n\n\n\n\nWrite an R function that maximizes R scalar functions using the simple Newton-Raphson method with analytical derivatives. The function should also have a starting value input, two reasonable options for the convergence criterion that can be specified by the user, and not run indefinitely. It should return a list with the following elements:\n\nthe maximizer\nthe function value at the optimum\nthe number of iterations used\na short description which stopping criterion was used and whether convergence was achieved\n\n\n\nSolution:\n\nNM &lt;- function(f, f_prime, f_sec, dX0, dTol = 1e-9, n.max = 1000){\n  dX &lt;- dX0\n  fx &lt;- f(dX)\n  fpx &lt;- f_prime(dX)\n  fsx &lt;- f_sec(dX)\n  n &lt;- 0\n  while ((abs(fpx) &gt; dTol) && (n &lt; n.max)) {\n    dX &lt;- dX - fpx/fsx\n    fx &lt;- f(dX)\n    fpx &lt;- f_prime(dX)\n    fsx &lt;- f_sec(dX)\n    n &lt;- n + 1\n  }\n  if (n == n.max) {\n    return(list(\n      maximizer = dX,\n      function.val = f(dX),\n      n.iter = n,\n      msg = \"Failed to converge. Max number of iterations reached.\"\n    ))\n  } else {\n    return(list(\n      maximizer = dX,\n      function.val = f(dX),\n      n.iter = n,\n      msg = \"Convergence reached.\"\n    ))\n  }\n}\n\n\nWrite a C++ function using Rcpp and RcppArmadillo that can be loaded via sourceCpp(). The function should contain a bisection method to find the root of a scalar function. The function should be able to take any R scalar function as input. Set the default interval for the root between [-1,1]. It should return or print a warning if the starting conditions for the bisection method are not fulfilled. Else it should return a list with the following elements:\n\nthe root\nthe function value at the root\nthe number of iterations used Remark: If you cannot solve it in C++ you can earn some points for doing it in R\n\n\n\n\n\n// [[Rcpp::depends(RcppArmadillo)]]\n#include &lt;RcppArmadillo.h&gt;\n\nusing namespace arma;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nList bisection_cpp(Function f, double dXLeft = -1.0, double dXRight = 1.0, double dTol = 0.0001, int maxIter = 1000) {\n  if (dXLeft &gt;= dXRight) {\n    stop(\"Starting conditions not met\");\n  }\n  double fLeft = as&lt;double&gt;(f(dXLeft));\n  double fRight = as&lt;double&gt;(f(dXRight));\n  if (fLeft == 0) {\n    List lOut;\n    lOut[\"root\"] = dXLeft;\n    lOut[\"func_at_root\"] = fLeft;\n    lOut[\"num_ite\"] = 0;\n    return lOut;\n  } else if (fRight == 0) {\n    List lOut;\n    lOut[\"root\"] = dXRight;\n    lOut[\"func_at_root\"] = fRight;\n    lOut[\"num_ite\"] = 0;\n    return lOut;\n  } else if (fLeft * fRight &gt; 0) {\n    stop(\"error: f(x.l)*f(x.r) &gt; 0\");\n  }\n  \n  int iter = 0;\n  while ((dXRight - dXLeft) &gt; dTol && (iter &lt; maxIter)) {\n    double dXMid = (dXLeft + dXRight)/2;\n    double fMid = as&lt;double&gt;(f(dXMid));\n    if (fMid == 0) {\n      return(dXMid);\n    } else if (fLeft * fMid &lt; 0) {\n      dXRight = dXMid;\n      fRight = fMid;\n    } else {\n      dXLeft = dXMid;\n      fLeft = fMid;\n    }\n    iter = iter + 1;\n  }\n  \n  List lOut;\n  lOut[\"root\"] = (dXLeft + dXRight)/2;\n  lOut[\"func_at_root\"] = as&lt;double&gt;(f((dXLeft + dXRight)/2));\n  lOut[\"num_ite\"] = iter;\n  return lOut;\n}\n\n\nsuppressMessages(library(Rcpp))\n#&gt; Warning: pakke 'Rcpp' blev bygget under R version 4.3.3\nsuppressMessages(library(RcppArmadillo))\n#&gt; Warning: pakke 'RcppArmadillo' blev bygget under R version 4.3.3\nsourceCpp('exam2020cpp.cpp')\n\n\nUse the functions from 2.) and 3.) and the build-in functions optim() and uniroot() to find the maximizer of \\(f(x)\\) in the interval [-1,1]. Use appropriate starting values. Also provide a benchmark comparing their computation times. Remark: If you cannot solve 2.) and/or 3.) you can still earn points for using the remaining functions only.\n\n\nSolution:\n\nNM(f, fPrime, fDoublePrime, 0)$maximizer\n#&gt; [1] -0.5707963\nbisection_cpp(fPrime, dXLeft = -1, dXRight = 1)$root\n#&gt; [1] -0.5707703\noptim(0, f, method = \"L-BFGS-B\", lower = -1, upper = 1, control=list(fnscale=-1))$par\n#&gt; [1] -0.5707963\nuniroot(fPrime, c(-1, 1))$root\n#&gt; [1] -0.5707835\n\nsuppressMessages(library(microbenchmark))\nmicrobenchmark(\n  NM = NM(f, fPrime, fDoublePrime, 0)$maximizer,\n  Bisection = bisection_cpp(fPrime, dXLeft = -1, dXRight = 1)$root,\n  Optim = optim(0, f, method = \"L-BFGS-B\", lower = -1, upper = 1, control=list(fnscale=-1))$par,\n  UniRoot = uniroot(fPrime, c(-1, 1))$root\n)\n#&gt; Unit: microseconds\n#&gt;       expr  min    lq   mean median    uq    max neval\n#&gt;         NM  8.2  9.00 10.097  10.00 10.65   15.8   100\n#&gt;  Bisection 20.6 21.95 37.109  25.00 26.70 1146.7   100\n#&gt;      Optim 35.4 39.20 43.086  41.05 43.45  116.2   100\n#&gt;    UniRoot 34.0 36.40 43.352  39.90 43.10  220.9   100\n\n\nCreate an R package that contains the functions from 2.) and 3.) and edit the title description to “This is my exam package”. Export the package as a bundled development version. Remark: If you cannot solve 2.) or 3.) you can still earn points. Create a package that contains an R and a C++ function with single scalar inputs that always return the number 5.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Ordinary exam 2020</span>"
    ]
  },
  {
    "objectID": "exam2021.html",
    "href": "exam2021.html",
    "title": "\n20  Ordinary exam 2021\n",
    "section": "",
    "text": "20.1 Problem 1:\nBegin by setting the seed to 134.\nSolution:\nset.seed(134)\n\nfn_slowCauchy &lt;- function(sigma, samples, length) {\n  mX &lt;- matrix(0, nrow = length, ncol = samples)\n  U &lt;- runif(samples * length)\n  mU &lt;- matrix(U, nrow = length, ncol = samples)\n  for (i in 1:samples) {\n    for (j in 1:length) {\n      mX[j, i] &lt;- tan(pi * (mU[j, i] - 1/2)) * sigma\n    }\n  }\n  return(mX)\n}\n\nmX &lt;- fn_slowCauchy(2, 5000, 1000)\nSolution:\nset.seed(134)\n\nfn_fastCauchy &lt;- function(sigma, samples, length) {\n  U &lt;- runif(samples * length)\n  vCauchy &lt;- tan(pi * (U - 1/2)) * sigma\n  mY &lt;- matrix(vCauchy, length, samples, byrow = FALSE)\n  return(mY)\n}\n\nmY &lt;- fn_fastCauchy(2, 5000, 1000)\n\nall.equal(mX, mY)\n#&gt; [1] TRUE\nSolution:\nfn_estimateScale &lt;- function(mX) {\n  calc_scale &lt;- function(vX) {\n    vY &lt;- ifelse(vX &lt; 0, vX * -1, vX)\n    vY &lt;- sort(vY, decreasing = FALSE)\n    return(vY[ceiling(length(vY) / 2)])\n  }\n  return(apply(mX, 2, calc_scale))\n}\n\nvSigma &lt;- fn_estimateScale(mX)\nhist(vSigma, \n     freq = F,\n     breaks = 50,\n     col = \"cornflowerblue\", \n     xlab = \"Estimates\",\n     ylab = \"\",\n     main = \"Distribution of estimates\",\n     xlim = c(min(vSigma), max(vSigma)))\n\ncurve(dnorm(x, mean(vSigma), sd(vSigma)),\n      from = min(vSigma),\n      to = max(vSigma),\n      col = \"red\",\n      lwd = 2,\n      add = TRUE)\n\nlegend(\"topright\", legend = c(\"histogram\", \"density\"), col = c(\"cornflowerblue\", \"red\"), lwd = 2)\ng &lt;- function(x) {\n  return(-0.2 * x^3 + 3 * x^2 + 5 * x - 3)\n}\n\ngPrime &lt;- function(x) {\n  return(-0.6 * x^2 + 6 * x + 5)\n}\n\nfn_findMax &lt;- function(f, gr, dX0 = 0, dTol = 0.00001, max.iter = 200) {\n  dXn &lt;- dX0 + sign(gr(dX0)) / sqrt(1)\n  n &lt;- 1\n  while (abs(gr(dXn)) &gt; dTol & n &lt; max.iter) {\n    dXn &lt;- dXn + sign(gr(dXn)) / sqrt(n + 1)\n    n &lt;- n + 1\n  }\n  \n  if (n == max.iter) {\n    return(list(\n      param = dXn,\n      max.val = f(dXn),\n      num.iter = n,\n      msg = \"Maximum number of iterations reached. Stopping...\"\n    ))\n  } else {\n    return(list(\n      param = dXn,\n      max.val = f(dXn),\n      num.iter = n,\n      msg = \"Convergence reached.\"\n    ))\n  }\n}\n\nlSolution &lt;- fn_findMax(g, gPrime)\nlSolution\n#&gt; $param\n#&gt; [1] 10.7735\n#&gt; \n#&gt; $max.val\n#&gt; [1] 148.98\n#&gt; \n#&gt; $num.iter\n#&gt; [1] 101\n#&gt; \n#&gt; $msg\n#&gt; [1] \"Convergence reached.\"\n\nvX &lt;- seq(-20, 20, by = 0.05)\nplot(vX, g(vX), type = \"l\")\nabline(v = lSolution$param, col = \"black\")",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ordinary exam 2021</span>"
    ]
  },
  {
    "objectID": "exam2021.html#problem-1",
    "href": "exam2021.html#problem-1",
    "title": "\n20  Ordinary exam 2021\n",
    "section": "",
    "text": "Use the inversion method to simulate data from a Cauchy distribution with scale-parameter \\(\\sigma = 2\\). The CDF is given by \\[ F(X) = \\frac{1}{\\pi} \\tan^{-1}\\left(\\frac{X}{\\sigma}\\right) + \\frac{1}{2} \\] and we wish to simulate \\(N=5000\\) samples of length \\(T=1000\\). Write a function, fn_slowCauchy, which uses a nested for-loop to loop over \\(N\\) and \\(T\\) and transforms one \\(U(0,1)\\) variable at a time. The function must output a \\(T \\times N\\) matrix mX, and must be populated column-wise.\n\n\n\n\nYou now realize that your function is very inefficient, and you decide to write a different one. Your new function, fn_fastCauchy, must be vectorized and generate a \\(T \\times N\\) matrix mY which is identical to mX. It is up to you to write it in whichever way you believe is most efficient (while still using the inversion method). Verify that mX and mY are identical with the all.equal function. (Hint: remember to re-set the seed before calling each of your functions).\n\n\n\n\nWe can estimate the scale-parameter of our Cauchy random variables as \\[ \\hat{\\sigma} = \\text{median}(|X|) \\] Write a function, fn_estimateScale, which estimates \\(\\hat{\\sigma}\\) for each column in mX. Save the resulting 5000 estimates in a vector vSigma. However, your function must NOT use the built-in functions abs and median. You must compute the median of the absolute value of each column without using those two functions.\n\n\n\n\nCreate a histogram of the estimates \\(\\hat{\\sigma}\\) contained in the vector vSigma. Set breaks = 50 and freq = F. Let the title of the plot be “Distribution of estimates” and set the label on the x-axis to “Estimates”. Add a density plot to the histogram you have just created. Assume \\(\\hat{\\sigma}\\) is Gaussian and use the empirical mean and standard deviation as the parameters. Superimpose the density plot on the histogram with lines().\n\n\n\nYou wish to find the maximum of a \\(3^{rd}\\) order polynomial given by \\[ g(x) = -0.2x^3 + 3x^2 + 5x - 3 \\] The method you have chosen is an iterative scheme which proceeds by the following algorithm\n\nBegin at \\(x_0 = 0\\)\n\nSet \\(x_n = x_{n-1} + \\frac{\\text{sign}(g'(x_{n-1}))}{\\sqrt{n}}\\)\n\nRepeat until \\(|g'(x_n)| &lt; \\varepsilon\\) with \\(\\varepsilon = 0.00001\\) where \\(g'\\) is the derivative of \\(g\\) wrt. \\(x\\). Write a function, fn_findMax, that implements the algorithm and returns a List with the final value of \\(x_n\\), the maximum value of \\(g(\\cdot)\\) in that point, as well as the number if iterations it took to get there.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ordinary exam 2021</span>"
    ]
  },
  {
    "objectID": "exam2021.html#problem-2-constrained-optimization-c-and-packaging",
    "href": "exam2021.html#problem-2-constrained-optimization-c-and-packaging",
    "title": "\n20  Ordinary exam 2021\n",
    "section": "\n20.2 Problem 2: (Constrained) Optimization, C++, and Packaging",
    "text": "20.2 Problem 2: (Constrained) Optimization, C++, and Packaging\nNote: This problem is best solved in order from 1 - 7.\nIn this problem, you are supposed to work with the mDATA.r file provided with the exam. It contains the daily stock returns for the stocks of two Danish companies, Danske Bank and Ørsted, from 9th of June 2016 to 9th of March 2021 traded on the Copenhagen Stock Exchange. In the following, we denote \\(r_{A,t}\\) as the return of Danske Bank and \\(r_{B,t}\\) as the return of Ørsted at time period \\(t\\).\n\nLoad the data file mDATA.r into your R workspace using readRDS().\n\n\nset.seed(123)\ngenData &lt;- function() {\n  # --- Generate mDATA.r ---\n  set.seed(123) # for reproducibility\n\n# True parameters for data generation\nT_obs &lt;- 1200 # Number of observations (approx 4.75 years of daily data)\n# GARCH parameters for stock A\nomega_A_true &lt;- 0.00001\nalpha_A_true &lt;- 0.08\nbeta_A_true  &lt;- 0.90\n# GARCH parameters for stock B\nomega_B_true &lt;- 0.000015\nalpha_B_true &lt;- 0.07\nbeta_B_true  &lt;- 0.91\n# Constant conditional correlation\nrho_true     &lt;- 0.6\n\n# Initialize vectors\nr_A &lt;- numeric(T_obs)\nr_B &lt;- numeric(T_obs)\nsigma2_A_t &lt;- numeric(T_obs)\nsigma2_B_t &lt;- numeric(T_obs)\n\n# Initial unconditional variances\nsigma2_A_t[1] &lt;- omega_A_true / (1 - alpha_A_true - beta_A_true)\nsigma2_B_t[1] &lt;- omega_B_true / (1 - alpha_B_true - beta_B_true)\n\n# Simulate GARCH processes and returns\n# Generate correlated normal innovations\ninnovations &lt;- matrix(rnorm(2 * T_obs), ncol = 2)\nchol_R &lt;- matrix(c(1, rho_true, rho_true, 1), nrow = 2)\nchol_decomp &lt;- chol(chol_R) # Cholesky decomposition of the correlation matrix\ncorrelated_innovations &lt;- innovations %*% chol_decomp\n\nfor (t in 1:T_obs) {\n    # Generate returns\n    r_A[t] &lt;- sqrt(sigma2_A_t[t]) * correlated_innovations[t, 1]\n    r_B[t] &lt;- sqrt(sigma2_B_t[t]) * correlated_innovations[t, 2]\n\n    # Update conditional variances for next period (if not the last observation)\n    if (t &lt; T_obs) {\n        sigma2_A_t[t+1] &lt;- omega_A_true + alpha_A_true * r_A[t]^2 + beta_A_true * sigma2_A_t[t]\n        sigma2_B_t[t+1] &lt;- omega_B_true + alpha_B_true * r_B[t]^2 + beta_B_true * sigma2_B_t[t]\n    }\n}\n\n# Create the data matrix\nsimulated_mDATA &lt;- cbind(r_A, r_B)\ncolnames(simulated_mDATA) &lt;- c(\"DanskeBank\", \"Orsted\")\nreturn(simulated_mDATA)\n}\n\nmDATA &lt;- genData()\n\nYou would like to model the joint risk of the two stocks. For that you choose to implement a multivariate constant conditional correlation GARCH (CCC-GARCH) model. In particular you assume that, for all periods \\(t=1, \\dots, T\\), the two stock returns \\((r_{A,t}, r_{B,t})\\) follow a bivariate normal distribution conditional on the information from all previous periods (denoted as \\(F_{t-1}\\)), i.e \\[ f(r_{A,t}, r_{B,t} | F_{t-1}) = \\frac{1}{\\sqrt{2\\pi \\det |\\Sigma_t|}} \\exp \\left( -\\frac{1}{2} (r_{A,t} \\ r_{B,t}) \\Sigma_t^{-1} (r_{A,t} \\ r_{B,t})' \\right) \\] \\[ \\Sigma_t = \\begin{pmatrix} \\sigma^2_{A,t} & \\rho \\sigma_{A,t} \\sigma_{B,t} \\\\ \\rho \\sigma_{A,t} \\sigma_{B,t} & \\sigma^2_{B,t} \\end{pmatrix} \\] with conditional variances each following a GARCH process \\[ \\sigma^2_{A,t} = \\omega_A + \\alpha_A r^2_{A,t-1} + \\beta_A \\sigma^2_{A,t-1} \\] \\[ \\sigma^2_{B,t} = \\omega_B + \\alpha_B r^2_{B,t-1} + \\beta_B \\sigma^2_{B,t-1}. \\] Their joint likelihood function is then given by \\[ L(\\omega_A, \\alpha_A, \\beta_A, \\omega_B, \\alpha_B, \\beta_B, \\rho) = \\prod_{t=1}^T f(r_{A,t}, r_{B,t} | F_{t-1}). \\]\n\nWrite an R function that returns the average negative log-likelihood function for the model above for periods \\(t=2,3,\\dots,T\\). It should take a vector of parameters as first input and a matrix of dimension \\([T \\times 2]\\) as second input. It should return a warning if the matrix has a column dimension different from 2. Hint: Initialize both GARCH processes at \\(t=1\\) with \\(\\sigma^2_{A,1} = \\omega_A / (1-\\alpha_A - \\beta_A)\\) and \\(\\sigma^2_{B,1} = \\omega_B / (1-\\alpha_B - \\beta_B)\\). You can use det() and solve() to calculate the determinant and the inverse of a matrix in R.\n\n\nSolution:\n\nfAvgNegLogLikGarch &lt;- function(vParams, mInput) {\n  if (dim(mInput)[2] != 2) {\n    warning(\"Incorrect number of dimensions in input matrix\")\n    return(NULL)\n  }\n  \n  dOmegaA &lt;- vParams[1]\n  dAlphaA &lt;- vParams[2]\n  dBetaA &lt;- vParams[3]\n  dOmegaB &lt;- vParams[4]\n  dAlphaB &lt;- vParams[5]\n  dBetaB &lt;- vParams[6]\n  dRho &lt;- vParams[7]\n  \n  dT &lt;- nrow(mInput)\n  \n  mSigma2 &lt;- matrix(0, nrow(mInput), 2)\n  mSigma2[1, 1] &lt;- dOmegaA / (1 - dAlphaA - dBetaA)\n  mSigma2[1, 2] &lt;- dOmegaB / (1 - dAlphaB - dBetaB)\n  \n  dOut &lt;- 0\n  \n  for (t in 2:dT) {\n    mSigma2[t, 1] &lt;- dOmegaA + dAlphaA * mInput[t - 1, 1]^2 + dBetaA * mSigma2[t - 1, 1]\n    mSigma2[t, 2] &lt;- dOmegaB + dAlphaB * mInput[t - 1, 2]^2 + dBetaB * mSigma2[t - 1, 2]\n    mSigmaCov &lt;- matrix(c(mSigma2[t, 1], rep(dRho * sqrt(mSigma2[t, 1]) * sqrt(mSigma2[t, 2]), 2), mSigma2[t, 2]), 2, 2)\n    dOut &lt;- dOut - log(sqrt(2 * pi * det(mSigmaCov))) - 0.5 * as.numeric(t(mInput[t, ]) %*% solve(mSigmaCov) %*% mInput[t, ])\n  }\n  \n  return(-dOut / (dT - 1))\n}\n\n\nUse the BFGS algorithm with optim() to find the maximizers of the log-likelihood function in 2). Use the following starting values: \\[ \\omega_A = s_A^2 \\times 0.05, \\quad \\alpha_A = 0.05, \\quad \\beta_A = 0.90, \\] \\[ \\omega_B = s_B^2 \\times 0.05, \\quad \\alpha_B = 0.05, \\quad \\beta_B = 0.90, \\] \\[ \\rho = 0 \\] where \\(s_A^2\\) and \\(s_B^2\\) are the sample standard deviations of the return series \\(A\\) and \\(B\\) respectively over all time periods. Did your algorithm converge? If not, briefly describe a potential source of the problem.\n\n\nvParams &lt;- c(sd(mDATA[, 1]) * 0.05, 0.05, 0.9, sd(mDATA[, 2]) * 0.05, 0.05, 0.9, 0)\n\noptim(vParams, fAvgNegLogLikGarch, mInput = mDATA, method = \"BFGS\")\n\n# optim cannot converge if it decides to explore negative values, since we cannot take the square root of a negative number\n\n\nWrite an R function that returns the average negative log-likelihood function for a reparameterized CCC-GARCH with same input types as in 2). The reparameterization should impose the following constraints on the original parameters of the likelihood function: \\[ \\omega_A &gt; 0, \\quad \\alpha_A \\in (0,1), \\quad \\beta_A \\in (0,1), \\] \\[ \\omega_B &gt; 0, \\quad \\alpha_B \\in (0,1), \\quad \\beta_B \\in (0,1), \\] \\[ \\rho \\in (-1,1) \\]\n\n\nSolution:\n\nfAvgNegLogLikGarchReparam &lt;- function(vParams, mInput) {\n  if (dim(mInput)[2] != 2) {\n    warning(\"Incorrect number of dimensions in input matrix\")\n    return(NULL)\n  }\n  \n  dOmegaA &lt;- exp(vParams[1])\n  dAlphaA &lt;- exp(vParams[2]) / (1 + exp(vParams[2]) + exp(vParams[3]))\n  dBetaA &lt;- exp(vParams[3]) / (1 + exp(vParams[2]) + exp(vParams[3]))\n  dOmegaB &lt;- exp(vParams[4])\n  dAlphaB &lt;- exp(vParams[5]) / (1 + exp(vParams[5]) + exp(vParams[6]))\n  dBetaB &lt;- exp(vParams[6]) / (1 + exp(vParams[5]) + exp(vParams[6]))\n  dRho &lt;- -1 + 2 * (exp(vParams[7])) / (1 + exp(vParams[7]))\n\n  dT &lt;- nrow(mInput)\n  \n  mSigma2 &lt;- matrix(0, nrow(mInput), 2)\n  mSigma2[1, 1] &lt;- dOmegaA / (1 - dAlphaA - dBetaA)\n  mSigma2[1, 2] &lt;- dOmegaB / (1 - dAlphaB - dBetaB)\n  \n  dOut &lt;- 0\n  \n  for (t in 2:dT) {\n    mSigma2[t, 1] &lt;- dOmegaA + dAlphaA * mInput[t - 1, 1]^2 + dBetaA * mSigma2[t - 1, 1]\n    mSigma2[t, 2] &lt;- dOmegaB + dAlphaB * mInput[t - 1, 2]^2 + dBetaB * mSigma2[t - 1, 2]\n    mSigmaCov &lt;- matrix(c(mSigma2[t, 1], rep(dRho * sqrt(mSigma2[t, 1]) * sqrt(mSigma2[t, 2]), 2), mSigma2[t, 2]), 2, 2)\n    dOut &lt;- dOut - log(sqrt(2 * pi * det(mSigmaCov))) - 0.5 * as.numeric(t(mInput[t, ]) %*% solve(mSigmaCov) %*% mInput[t, ])\n  }\n  \n  return(-dOut / (dT - 1))\n}\n\n\nUse the BFGS algorithm with optim() to find the maximizers of the reparameterized log-likelihood function. Use starting values that correspond to the starting values of the original parameterization in 3). What are your estimates of the transformed parameters?\n\nSolution:\n\nvParams &lt;- c(sd(mDATA[, 1]) * 0.05, 0.05, 0.9, sd(mDATA[, 2]) * 0.05, 0.05, 0.9, 0)\n# transform\nvParams[1] &lt;- log(vParams[1])\nvParams[2] &lt;- log(vParams[2] / (1 - vParams[2]- vParams[3]))\nvParams[3] &lt;- log(vParams[3] / (1 - vParams[2]- vParams[3]))\nvParams[4] &lt;- log(vParams[4])\nvParams[5] &lt;- log(vParams[5] / (1 - vParams[5]- vParams[6]))\nvParams[6] &lt;- log(vParams[6] / (1 - vParams[5]- vParams[6]))\nvParams[7] &lt;- log(((vParams[7] + 1) / 2.0) / (1 - ((vParams[7] + 1) / 2.0)))\n\noptim_results &lt;- optim(vParams, fAvgNegLogLikGarchReparam, mInput = mDATA, method = \"BFGS\")\noptim_results\n#&gt; $par\n#&gt; [1] -10.9045231   0.3523736   3.0242257 -11.6615536   1.7656300   4.3434653\n#&gt; [7]   1.3882186\n#&gt; \n#&gt; $value\n#&gt; [1] -5.925866\n#&gt; \n#&gt; $counts\n#&gt; function gradient \n#&gt;      100       98 \n#&gt; \n#&gt; $convergence\n#&gt; [1] 0\n#&gt; \n#&gt; $message\n#&gt; NULL\n\nvParams &lt;- optim_results$par\ndOmegaA &lt;- exp(vParams[1])\ndAlphaA &lt;- exp(vParams[2]) / (1 + exp(vParams[2]) + exp(vParams[3]))\ndBetaA &lt;- exp(vParams[3]) / (1 + exp(vParams[2]) + exp(vParams[3]))\ndOmegaB &lt;- exp(vParams[4])\ndAlphaB &lt;- exp(vParams[5]) / (1 + exp(vParams[5]) + exp(vParams[6]))\ndBetaB &lt;- exp(vParams[6]) / (1 + exp(vParams[5]) + exp(vParams[6]))\ndRho &lt;- -1 + 2 * (exp(vParams[7])) / (1 + exp(vParams[7]))\n\nprint(paste0(\"Est. OmegaA: \", dOmegaA, \" vs act.: \", 0.0001))\n#&gt; [1] \"Est. OmegaA: 1.83749335729341e-05 vs act.: 1e-04\"\nprint(paste0(\"Est. AlphaA: \", dAlphaA, \" vs act.: \", 0.08))\n#&gt; [1] \"Est. AlphaA: 0.0618438534243581 vs act.: 0.08\"\nprint(paste0(\"Est. BetaA: \", dBetaA, \" vs act.: \", 0.90))\n#&gt; [1] \"Est. BetaA: 0.894678841062443 vs act.: 0.9\"\nprint(paste0(\"Est. OmegaB: \", dOmegaB, \" vs act.: \", 0.000015))\n#&gt; [1] \"Est. OmegaB: 8.61889557714679e-06 vs act.: 1.5e-05\"\nprint(paste0(\"Est. AlphaB: \", dAlphaB, \" vs act.: \", 0.07))\n#&gt; [1] \"Est. AlphaB: 0.0697365650771858 vs act.: 0.07\"\nprint(paste0(\"Est. BetaB: \", dBetaB, \" vs act.: \", 0.91))\n#&gt; [1] \"Est. BetaB: 0.918332975052663 vs act.: 0.91\"\nprint(paste0(\"Est. Rho: \", dRho, \" vs act.: \", 0.6))\n#&gt; [1] \"Est. Rho: 0.600615406946657 vs act.: 0.6\"\n\n\nWrite a C++ function using Rcpp and RcppArmadillo (optional) that can be loaded with sourceCpp(). It should take a numerical vector input corresponding to the parameters of the reparameterized likelihood function input and return a numerical vector that corresponds to the parameters of the original likelihood function. Apply it to your results in 5). What is your estimate for \\(\\rho\\)?\n\nSolution:\n\n// [[Rcpp::depends(RcppArmadillo)]]\n#include &lt;RcppArmadillo.h&gt;\n\nusing namespace arma;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nvec transToOrig_cpp(vec vInput) {\n  vec vOut = zeros&lt;vec&gt;(vInput.size());\n  vOut[0] = exp(vInput[0]);\n  vOut[1] = exp(vInput[1]) / (1 + exp(vInput[1]) + exp(vInput[2]));\n  vOut[2] = exp(vInput[2]) / (1 + exp(vInput[1]) + exp(vInput[2]));\n  vOut[3] = exp(vInput[3]);\n  vOut[4] = exp(vInput[4]) / (1 + exp(vInput[4]) + exp(vInput[5]));\n  vOut[5] = exp(vInput[5]) / (1 + exp(vInput[4]) + exp(vInput[5]));\n  vOut[6] = -1 + 2 * (exp(vInput[6])) / (1 + exp(vInput[6]));\n  \n  return vOut;\n}\n\n\nsuppressMessages(library(Rcpp))\n#&gt; Warning: pakke 'Rcpp' blev bygget under R version 4.3.3\nsuppressMessages(library(RcppArmadillo))\n#&gt; Warning: pakke 'RcppArmadillo' blev bygget under R version 4.3.3\nsourceCpp(\"exam2021cpp.cpp\")\n\nvNew &lt;- transToOrig_cpp(vParams)\nvNew[7]\n#&gt; [1] 0.6006154\n\n\nCreate an R package that contains the functions from 4.) and 6.) and edit the title description to “This is my exam package”. Export the package as a bundled development version. Remark: If you cannot solve 4.) or 6.) create a package that contains an R and a C++ function with single scalar inputs that always return the number 5.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ordinary exam 2021</span>"
    ]
  },
  {
    "objectID": "exam2021re.html",
    "href": "exam2021re.html",
    "title": "\n21  Retake exam 2021\n",
    "section": "",
    "text": "21.1 Problem 1:\nBegin by setting the seed to 134.\nSolution:\nset.seed(134)\n\n### Auxiliary functions ###\n\ngamma.pdf &lt;- function(x, k, theta) {\n  return(theta^k * x^(k-1) * exp(-theta*x)/gamma(k))\n}\n\nexponential.pdf &lt;- function(x, lambda) {\n  return(lambda * exp(-lambda*x))\n}\n\nExponential.Simulate &lt;- function(lambda, size = 1) {\n  V &lt;- runif(size)\n  return(-1/lambda * log(V))\n}\n\nGamma.Simulate &lt;- function(k, theta, size = 1) {\n  \n  lambda &lt;- theta/k\n  c &lt;- k^k * exp(-k+1) / gamma(k)\n  \n  U &lt;- rep(NA, size)\n  Y &lt;- rep(NA, size)\n  X &lt;- rep(NA, size)\n  Unaccepted &lt;- rep(TRUE, size)\n  \n  while (any(Unaccepted)) {\n    \n    UnacceptedCount &lt;- sum(Unaccepted)\n    \n    U &lt;- runif(UnacceptedCount)\n    Y &lt;- Exponential.Simulate(lambda, UnacceptedCount)\n    \n    Accepted_ThisTime &lt;- Unaccepted[Unaccepted] &\n      ( U &lt;= ( gamma.pdf(Y, k, theta) / exponential.pdf(Y, lambda)/c ) )\n    \n    X[Unaccepted][Accepted_ThisTime] &lt;- Y[Accepted_ThisTime]\n    Unaccepted[Unaccepted] &lt;- !Accepted_ThisTime\n    \n  }\n  \n  return(X)\n  \n}\n\nvW &lt;- Gamma.Simulate(k = 3, theta = 3, size = 10000)\nSolution:\nBoxMuller &lt;- function(size = 1) {\n  \n  U &lt;- runif(size)\n  V &lt;- runif(size)\n  \n  X &lt;- sqrt(-2*log(U)) * cos(2*pi*V)\n  Y &lt;- sqrt(-2*log(U)) * sin(2*pi*V)\n  \n  return(c(X,Y))\n  \n}\n\nvZ &lt;- BoxMuller(10000/2)\nvT &lt;- 1 / sqrt(vW) * vZ\n\nhist(vT, breaks = 200, freq = FALSE,\n     main = \"\",\n     col = \"cornflowerblue\",\n     xlim = c(min(vT), max(vT)))\n\nxticks = seq(min(vT), max(vT), 0.1)\nlines(xticks, dt(xticks, 6), col = \"red\")\nlegend(\"topright\", legend = c(\"Simulated\", \"Theoretical\"), lty = c(1, 1), lwd = c(5, 2), col = c(\"cornflowerblue\", \"red\"))\nSolution:\nf &lt;- function(iN, iA, iB) {\n  vX &lt;- seq(1:iN)\n  vOut &lt;- vX[vX %% iA == 0 | vX %% iB == 0]\n  return(list(\n    numbers = vOut,\n    sumofnumbers = sum(vOut)\n  ))\n}\n\nf(iN = 300, iA = 11, iB = 42)\n#&gt; $numbers\n#&gt;  [1]  11  22  33  42  44  55  66  77  84  88  99 110 121 126 132 143 154 165 168\n#&gt; [20] 176 187 198 209 210 220 231 242 252 253 264 275 286 294 297\n#&gt; \n#&gt; $sumofnumbers\n#&gt; [1] 5334\nSolution:\nf &lt;- function(vX) {\n  bFound &lt;- FALSE\n  dOut &lt;- NULL\n  i &lt;- 1\n  while (bFound == FALSE) {\n    if (sum(i %% vX) == 0) {\n      dOut &lt;- i\n      bFound &lt;- TRUE\n    }\n    i &lt;- i + 1\n  }\n  return(dOut)\n}\n\nf(c(3, 5, 7))\n#&gt; [1] 105\nf(seq(1, 15))\n#&gt; [1] 360360",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Retake exam 2021</span>"
    ]
  },
  {
    "objectID": "exam2021re.html#problem-1",
    "href": "exam2021re.html#problem-1",
    "title": "\n21  Retake exam 2021\n",
    "section": "",
    "text": "Use the Acceptance-Rejection method with an Exp(\\(\\lambda\\)) envelope to simulate \\(T = 10,000\\) draws from a Gamma(\\(k, \\theta\\)) distribution with parameters \\((k, \\theta) = (3,3)\\). You may set \\(\\lambda = \\frac{\\theta}{k}\\) and \\[ c = \\frac{k^{k} e^{-(k-1)}}{\\Gamma(k)} \\] where \\(\\Gamma\\) is the gamma function. The density function of the Gamma distribution is \\[ f(x) = \\frac{\\theta^k}{\\Gamma(k)} x^{k-1} e^{-\\theta x}, \\quad x &gt; 0 \\] while for the Exponential distribution it is \\[ g(y; \\lambda) = \\lambda e^{-\\lambda y}, \\quad y &gt; 0. \\] Use the inversion method to generate the Exponential random variables. The corresponding CDF is \\[ G(y) = 1 - e^{-\\lambda y}. \\] Store your Gamma random variables in a vector vW.\n\n\n\n\nUse the Box-Muller algorithm to simulate a sample of length \\(T = 10,000\\) from a Normal(0,1) distribution. Store your Normal random variables in a vector vZ.\n\n\n\n\nIf \\(Z \\sim \\text{Normal}(0,1)\\) and \\(W \\sim \\text{Gamma}\\left(\\frac{\\nu}{2}, \\frac{1}{2}\\nu\\right)\\) then \\(\\frac{Z}{\\sqrt{W}} \\sim t(\\nu)\\). Use this result and the vectors vZ and vW generated in problems (1) and (2) to create 10,000 random variables from a Student’s \\(t\\) distribution with \\(\\nu\\) degrees of freedom. Store these 10,000 draws in a vector vT. Then, create a histogram of vT and superimpose the theoretical density in red. Set breaks = 200 in your call to hist().\n\n\n\nWrite a function which takes three inputs: iN, iA and iB. The purpose of the function is to extract those integers smaller than or equal to iN that have iA or iB as factors. The function must return a list which contains two elements - a vector with the extracted numbers, and an integer which is the sum of this vector. Set iN = 300, iA = 11 and iB = 42 when you run your function. As an example, for iN = 10, iA = 3 and iB = 5 the extracted numbers are (3, 5, 6, 9, 10) and their sum is 33.\n\n\n\n\nWrite a function which returns the smallest natural number which is evenly divisible by all positive integers contained in a vector vX. Use this function to find the smallest number divisible by vX = seq(1, 15). As an example, the smallest number which is evenly divisible by the vector vX = (3, 5, 7) is 105.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Retake exam 2021</span>"
    ]
  },
  {
    "objectID": "exam2021re.html#problem-2-constrained-optimization-c-and-packaging",
    "href": "exam2021re.html#problem-2-constrained-optimization-c-and-packaging",
    "title": "\n21  Retake exam 2021\n",
    "section": "\n21.2 Problem 2: (Constrained) Optimization, C++, and Packaging",
    "text": "21.2 Problem 2: (Constrained) Optimization, C++, and Packaging\nNote: This problem is best solved in order from 1 - 5.\nIn this problem, you are supposed to work with the mDATA_retake.r file provided with the exam. It contains the daily stock returns for the stocks of Danske Bank from 9th of June 2016 til 9th of March 2021 traded on the Copenhagen Stock Exchange. In the following, we denote \\(r_t\\) as the return of Danske Bank at time period \\(t\\).\n\nLoad the data file mDATA_retake.r into your R workspace using readRDS().\n\nSolution:\n\nset.seed(123)\ngenData &lt;- function() {\n  # --- Generate mDATA.r ---\n  set.seed(123) # for reproducibility\n\n# True parameters for data generation\nT_obs &lt;- 1200 # Number of observations (approx 4.75 years of daily data)\n# GARCH parameters for stock A\nomega_A_true &lt;- 0.000001\nalpha_A_true &lt;- 0.08\nbeta_A_true  &lt;- 0.9\n# GARCH parameters for stock B\nomega_B_true &lt;- 0.000001\nalpha_B_true &lt;- 0.09\nbeta_B_true  &lt;- 0.91\n# Constant conditional correlation\nrho_true     &lt;- 0.6\n\n# Initialize vectors\nr_A &lt;- numeric(T_obs)\nr_B &lt;- numeric(T_obs)\nsigma2_A_t &lt;- numeric(T_obs)\nsigma2_B_t &lt;- numeric(T_obs)\n\n# Initial unconditional variances\nsigma2_A_t[1] &lt;- omega_A_true / (1 - alpha_A_true - beta_A_true)\nsigma2_B_t[1] &lt;- omega_B_true / (1 - alpha_B_true - beta_B_true)\n\n# Simulate GARCH processes and returns\n# Generate correlated normal innovations\ninnovations &lt;- matrix(rnorm(2 * T_obs), ncol = 2)\nchol_R &lt;- matrix(c(1, rho_true, rho_true, 1), nrow = 2)\nchol_decomp &lt;- chol(chol_R) # Cholesky decomposition of the correlation matrix\ncorrelated_innovations &lt;- innovations %*% chol_decomp\n\nfor (t in 1:T_obs) {\n    # Generate returns\n    r_A[t] &lt;- sqrt(sigma2_A_t[t]) * correlated_innovations[t, 1]\n    r_B[t] &lt;- sqrt(sigma2_B_t[t]) * correlated_innovations[t, 2]\n\n    # Update conditional variances for next period (if not the last observation)\n    if (t &lt; T_obs) {\n        sigma2_A_t[t+1] &lt;- omega_A_true + alpha_A_true * r_A[t]^2 + beta_A_true * sigma2_A_t[t]\n        sigma2_B_t[t+1] &lt;- omega_B_true + alpha_B_true * r_B[t]^2 + beta_B_true * sigma2_B_t[t]\n    }\n}\n\n# Create the data matrix\nsimulated_mDATA &lt;- cbind(r_A, r_B)\ncolnames(simulated_mDATA) &lt;- c(\"DanskeBank\", \"Orsted\")\nreturn(simulated_mDATA)\n}\n\nmDATA &lt;- genData()\n\nYou want to model the volatility of the stock. You assume that, for all periods \\(t=1, \\dots, T\\), the stock return \\(r_t\\) are generated by a location scale model \\[ r_t = \\mu + \\varepsilon_t \\sigma_t \\] with conditional variances each following a zero drift GARCH process \\[ \\sigma^2_{t} = \\alpha r^2_{t-1} + \\beta \\sigma^2_{t-1} \\] and \\(\\varepsilon_t\\) following a \\(t\\)-distribution with \\(\\nu\\) degrees of freedom conditional on the information from all previous periods (denoted as \\(F_{t-1}\\)), i.e \\[ f(\\varepsilon_t | F_{t-1}) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu \\pi} \\Gamma\\left(\\frac{\\nu}{2}\\right)} \\left(1 + \\frac{\\varepsilon_t^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}}. \\] The likelihood functions of the returns is then given by \\[ L(\\mu, \\alpha, \\beta, \\nu) = \\prod_{t=1}^T f(r_t | F_{t-1}). \\]\n\nWrite an R function that returns the average negative log-likelihood function of a reparameterized version of the model above for periods \\(t=2,3,\\dots,T\\). It should\n\ntake a vector of parameters as first input and a return series of length \\(T\\) as second input.\nalways fulfill the following constraints \\(\\alpha + \\beta = 1, 0 &lt; \\alpha &lt; 1\\), and \\(\\nu &gt; 2\\). Hint: Initialize the conditional variance processes at \\(t=1\\) with \\(\\sigma_1^2\\) equal to the unconditional variance of your returns. You can use dt() to calculate the density of a t-distributed random variable in R. Or you can use the formula above with gamma() for the \\(\\Gamma\\)-function in R.\n\n\n\n\nSolution:\n\nfAvgNegLogLik &lt;- function(vParams, vInput) {\n  dMu &lt;- vParams[1]\n  dAlpha &lt;- exp(vParams[2]) / (1 + exp(vParams[2]))\n  dBeta &lt;- 1 - dAlpha\n  dNu &lt;- 2 + exp(vParams[4])\n  \n  dSum &lt;- 0\n  dT &lt;- length(vInput)\n  \n  vSigma2 &lt;- numeric(dT)\n  vSigma2[1] &lt;- var(vInput)\n  \n  for (t in 2:dT) {\n    vSigma2[t] &lt;- dAlpha * vInput[t - 1]^2 + dBeta * vSigma2[t-1]\n    dSum &lt;- dSum + log(dt((vInput[t] - dMu) / sqrt(vSigma2[t]), dNu)) - log(sqrt(vSigma2[t]))\n  }\n  \n  return(-dSum / (dT - 1))\n}\n\n\nUse the BFGS algorithm with optim() to find the maximizers of the reparameterized log-likelihood function in 2). Use starting values that correspond to the original likelihood parameters \\[ \\mu = 0, \\alpha = 0.90, \\beta = 0.10, \\nu = 5. \\] Retransform your results. What are the estimates for \\((\\mu, \\alpha, \\beta, \\nu)\\)?\n\n\nvParams &lt;- c(0, log(0.9/(1-0.9)), 1 - log(0.9/(1-0.9)), log(5 - 2))\noptim_results &lt;- optim(vParams, fAvgNegLogLik, vInput = mDATA[, 1], method = \"BFGS\")\noptim_results\n#&gt; $par\n#&gt; [1]  0.0001021925 -2.9909391286 -1.1972245773  6.1836752296\n#&gt; \n#&gt; $value\n#&gt; [1] -3.631868\n#&gt; \n#&gt; $counts\n#&gt; function gradient \n#&gt;       54       24 \n#&gt; \n#&gt; $convergence\n#&gt; [1] 0\n#&gt; \n#&gt; $message\n#&gt; NULL\nvParams &lt;- optim_results$par\n\n# retransformation\ndMu_star &lt;- vParams[1]\ndAlpha_star &lt;- exp(vParams[2]) / (1 + exp(vParams[2]))\ndBeta_star &lt;- 1 - dAlpha_star\ndNu_star &lt;- 2 + exp(vParams[4])\n\nprint(paste0(\"Est. mu: \", dMu_star))\n#&gt; [1] \"Est. mu: 0.000102192473262923\"\nprint(paste0(\"Est. alpha: \", dAlpha_star, \" vs. true: \", 0.08))\n#&gt; [1] \"Est. alpha: 0.0478368957544601 vs. true: 0.08\"\nprint(paste0(\"Est. beta: \", dBeta_star, \" vs. true: \", 0.90))\n#&gt; [1] \"Est. beta: 0.95216310424554 vs. true: 0.9\"\nprint(paste0(\"Est. nu: \", dNu_star))\n#&gt; [1] \"Est. nu: 486.770328670692\"\n\n\nWrite a C++ function using Rcpp and RcppArmadillo that can be loaded via sourceCpp(). It should simulate conditional variances and returns from a zero mean GARCH(1,1) for \\(t=1, \\dots, T\\): \\[ r_t = \\varepsilon_t \\sigma_t \\] \\[ \\sigma_t^2 = \\omega + \\alpha \\varepsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2 \\] with errors \\(\\varepsilon_t\\) coming from an arbitrary distribution function specified by the user. The function should have the following inputs\n\nThe number of periods (default = 1)\nA numerical vector of GARCH parameters \\((\\omega, \\alpha, \\beta)\\)\n\nAn R function that simulates random variables from a specific distribution (such as e.g. rnorm() or rt())\nAn numerical vector of inputs for the optional parameters of input (c) (default = 0)\n\n\n\nThe function should return an arma::List object with two vectors containing i) conditional variances for all time periods and ii) simulated return series. Use your function to simulate 1000 draws from a GARCH(1,1) process with t-distributed errors with 5 degrees of freedom. Plot the volatilities. Hint: If you would like to convert a one-dimensional SEXP object with numerical values in C++ to use it as a double for standard calculations, you can use REAL() in C++. Example:\n\nSEXP a = myFunction(input);\ndouble x = *REAL(a);\ndouble y = 2*x + pow(x,2);\n\n\n// [[Rcpp::depends(RcppArmadillo)]]\n#include &lt;RcppArmadillo.h&gt;\n\nusing namespace arma;\nusing namespace Rcpp;\n\n// Helper function to get a shock\ninline double get_shock(Function f, const vec& vInputs) {\n  SEXP s;\n  int n_extra_args = vInputs.n_elem;\n  \n  if (n_extra_args == 0) { // e.g. f takes only n, like a pre-wrapped user function or rnorm(n) with defaults\n    s = f(1);\n  } else if (n_extra_args == 1) { // e.g. rt(n, df) or rnorm(n, mean)\n    s = f(1, vInputs[0]);\n  } else if (n_extra_args == 2) { // e.g. rnorm(n, mean, sd)\n    s = f(1, vInputs[0], vInputs[1]);\n  } else {\n    // Fallback or error for more arguments, or could extend this if/else\n    Rcpp::stop(\"vInputs has too many elements for this simplified handler. Max 2 supported.\");\n  }\n  if (TYPEOF(s) != REALSXP || Rf_length(s) != 1) {\n    Rcpp::stop(\"The user-supplied function 'f' did not return a single numeric value.\");\n  }\n  return Rcpp::as&lt;double&gt;(s);\n}\n\n\n// [[Rcpp::export]]\nList GarchSim(int iT_in, vec vParams, Function f, vec vInputs) {\n  int iT = iT_in;\n  if (iT &lt;= 0) { iT = 1; } // Simplified default handling\n  \n  double dOmega = vParams[0];\n  double dAlpha = vParams[1];\n  double dBeta  = vParams[2];\n  \n  vec vY(iT, fill::zeros);\n  vec vSigma2(iT, fill::zeros);\n  \n  if (dOmega &lt;= 0 || dAlpha &lt; 0 || dBeta &lt; 0 || (dAlpha + dBeta) &gt;= 1.0) {\n    Rcpp::stop(\"Invalid GARCH parameters.\");\n  }\n  \n  double unconditional_variance = dOmega / (1.0 - dAlpha - dBeta);\n  vSigma2(0) = unconditional_variance;\n  \n  double current_eps = get_shock(f, vInputs); // Call helper\n  vY(0) = sqrt(vSigma2(0)) * current_eps;\n  \n  for (int t = 1; t &lt; iT; t++) {\n    vSigma2(t) = dOmega + dAlpha * pow(current_eps, 2) + dBeta * vSigma2(t-1);\n    if (vSigma2(t) &lt;= 0) vSigma2(t) = unconditional_variance; \n    \n    current_eps = get_shock(f, vInputs); // Call helper\n    vY(t) = sqrt(vSigma2(t)) * current_eps;\n  }\n  \n  return List::create(_[\"vSigma2\"] = vSigma2, _[\"vY\"] = vY);\n}\n\n\nsuppressMessages(library(Rcpp))\n#&gt; Warning: pakke 'Rcpp' blev bygget under R version 4.3.3\nsuppressMessages(library(RcppArmadillo))\n#&gt; Warning: pakke 'RcppArmadillo' blev bygget under R version 4.3.3\n\nsourceCpp(\"exam2021recpp.cpp\")\n\niT &lt;- 1000\ndOmega &lt;- 0.1\ndAlpha &lt;- 0.05\ndBeta &lt;- 0.9\ndf_t &lt;- 5\n\nlSim &lt;- GarchSim(iT, c(dOmega, dAlpha, dBeta), f = rt, vInputs = df_t)\n\nplot(1:1000, lSim[[\"vSigma2\"]], type = \"l\", lty = 1, xlab = \"Time\")\nlines(1:1000, lSim[[\"vY\"]],col=\"green\")\n\n\n\n\n\n\n\n\nCreate an R package that contains the functions from 1.) and 4.) and edit the title description to “This is my exam package”. Export the package as a bundled development version. Remark: If you cannot solve 1.) or 4.) create a package that contains an R and a C++ function with single scalar inputs that always return the number 5.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Retake exam 2021</span>"
    ]
  },
  {
    "objectID": "exam2022.html",
    "href": "exam2022.html",
    "title": "\n22  Ordinary exam 2022\n",
    "section": "",
    "text": "22.1 Problem 1:\nBegin by setting the seed to 123.\nSolution:\n### Auxiliary functions ###\n\ngamma.pdf &lt;- function(x, k, theta) {\n  return(theta^k * x^(k-1) * exp(-theta*x)/gamma(k))\n}\n\nexponential.pdf &lt;- function(x, lambda) {\n  return(lambda * exp(-lambda*x))\n}\n\nExponential.Simulate &lt;- function(lambda, size = 1) {\n  V &lt;- runif(size)\n  return(-1/lambda * log(V))\n}\n\nGamma.Simulate &lt;- function(k, theta, size = 1) {\n  \n  lambda &lt;- theta/k\n  c &lt;- k^k * exp(-k+1) / gamma(k)\n  \n  U &lt;- rep(NA, size)\n  Y &lt;- rep(NA, size)\n  X &lt;- rep(NA, size)\n  Unaccepted &lt;- rep(TRUE, size)\n  \n  while (any(Unaccepted)) {\n    \n    UnacceptedCount &lt;- sum(Unaccepted)\n    \n    U &lt;- runif(UnacceptedCount)\n    Y &lt;- Exponential.Simulate(lambda, UnacceptedCount)\n    \n    Accepted_ThisTime &lt;- Unaccepted[Unaccepted] &\n      ( U &lt;= ( gamma.pdf(Y, k, theta) / exponential.pdf(Y, lambda)/c ) )\n    \n    X[Unaccepted][Accepted_ThisTime] &lt;- Y[Accepted_ThisTime]\n    Unaccepted[Unaccepted] &lt;- !Accepted_ThisTime\n    \n  }\n  \n  return(X)\n  \n}\n\nset.seed(123)\n\niN &lt;- 100\niT &lt;- 1000\nvW1 &lt;- matrix(Gamma.Simulate(3, 3, iN * iT), iT, iN)\nvW2 &lt;- matrix(Gamma.Simulate(1, 3, iN * iT), iT, iN)\nSolution:\nvB &lt;- matrix(vW1 / (vW1 + vW2), iT, iN)\nfn_estimateMean &lt;- function(mInput) {\n  mOut &lt;- apply(mInput, 2, mean)\n  return(mOut)\n}\n\ndU &lt;- 3 / (1 + 3)\nvDiffMu &lt;- fn_estimateMean(vB) - dU\nhist(vDiffMu, breaks = 25, freq = FALSE,\n     main = \"Distribution of error\",\n     col = \"cornflowerblue\",\n     xlim = c(min(vDiffMu), max(vDiffMu)),\n     xlab = \"Estimates of error\")\n\nxticks = seq(min(vDiffMu), max(vDiffMu), 0.001)\nlines(xticks, dnorm(xticks, mean(vDiffMu), sd(vDiffMu)), col = \"red\")\nlegend(\"topright\", legend = c(\"Simulated\", \"Theoretical\"), lty = c(1, 1), lwd = c(5, 1), col = c(\"cornflowerblue\", \"red\"))",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Ordinary exam 2022</span>"
    ]
  },
  {
    "objectID": "exam2022.html#problem-1",
    "href": "exam2022.html#problem-1",
    "title": "\n22  Ordinary exam 2022\n",
    "section": "",
    "text": "Use the Acceptance-Rejection method with envelopes Exp(\\(\\lambda_1\\)) and Exp(\\(\\lambda_2\\)) to simulate data from two gamma distributions Gamma(\\(k_1, \\theta\\)) with parameters \\((k_1, \\theta) = (3,3)\\) and Gamma(\\(k_2, \\theta\\)) with parameters \\((k_2, \\theta) = (1,3)\\). You may set \\(\\lambda_1 = \\frac{\\theta}{k_1}\\), \\(\\lambda_2 = \\frac{\\theta}{k_2}\\) and \\[ c_1 = \\frac{k_1^{k_1} e^{-(k_1-1)}}{\\Gamma(k_1)} \\] \\[ c_2 = \\frac{k_2^{k_2} e^{-(k_2-1)}}{\\Gamma(k_2)} \\] where \\(\\Gamma\\) is the gamma function. We wish to simulate \\(N=100\\) samples each of length \\(T=1000\\). The density function of the Gamma distribution is \\[ f(x) = \\frac{\\theta^{k_i}}{\\Gamma(k_i)} x^{k_i-1} e^{-\\theta x}, \\quad x &gt; 0, \\quad i=1,2 \\] while for the Exponential distribution it is \\[ g(y; \\lambda) = \\lambda_i e^{-\\lambda_i y}, \\quad y &gt; 0, \\quad i=1,2. \\] Use the inversion method to generate the Exponential random variables. The corresponding CDF is \\[ G(y) = 1 - e^{-\\lambda_i y}, \\quad i=1,2. \\] Store your Gamma random variables in vectors vW1 and vW2 of size \\(T \\times N\\) each.\n\n\n\n\nNote that \\(B(k_1, k_2) = \\frac{\\text{Gamma}(k_1, \\theta)}{\\text{Gamma}(k_1, \\theta) + \\text{Gamma}(k_2, \\theta)}\\) follows Beta distribution. Use this result and the vectors vW1 and vW2 generated in the problem (1) to simulate \\(N=100\\) samples each of length \\(T=1000\\) of random variables from Beta distribution with the parameters \\(k_1\\) and \\(k_2\\). Store these draws in a vector vB of size \\(T \\times N\\).\n\n\n\n\nThe mean of the beta distribution is given as \\[ \\mu = \\frac{k_1}{k_1+k_2} \\] Write a function, fn_estimateMean, which provides estimates \\(\\hat{\\mu}\\) for each column in mB by calculating its empirical mean and then subtract \\(\\mu\\) calculated with the formula above, i.e. calculate \\(\\hat{\\mu} - \\mu\\) for each column in mB. Save the resulting 100 estimates in a vector vDiffMu.\n\n\n\nCreate a histogram of the estimates contained in the vector vDiffMu. Let the title of the plot be “Distribution of error” and set the label on the x-axis to “Estimates of error”. Add a density plot to the histogram you have just created. Assume the error \\(\\hat{\\mu} - \\mu\\) is Gaussian and use it to calculate its empirical mean and standard deviation using the calculations contained in the vector vDiffMu. Superimpose the density plot on the histogram with lines() defined at these values of mean and standard deviation.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Ordinary exam 2022</span>"
    ]
  },
  {
    "objectID": "exam2022.html#problem-2-constrained-optimization-c-and-packaging",
    "href": "exam2022.html#problem-2-constrained-optimization-c-and-packaging",
    "title": "\n22  Ordinary exam 2022\n",
    "section": "\n22.2 Problem 2: (Constrained) Optimization, C++, and Packaging",
    "text": "22.2 Problem 2: (Constrained) Optimization, C++, and Packaging\nNote: This problem is best solved in order from 1 - 5.\nIn this problem, you are supposed to work with the vDATA.r file provided with the exam. It contains the daily stock returns for the stocks of Danske Bank from 9 June 2016 to 9 March 2021 traded on the Copenhagen Stock Exchange. In the following, we denote \\(r_t\\) as the return of Danske Bank at time period \\(t\\).\n\nLoad the data file vDATA.r into your R workspace using readRDS().\n\n\n# Ensure rugarch is installed and loaded\n# install.packages(\"rugarch\")\nlibrary(rugarch)\n#&gt; Warning: pakke 'rugarch' blev bygget under R version 4.3.3\n#&gt; Indlæser krævet pakke: parallel\n#&gt; \n#&gt; Vedhæfter pakke: 'rugarch'\n#&gt; Det følgende objekt er maskeret fra 'package:stats':\n#&gt; \n#&gt;     sigma\n\n# --- True parameters for new data generation (MODIFIED FOR PERSISTENCE &lt; 1) ---\nset.seed(456) # Use a different seed\nT_obs_new &lt;- 1500    # Number of observations for the new series\nmu_true_new &lt;- 0.0002 # True mean\nomega_true_new &lt;- 0.000015 # True omega\n\n# Modify alphas so their sum is slightly less than 1\nalpha1_true_gen &lt;- 0.65\nalpha2_true_gen &lt;- 0.349  # Sum = 0.65 + 0.349 = 0.999\n\ncat(\"Generating data with persistence:\", alpha1_true_gen + alpha2_true_gen, \"\\n\")\n#&gt; Generating data with persistence: 0.999\n\n# --- Simulate data using rugarch ---\nspec_gen &lt;- ugarchspec(\n  variance.model = list(model = \"sGARCH\", garchOrder = c(2, 0)), # ARCH(2)\n  mean.model = list(armaOrder = c(0, 0), include.mean = TRUE, archm = FALSE, arfima = FALSE), # Ensure no other variance affecting terms\n  distribution.model = \"norm\",\n  fixed.pars = list(mu = mu_true_new,\n                    omega = omega_true_new,\n                    alpha1 = alpha1_true_gen, # Use modified alpha\n                    alpha2 = alpha2_true_gen) # Use modified alpha\n)\n\n# Simulate the path\n# n.start is for burn-in; rugarchpath typically handles this well.\n# For explicit burn-in to remove: n.sim = T_obs_new + 200, then trim first 200.\n# Or use n.start within ugarchpath if it directly supports it for the path output.\n# Simpler: simulate longer and trim.\npath_sim_obj &lt;- ugarchpath(spec_gen, n.sim = T_obs_new + 200, n.start = 0, m.sim = 1)\nvR_new &lt;- as.numeric(fitted(path_sim_obj)[-(1:200)]) # Get returns, remove first 200 as burn-in\n\ncat(\"Generated vR_new with length:\", length(vR_new), \"\\n\")\n#&gt; Generated vR_new with length: 1500\n# plot(vR_new, type=\"l\", main=\"Simulated ARCH(2) Returns (Persistence &lt; 1)\")\n# acf(vR_new^2, main=\"ACF of Squared Simulated Returns\")\n\nYou want to model the volatility of the stock. You assume that, for all periods \\(t=1, \\dots, T\\), the stock return \\(r_t\\) is generated by a location scale model \\[ r_t = \\mu + \\varepsilon_t \\sigma_t \\] with conditional variances following an ARCH(p) process \\[ \\sigma_t^2 = \\omega + \\sum_{j=1}^p \\alpha_j r_{t-j}^2 \\] Assume \\(\\varepsilon_t\\) has a standard normal distribution conditional on the information from all previous periods. Thus, the likelihood function of the returns \\(r_t\\) is given by \\[ L(\\mu, \\omega, \\alpha_1, \\alpha_2, \\dots, \\alpha_p) = \\prod_{t=1}^T \\left( \\frac{1}{\\sigma_t} \\phi\\left(\\frac{r_t - \\mu}{\\sigma_t}\\right) \\right). \\] where \\(\\phi(\\cdot)\\) denotes the density of the standard normal distribution.\n\nWrite an R function that returns the average negative log-likelihood function of a reparameterized version of the model above for periods \\(t=p+1, p+2, \\dots, T\\) for a flexible p. It should have the following inputs:\n\nA vector of parameters \\((\\mu, \\omega, \\alpha_1, \\alpha_2, \\dots, \\alpha_p)\\),\na return series of length \\(T\\),\nthe order \\(p\\) of the ARCH process with default \\(p=2\\), and fulfill the following constraints:\n\n\n\\(\\omega &gt; 0\\)\n\n\\(\\sum_{j=1}^p \\alpha_j = 1\\) Hint: You may initialize the conditional variance process at \\(t=1,2,\\dots,p\\) with \\(\\sigma_t^2\\) equal to the unconditional variance of your returns. You can use dnorm() to calculate the density of a normally distributed random variable in R.\n\n\n\n\nSolution:\n\nfAvgNegLogLik &lt;- function(vParams, vR, p = 2) {\n  dMu &lt;- vParams[1]\n  dOmega &lt;- exp(vParams[2])\n  vParams[3:length(vParams)] &lt;- vParams[3:length(vParams)] / sum(vParams[3:length(vParams)])\n\n  iT &lt;- length(vR)\n  vSigma2 &lt;- numeric(iT)\n  for (j in 1:p) {\n    vSigma2[j] &lt;- dOmega / (1 - sum(vParams[3:length(vParams)]))\n  }\n  \n  dSum &lt;- 0\n  \n  for (t in (p + 1):iT) {\n    vSigma2[t] &lt;- dOmega\n    \n    for (j in 1:p) {\n      vSigma2[t] &lt;- vSigma2[t] + vParams[j + 2] * vR[t - j]^2 \n    }\n    \n    dSum &lt;- dSum - log(sqrt(vSigma2[t])) + log(dnorm((vR[t] - dMu)/sqrt(vSigma2[t])))\n  }\n  \n  return(-dSum / (iT - p))\n}\n\n\nSet \\(p=2\\). Use the BFGS algorithm with optim() to find the maximizers of the reparameterized log-likelihood function in 2). Use starting values that correspond to the original likelihood parameters \\[ \\mu=0, \\omega = \\frac{1}{T}\\sum_{t=1}^T r_t^2, \\alpha_1 = 0.7, \\alpha_2 = 0.3. \\] Retransform your results. What are the estimates for \\((\\mu, \\omega, \\alpha_1, \\alpha_2)\\)?\n\n\noptim_res &lt;- optim(c(0, log(mean(vR_new^2)), 0.7, 0.3), fAvgNegLogLik, vR = vR_new, method = \"BFGS\")\noptim_res\n#&gt; $par\n#&gt; [1]   0.000669589 -11.162179071   0.691584866   0.332063803\n#&gt; \n#&gt; $value\n#&gt; [1] -3.50962\n#&gt; \n#&gt; $counts\n#&gt; function gradient \n#&gt;       24       10 \n#&gt; \n#&gt; $convergence\n#&gt; [1] 0\n#&gt; \n#&gt; $message\n#&gt; NULL\n\nvParams &lt;- optim_res$par\ndMu_star &lt;- vParams[1]\ndOmega_star &lt;- exp(vParams[2])\nvAlphas_star &lt;- vParams[3:length(vParams)] / sum(vParams[3:length(vParams)])\n\ndMu_star\n#&gt; [1] 0.000669589\ndOmega_star\n#&gt; [1] 1.420127e-05\nvAlphas_star\n#&gt; [1] 0.6756076 0.3243924\n\nspec_fit_rugarch &lt;- ugarchspec(\n  variance.model = list(model = \"sGARCH\", garchOrder = c(2, 0)), # ARCH(2)\n  mean.model = list(armaOrder = c(0, 0), include.mean = TRUE), # Estimate mu\n  distribution.model = \"norm\"\n)\n\n# Fit the model using rugarch\nfit_rugarch &lt;- ugarchfit(spec = spec_fit_rugarch, data = vR_new, solver = \"nloptr\") # or \"nloptr\"\n\nprint(\"--- rugarch Results ---\")\n#&gt; [1] \"--- rugarch Results ---\"\nprint(fit_rugarch)\n#&gt; \n#&gt; *---------------------------------*\n#&gt; *          GARCH Model Fit        *\n#&gt; *---------------------------------*\n#&gt; \n#&gt; Conditional Variance Dynamics    \n#&gt; -----------------------------------\n#&gt; GARCH Model  : sGARCH(2,0)\n#&gt; Mean Model   : ARFIMA(0,0,0)\n#&gt; Distribution : norm \n#&gt; \n#&gt; Optimal Parameters\n#&gt; ------------------------------------\n#&gt;         Estimate  Std. Error  t value Pr(&gt;|t|)\n#&gt; mu      0.000494    0.000017  28.8508        0\n#&gt; omega   0.000014    0.000000 132.2038        0\n#&gt; alpha1  0.670886    0.056970  11.7761        0\n#&gt; alpha2  0.323013    0.040834   7.9105        0\n#&gt; \n#&gt; Robust Standard Errors:\n#&gt;         Estimate  Std. Error  t value Pr(&gt;|t|)\n#&gt; mu      0.000494    0.000010  49.4934        0\n#&gt; omega   0.000014    0.000000 129.1425        0\n#&gt; alpha1  0.670886    0.058066  11.5538        0\n#&gt; alpha2  0.323013    0.037942   8.5133        0\n#&gt; \n#&gt; LogLikelihood : 5262.87 \n#&gt; \n#&gt; Information Criteria\n#&gt; ------------------------------------\n#&gt;                     \n#&gt; Akaike       -7.0118\n#&gt; Bayes        -6.9977\n#&gt; Shibata      -7.0118\n#&gt; Hannan-Quinn -7.0065\n#&gt; \n#&gt; Weighted Ljung-Box Test on Standardized Residuals\n#&gt; ------------------------------------\n#&gt;                         statistic p-value\n#&gt; Lag[1]                     0.5291  0.4670\n#&gt; Lag[2*(p+q)+(p+q)-1][2]    0.5715  0.6610\n#&gt; Lag[4*(p+q)+(p+q)-1][5]    2.5128  0.5032\n#&gt; d.o.f=0\n#&gt; H0 : No serial correlation\n#&gt; \n#&gt; Weighted Ljung-Box Test on Standardized Squared Residuals\n#&gt; ------------------------------------\n#&gt;                         statistic p-value\n#&gt; Lag[1]                  0.0004559  0.9830\n#&gt; Lag[2*(p+q)+(p+q)-1][5] 0.7997152  0.9031\n#&gt; Lag[4*(p+q)+(p+q)-1][9] 1.8692596  0.9200\n#&gt; d.o.f=2\n#&gt; \n#&gt; Weighted ARCH LM Tests\n#&gt; ------------------------------------\n#&gt;             Statistic Shape Scale P-Value\n#&gt; ARCH Lag[3]   0.08255 0.500 2.000  0.7739\n#&gt; ARCH Lag[5]   1.75598 1.440 1.667  0.5277\n#&gt; ARCH Lag[7]   2.36933 2.315 1.543  0.6395\n#&gt; \n#&gt; Nyblom stability test\n#&gt; ------------------------------------\n#&gt; Joint Statistic:  0.4199\n#&gt; Individual Statistics:              \n#&gt; mu     0.08636\n#&gt; omega  0.07282\n#&gt; alpha1 0.14335\n#&gt; alpha2 0.09463\n#&gt; \n#&gt; Asymptotic Critical Values (10% 5% 1%)\n#&gt; Joint Statistic:          1.07 1.24 1.6\n#&gt; Individual Statistic:     0.35 0.47 0.75\n#&gt; \n#&gt; Sign Bias Test\n#&gt; ------------------------------------\n#&gt;                    t-value   prob sig\n#&gt; Sign Bias           0.7563 0.4496    \n#&gt; Negative Sign Bias  0.8591 0.3904    \n#&gt; Positive Sign Bias  0.3725 0.7096    \n#&gt; Joint Effect        2.4834 0.4783    \n#&gt; \n#&gt; \n#&gt; Adjusted Pearson Goodness-of-Fit Test:\n#&gt; ------------------------------------\n#&gt;   group statistic p-value(g-1)\n#&gt; 1    20     11.97       0.8868\n#&gt; 2    30     14.68       0.9874\n#&gt; 3    40     32.64       0.7539\n#&gt; 4    50     39.73       0.8249\n#&gt; \n#&gt; \n#&gt; Elapsed time : 0.325016",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Ordinary exam 2022</span>"
    ]
  },
  {
    "objectID": "exam2022re.html",
    "href": "exam2022re.html",
    "title": "\n23  Retake exam 2022\n",
    "section": "",
    "text": "23.1 Problem 1:\nBegin by setting the seed to 134.\nSolution:\nset.seed(134)\n\nfn_Laplace &lt;- function(dMu, dSigma, iSamples, iLength) {\n  \n  U &lt;- runif(iSamples * iLength)\n  vTemp &lt;- dMu - dSigma * sign(U-0.5) * log(1 - 2 * abs(U - 0.5))\n  return(matrix(vTemp, nrow = iLength, ncol = iSamples))\n  \n}\n\nmX &lt;- fn_Laplace(dMu = 1, dSigma = 2, iSamples = 5000, iLength = 1000)\nSolution:\nfLapDen &lt;- function(dMu, dSigma, x) {\n  return(1 / (2 * dSigma) * exp(-abs(x - dMu) / dSigma))\n}\n\nhist(mX[, 1], \n     freq = FALSE,\n     breaks = 200,\n     col = \"cornflowerblue\", \n     xlab = \"\",\n     ylab = \"Density\",\n     main = \"\",\n     xlim = c(min(mX[, 1]), max(mX[, 1])))\n\nvX &lt;- seq(min(mX[, 1]), max(mX[, 1]), by = 0.05)\nlines(vX, fLapDen(dMu = 1, dSigma = 2, vX), type = \"l\", col = \"red\", lwd = 2)\n\nlegend(\"topright\",\n       legend = c(\"histogram\", \"theoretical\"),\n       col = c(\"cornflowerblue\", \"red\"),\n       lwd = 2)\nSolution:\nfn_estimateLaplace &lt;- function(mInput) {\n  fTempMed &lt;- function(vX) {\n    return(sort(vX, decreasing = FALSE)[ceiling(length(vX) / 2)])\n  }\n  fSigmaCal &lt;- function(vX) {\n    vOut &lt;- ifelse(vX - fTempMed(vX) &lt; 0, (vX - fTempMed(vX)) * -1, vX - fTempMed(vX))\n    return(mean(vOut))\n  }\n  \n  return(list(\n    vMu = apply(mInput, 2, fTempMed),\n    vSigma = apply(mInput, 2, fSigmaCal)\n  ))\n}\n\nvMu &lt;- fn_estimateLaplace(mX)$vMu\nvSigma &lt;- fn_estimateLaplace(mX)$vSigma\nSolution:\nhist(vMu, \n     freq = FALSE,\n     breaks = 50,\n     col = \"cornflowerblue\", \n     xlab = \"Estimates\",\n     ylab = \"\",\n     main = \"Distribution of estimates\",\n     xlim = c(min(vMu), max(vMu)))\n\nvX &lt;- seq(min(vMu), max(vMu), by = 0.05)\nlines(vX, dnorm(vX, mean = mean(vMu), sd = sd(vMu)), type = \"l\", col = \"red\", lwd = 2)\n\nlegend(\"topright\",\n       legend = c(\"histogram\", \"theoretical\"),\n       col = c(\"cornflowerblue\", \"red\"),\n       lwd = 2)\n\n\n\n\n\n\n\nhist(vSigma, \n     freq = FALSE,\n     breaks = 50,\n     col = \"cornflowerblue\", \n     xlab = \"Estimates\",\n     ylab = \"\",\n     main = \"Distribution of estimates\",\n     xlim = c(min(vSigma), max(vSigma)))\n\nvX &lt;- seq(min(vSigma), max(vSigma), by = 0.05)\nlines(vX, dnorm(vX, mean = mean(vSigma), sd = sd(vSigma)), type = \"l\", col = \"red\", lwd = 2)\n\nlegend(\"topright\",\n       legend = c(\"histogram\", \"theoretical\"),\n       col = c(\"cornflowerblue\", \"red\"),\n       lwd = 2)\nSolution:\nfn_estimateVariance &lt;- function(mInput) {\n  return(apply(mInput, 2, var))\n}\n\ndSigma &lt;- 2\nvTemp &lt;- fn_estimateVariance(mX) - 2 * dSigma^2\nmean(vTemp)\n#&gt; [1] 0.01253421\nsd(vTemp)\n#&gt; [1] 0.5653084",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Retake exam 2022</span>"
    ]
  },
  {
    "objectID": "exam2022re.html#problem-1",
    "href": "exam2022re.html#problem-1",
    "title": "\n23  Retake exam 2022\n",
    "section": "",
    "text": "Use the inversion method to simulate data from a Laplace distribution with the location \\(\\mu = 1\\) and the scale \\(\\sigma = 2\\) parameters. The inverse CDF is given as \\[ F^{-1}(U) = \\mu - \\sigma \\text{sign}(U-0.5) \\ln(1-2|U-0.5|) \\] and we wish to simulate \\(N = 5000\\) samples of length \\(T = 1000\\). sign is the sign function built in R. Write a function, fn_Laplace, which simulates and returns the \\(T \\times N\\) matrix mX as output for given values \\(\\mu\\) and \\(\\sigma\\).\n\n\n\n\nThe density function of the Laplace distribution is given as \\[ f(X) = \\frac{1}{2\\sigma} \\exp\\left(-\\frac{|X-\\mu|}{\\sigma}\\right) \\] Pick the first column of the matrix mX and create a histogram and superimpose the theoretical density in red. Set breaks = 200 in your call to hist().\n\n\n\n\nThe maximum likelihood estimates of the parameters are given as \\[ \\hat{\\mu} = \\text{median}(X) \\] \\[ \\hat{\\sigma} = \\frac{1}{T} \\sum_{t=1}^T |X_t - \\hat{\\mu}| \\] Write a function, fn_estimateLaplace, which estimates \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\) for each column in mX. Save the resulting 5000 estimates in vectors vMu and vSigma. However, your function must NOT use the built-in functions abs and median. You must compute the median of the absolute value of each column without using those two functions.\n\n\n\n\nCreate two histograms of the estimates \\(\\hat{\\sigma}\\) and \\(\\hat{\\mu}\\) contained in the vectors vSigma and vMu. Set breaks = 50 and freq = F. Let the title of the plot be “Distribution of estimates” and set the label on the x-axis to “Estimates”. Add a density plot to the histogram you have just created. Assume \\(\\hat{\\sigma}\\) and \\(\\hat{\\mu}\\) are Gaussian and use the empirical mean and standard deviation of the calculations in vSigma and vMu as the parameters. Superimpose the density plot on the histogram with lines().\n\n\n\n\nThe variance of the Laplace distribution is given as \\[ \\text{var}(X) = 2\\sigma^2 \\] Write a function, fn_estimateVariance, which defines \\(\\text{var}(X)\\) for each column mX by calculating its empirical variance. Define \\(\\text{var}(X) = 2\\sigma^2\\) and calculate its mean and standard deviation.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Retake exam 2022</span>"
    ]
  },
  {
    "objectID": "exam2022re.html#problem-2-constrained-optimization-c-and-packaging",
    "href": "exam2022re.html#problem-2-constrained-optimization-c-and-packaging",
    "title": "\n23  Retake exam 2022\n",
    "section": "\n23.2 Problem 2: (Constrained) Optimization, C++, and Packaging",
    "text": "23.2 Problem 2: (Constrained) Optimization, C++, and Packaging\nNote: This problem is best solved in order from 1 - 6.\nIn this problem, you are supposed to work with the vDATA.r file provided with the exam. It contains the daily stock returns for the stocks of Danske Bank from 9 June 2016 to 9 March 2021 traded on the Copenhagen Stock Exchange. In the following, we denote \\(r_t\\) as the return of Danske Bank at time period \\(t\\).\n\nLoad the data file vDATA.r into your R workspace using readRDS(). You want to model the volatility of the stock. You assume that, for all periods \\(t=1, \\dots, T\\), the stock return \\(r_t\\) is generated by a simple scale model \\[ r_t = \\varepsilon_t \\sigma_t \\] with conditional variances each following a zero drift GARCH(2,1) process \\[ \\sigma_t^2 = \\alpha_1 r_{t-1}^2 + \\alpha_2 r_{t-2}^2 + \\beta \\sigma_{t-1}^2 \\] and independent \\(\\varepsilon_t\\) following some distribution conditional on the information from all previous periods (denoted as \\(F_{t-1}\\)) according to density function \\(f(\\cdot|F_{t-1}; \\theta)\\). \\(\\theta\\) denotes the parameters of the distribution (to be specified later). The likelihood function of the returns is then given by \\[ L(\\mu, \\alpha, \\beta, \\theta) = \\prod_{t=1}^T \\frac{1}{\\sigma_t} f(r_t/\\sigma_t | F_{t-1}; \\theta) \\]\n\n\nSolution:\n\ngenerate_sample_garch_data &lt;- function(num_observations = 1200,\n                                       burn_in_period = 200,\n                                       alpha1_sim = 0.10,\n                                       alpha2_sim = 0.10,\n                                       beta_sim = 0.80,\n                                       nu_sim = 5,\n                                       initial_sigma2 = (0.01)^2,\n                                       seed = 42) {\n  if (!is.null(seed)) {\n    set.seed(seed)\n  }\n\n  total_length = num_observations + burn_in_period\n\n  # --- Parameter Validation ---\n  if (alpha1_sim &lt; 0 || alpha2_sim &lt; 0 || beta_sim &lt; 0) {\n    stop(\"GARCH parameters (alpha1, alpha2, beta) must be non-negative.\")\n  }\n  # The problem later constrains alpha1+alpha2+beta=1.\n  # For simulation, stationarity usually requires alpha1+alpha2+beta &lt; 1 if an intercept omega &gt; 0 exists.\n  # Here, omega=0. If alpha1+alpha2+beta &gt;= 1, it's an IGARCH or explosive process.\n  # The chosen defaults sum to 1 (IGARCH), which is common for financial data.\n\n  if (nu_sim &lt;= 2) {\n    stop(\"Degrees of freedom 'nu_sim' for t-distribution must be &gt; 2 for finite variance.\")\n  }\n  if (initial_sigma2 &lt;= 0) {\n    stop(\"Initial variance 'initial_sigma2' must be positive.\")\n  }\n\n  # --- Initialize Vectors ---\n  r_t &lt;- numeric(total_length)      # Vector for returns\n  sigma2_t &lt;- numeric(total_length) # Vector for conditional variances\n\n  # --- Generate Standardized t-distributed Innovations (mean 0, variance 1) ---\n  # A standard t-distribution with nu degrees of freedom has variance nu / (nu - 2).\n  innovations &lt;- rt(total_length, df = nu_sim) * sqrt((nu_sim - 2) / nu_sim)\n\n  # --- Set Initial Values ---\n  # For GARCH(2,1): sigma_t^2 = alpha1*r_{t-1}^2 + alpha2*r_{t-2}^2 + beta*sigma_{t-1}^2\n  # We need to bootstrap the process for the first few periods.\n  \n  # Period 1:\n  sigma2_t[1] &lt;- initial_sigma2\n  r_t[1] &lt;- innovations[1] * sqrt(sigma2_t[1])\n\n  # Period 2:\n  # sigma_2^2 depends on r_1^2, r_0^2, sigma_1^2.\n  # Assume r_0 = 0 (or its squared value is negligible or absorbed into an effective initial sigma).\n  # So, sigma_2^2 = alpha1*r_1^2 + beta*sigma_1^2 (alpha2*r_0^2 term is zero)\n  sigma2_t[2] &lt;- alpha1_sim * r_t[1]^2 + beta_sim * sigma2_t[1]\n  if (sigma2_t[2] &lt;= 1e-9) sigma2_t[2] &lt;- 1e-9 # Floor variance\n  r_t[2] &lt;- innovations[2] * sqrt(sigma2_t[2])\n\n  # --- Simulate GARCH(2,1) Process ---\n  for (t in 3:total_length) {\n    sigma2_t[t] &lt;- alpha1_sim * r_t[t-1]^2 +\n                   alpha2_sim * r_t[t-2]^2 +\n                   beta_sim * sigma2_t[t-1]^2\n    \n    # Ensure variance remains positive (numerical stability)\n    if (sigma2_t[t] &lt;= 1e-9) {\n      sigma2_t[t] &lt;- 1e-9 # Floor variance to prevent issues like sqrt(negative)\n    }\n    \n    r_t[t] &lt;- innovations[t] * sqrt(sigma2_t[t])\n  }\n\n  # --- Discard Burn-in Period ---\n  sample_returns &lt;- r_t[(burn_in_period + 1):total_length]\n  \n  # Final check for non-finite values (e.g. if sigma2_t became 0 or negative despite floor)\n  if(any(!is.finite(sample_returns))){\n      warning(\"Non-finite values were generated in sample_returns. This might indicate issues with parameters or initialization. Non-finite values replaced with 0.\")\n      sample_returns[!is.finite(sample_returns)] = 0 \n  }\n\n  return(sample_returns)\n}\n\nvDATA &lt;- generate_sample_garch_data(\n  num_observations = 1197,\n  alpha1_sim = 0.10,\n  alpha2_sim = 0.10,\n  beta_sim = 0.80, # Note: alpha1+alpha2+beta = 1.0 (IGARCH process)\n  nu_sim = 5,\n  seed = 123 # For reproducibility\n)\n\n\nWrite an R function that returns the average negative log-likelihood function of the model above for periods \\(t=2,3,\\dots,T\\) for an arbitrary, user-specified density function \\(f(\\cdot)\\). The R function should have the following inputs:\n\nA vector of parameters collecting all the GARCH parameters \\((\\alpha_1, \\alpha_2, \\beta)\\) and the conditional density parameters \\(\\theta\\),\na time-series of length \\(T\\),\na flexible function object for the density \\(f(\\cdot)\\) with default rnorm. Hint: Initialize the conditional variance process at \\(t=1\\) with \\(\\sigma_1^2\\) equal to the unconditional variance of your returns.\n\n\n\n\nSolution:\n\nfAvgNegLogLik &lt;- function(vParams, vR, f = rnorm) {\n  dAlpha1 &lt;- vParams[1]\n  dAlpha2 &lt;- vParams[2]\n  dBeta &lt;- vParams[3]\n  vDensParams &lt;- vParams[4:length(vParams)]\n  \n  dT &lt;- length(vR)\n  dSum &lt;- 0\n  vSigma2 &lt;- numeric(dT)\n  \n  vSigma2[1] &lt;- var(vR)\n  vSigma2[2] &lt;- dAlpha1 * vR[1]^2 + dBeta * vSigma2[1]\n  \n  for (t in 3:dT) {\n    vSigma2[t] &lt;- dAlpha1 * vR[t-1]^2 + dAlpha2 * vR[t-2]^2 + dBeta * vSigma2[t-1]\n    dSum &lt;- dSum - log(sqrt(vSigma2[t])) + log(do.call(f, as.list(c(vR[t]/sqrt(vSigma2[t]), vDensParams))))\n  }\n  \n  return(-dSum / (dT - 1))\n}\n\n\nWrite another R function that uses the function in (2.) to return the average negative log-likelihood function of a reparameterized version of the model above for periods \\(t=2,3,\\dots,T\\) assuming a \\(t\\)-distribution with \\(\\nu\\) degrees of freedom, i.e \\[ f(\\varepsilon_t | F_{t-1}) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu \\pi} \\Gamma\\left(\\frac{\\nu}{2}\\right)} \\left(1 + \\frac{\\varepsilon_t^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}} \\] It should have the following inputs:\n\nA vector of parameters collecting all the GARCH parameters and the conditional density parameters,\na time-series of length \\(T\\), and fulfill the following constraints\n\n\n\\(0 &lt; \\alpha_1 &lt; 1\\)\n\\(0 &lt; \\alpha_2 &lt; 1\\)\n\\(0 &lt; \\beta &lt; 1\\)\n\n\\(\\alpha_1 + \\alpha_2 + \\beta = 1\\),\n\n\\(\\nu &gt; 2\\). Note: You do not need to write a new likelihood function. Use 2. Hint: You can use dt() to calculate the density of a t-distributed random variable in R. Or you can use the formula above with gamma() for the \\(\\Gamma\\)-function in R.\n\n\n\n\nSolution:\n\nfAvgNegLogLikRep &lt;- function(vParams, vR) {\n  dAlpha1 &lt;- exp(vParams[1]) / (1 + exp(vParams[1]) + exp(vParams[2]))\n  dAlpha2 &lt;- exp(vParams[2]) / (1 + exp(vParams[1]) + exp(vParams[2]))\n  dBeta &lt;- 1 - dAlpha1 - dAlpha2\n  dNu &lt;- 2 + exp(vParams[4])\n  \n  vParams &lt;- c(dAlpha1, dAlpha2, dBeta, dNu)\n  return(fAvgNegLogLik(vParams, vR, dt))\n}\n\n\nUse the BFGS algorithm with optim() to find the maximizers of the reparameterized log-likelihood function in 3). Use starting values that correspond to the original likelihood parameters \\[ \\alpha_1 = 0.10, \\alpha_2 = 0.10, \\beta = 0.80, \\nu = 5. \\] Retransform your results. What are the estimates for \\((\\alpha_1, \\alpha_2, \\beta, \\nu)\\)?\n\nSolution:\n\nvParams &lt;- c(0.1, 0.1, 0.8, 5)\n# transform\ndAlpha1 &lt;- log(vParams[1] / (1 - vParams[1] - vParams[2]))\ndAlpha2 &lt;- log(vParams[2] / (1 - vParams[1] - vParams[2]))\ndBeta &lt;- vParams[3] # constraint ensured by functions\ndNu &lt;- log(vParams[4] - 2)\nvParams &lt;- c(dAlpha1, dAlpha2, dBeta, dNu)\n\noptim_results &lt;- optim(vParams, fAvgNegLogLikRep, vR = vDATA, method = \"BFGS\")\noptim_results\n#&gt; $par\n#&gt; [1] -9.187285 -9.375245  0.800000  2.441869\n#&gt; \n#&gt; $value\n#&gt; [1] -8.964638\n#&gt; \n#&gt; $counts\n#&gt; function gradient \n#&gt;       38       33 \n#&gt; \n#&gt; $convergence\n#&gt; [1] 0\n#&gt; \n#&gt; $message\n#&gt; NULL\n\nvParams &lt;- optim_results$par\ndAlpha1 &lt;- exp(vParams[1]) / (1 + exp(vParams[1]) + exp(vParams[2]))\ndAlpha2 &lt;- exp(vParams[2]) / (1 + exp(vParams[1]) + exp(vParams[2]))\ndBeta &lt;- 1 - dAlpha1 - dAlpha2\ndNu &lt;- 2 + exp(vParams[4])\n\nprint(paste0(\"Est. Alpha1: \", dAlpha1, \" vs act.: \", 0.1))\n#&gt; [1] \"Est. Alpha1: 0.00010231320414325 vs act.: 0.1\"\nprint(paste0(\"Est. Alpha2: \", dAlpha2, \" vs act.: \", 0.1))\n#&gt; [1] \"Est. Alpha2: 8.47815778203704e-05 vs act.: 0.1\"\nprint(paste0(\"Est. Beta: \", dBeta, \" vs act.: \", 0.8))\n#&gt; [1] \"Est. Beta: 0.999812905218036 vs act.: 0.8\"\nprint(paste0(\"Est. Nu: \", dNu, \" vs act.: \", 5))\n#&gt; [1] \"Est. Nu: 13.4945084264308 vs act.: 5\"\n\nSanity check:\n\nsuppressMessages(library(rugarch))\n#&gt; Warning: pakke 'rugarch' blev bygget under R version 4.3.3\n\nspec_fit_rugarch &lt;- ugarchspec(\n  variance.model = list(model = \"sGARCH\", garchOrder = c(2, 1)), # ARCH(2)\n  mean.model = list(armaOrder = c(0, 0), include.mean = FALSE), # Estimate mu\n  distribution.model = \"std\"\n)\n\n# Fit the model using rugarch\nfit_rugarch &lt;- ugarchfit(spec = spec_fit_rugarch, data = vDATA, solver = \"nloptr\") # or \"nloptr\"\n\nprint(\"--- rugarch Results ---\")\n#&gt; [1] \"--- rugarch Results ---\"\nprint(fit_rugarch)\n#&gt; \n#&gt; *---------------------------------*\n#&gt; *          GARCH Model Fit        *\n#&gt; *---------------------------------*\n#&gt; \n#&gt; Conditional Variance Dynamics    \n#&gt; -----------------------------------\n#&gt; GARCH Model  : sGARCH(2,1)\n#&gt; Mean Model   : ARFIMA(0,0,0)\n#&gt; Distribution : std \n#&gt; \n#&gt; Optimal Parameters\n#&gt; ------------------------------------\n#&gt;         Estimate  Std. Error    t value Pr(&gt;|t|)\n#&gt; omega   0.000000    0.000000   0.048587  0.96125\n#&gt; alpha1  0.000071    0.022045   0.003227  0.99743\n#&gt; alpha2  0.000256    0.022012   0.011636  0.99072\n#&gt; beta1   0.969460    0.002236 433.632645  0.00000\n#&gt; shape   4.974784    0.730757   6.807711  0.00000\n#&gt; \n#&gt; Robust Standard Errors:\n#&gt;         Estimate  Std. Error  t value Pr(&gt;|t|)\n#&gt; omega   0.000000    0.000000 0.000812 0.999352\n#&gt; alpha1  0.000071    0.051016 0.001394 0.998887\n#&gt; alpha2  0.000256    0.294820 0.000869 0.999307\n#&gt; beta1   0.969460    0.307556 3.152144 0.001621\n#&gt; shape   4.974784    8.785449 0.566253 0.571222\n#&gt; \n#&gt; LogLikelihood : 10763.72 \n#&gt; \n#&gt; Information Criteria\n#&gt; ------------------------------------\n#&gt;                     \n#&gt; Akaike       -17.976\n#&gt; Bayes        -17.955\n#&gt; Shibata      -17.976\n#&gt; Hannan-Quinn -17.968\n#&gt; \n#&gt; Weighted Ljung-Box Test on Standardized Residuals\n#&gt; ------------------------------------\n#&gt;                         statistic p-value\n#&gt; Lag[1]                     0.1481  0.7004\n#&gt; Lag[2*(p+q)+(p+q)-1][2]    0.9820  0.5041\n#&gt; Lag[4*(p+q)+(p+q)-1][5]    2.1393  0.5859\n#&gt; d.o.f=0\n#&gt; H0 : No serial correlation\n#&gt; \n#&gt; Weighted Ljung-Box Test on Standardized Squared Residuals\n#&gt; ------------------------------------\n#&gt;                          statistic p-value\n#&gt; Lag[1]                    0.005858  0.9390\n#&gt; Lag[2*(p+q)+(p+q)-1][8]   0.966063  0.9763\n#&gt; Lag[4*(p+q)+(p+q)-1][14]  3.722254  0.9052\n#&gt; d.o.f=3\n#&gt; \n#&gt; Weighted ARCH LM Tests\n#&gt; ------------------------------------\n#&gt;             Statistic Shape Scale P-Value\n#&gt; ARCH Lag[4]    0.5723 0.500 2.000  0.4494\n#&gt; ARCH Lag[6]    0.7583 1.461 1.711  0.8178\n#&gt; ARCH Lag[8]    0.8049 2.368 1.583  0.9508\n#&gt; \n#&gt; Nyblom stability test\n#&gt; ------------------------------------\n#&gt; Joint Statistic:  280.0863\n#&gt; Individual Statistics:               \n#&gt; omega  77.26025\n#&gt; alpha1  0.27309\n#&gt; alpha2  0.22811\n#&gt; beta1   0.16494\n#&gt; shape   0.03994\n#&gt; \n#&gt; Asymptotic Critical Values (10% 5% 1%)\n#&gt; Joint Statistic:          1.28 1.47 1.88\n#&gt; Individual Statistic:     0.35 0.47 0.75\n#&gt; \n#&gt; Sign Bias Test\n#&gt; ------------------------------------\n#&gt;                    t-value   prob sig\n#&gt; Sign Bias           0.3901 0.6965    \n#&gt; Negative Sign Bias  0.1214 0.9034    \n#&gt; Positive Sign Bias  0.2282 0.8195    \n#&gt; Joint Effect        0.5120 0.9163    \n#&gt; \n#&gt; \n#&gt; Adjusted Pearson Goodness-of-Fit Test:\n#&gt; ------------------------------------\n#&gt;   group statistic p-value(g-1)\n#&gt; 1    20     12.41       0.8675\n#&gt; 2    30     24.38       0.7101\n#&gt; 3    40     28.03       0.9041\n#&gt; 4    50     39.97       0.8178\n#&gt; \n#&gt; \n#&gt; Elapsed time : 0.886915\n\n\nWrite a C++ function named “CppGfunc” with two inputs i) a numerical vector \\(x\\) and an arbitrary R function \\(g(\\cdot)\\) with no default. It should return \\(g(x)+2\\).\n\nSolution:\n\n// [[Rcpp::depends(RcppArmadillo)]]\n#include &lt;RcppArmadillo.h&gt;\n\nusing namespace arma;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\narma::vec CppGfunc(Function g, arma::vec x) {\n  \n  arma::vec vOut = zeros&lt;vec&gt;(x.size());\n  \n  for (arma::uword i = 0; i &lt; x.size(); i++) {\n    SEXP s;\n    s = g(x[i]);\n    vOut[i] = as&lt;double&gt;(s) + 2.0;\n  }\n  \n  return(vOut);\n}\n\n\nsuppressMessages(library(Rcpp))\n#&gt; Warning: pakke 'Rcpp' blev bygget under R version 4.3.3\nsuppressMessages(library(RcppArmadillo))\n#&gt; Warning: pakke 'RcppArmadillo' blev bygget under R version 4.3.3\n\nsourceCpp(\"exam2022recpp.cpp\")\n\n\nCppGfunc(function(x) x^2, c(1,2,3))\n#&gt;      [,1]\n#&gt; [1,]    3\n#&gt; [2,]    6\n#&gt; [3,]   11\n\n\nCreate an R package that contains the functions from 2.) and 5.) and edit the title description to “This is my exam package”. Export the package as a bundled development version. Remark: If you cannot solve 2.) or 5.) create a package that contains an R and a C++ function with single scalar inputs that always return the number 17.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Retake exam 2022</span>"
    ]
  },
  {
    "objectID": "exam2023.html",
    "href": "exam2023.html",
    "title": "\n24  Ordinary exam 2023\n",
    "section": "",
    "text": "24.1 Problem 1:\nBegin by setting the seed to 123.\nSolution:\nset.seed(123)\n\nGamma.Simulate &lt;- function(iK, dTheta, iSamples, iLength) {\n\n  U &lt;- runif(iSamples * iLength)\n  vOut &lt;- qgamma(U, shape = iK, scale = dTheta)\n  return(matrix(vOut, nrow = iLength, ncol = iSamples))\n  \n}\n\nmG1 &lt;- Gamma.Simulate(iK = 2, dTheta = 1, iSamples = 100, iLength = 1000)\nmG2 &lt;- Gamma.Simulate(iK = 1, dTheta = 2, iSamples = 100, iLength = 1000)\nSolution:\nmL &lt;- mG1 - mG2\nSolution:\nfn_estimateSkewness &lt;- function(mInput) {\n  fn_CalcSkew &lt;- function(vX) {\n    dNum &lt;- mean((vX - mean(vX))^3)\n    dDenom &lt;- mean((vX - mean(vX))^2)^(3/2)\n    return(dNum / dDenom)\n  }\n  return(apply(mInput, 2, fn_CalcSkew))\n}\n\niK1 &lt;- 2\niK2 &lt;- 1\ndTheta1 &lt;- 1\ndTheta2 &lt;- 2\n\nvSkHat &lt;- fn_estimateSkewness(mL)\nvSkAct &lt;- (2 * iK1 * dTheta1^3 - 2 * iK2 * dTheta2^3) / (iK1 * dTheta1^2 + iK2 * dTheta2^2)^(3/2)\n\nvDiffSk &lt;- vSkHat - vSkAct\nSolution:\nhist(vDiffSk, \n     freq = FALSE,\n     breaks = 41,\n     col = \"cornflowerblue\", \n     xlab = \"Estimates of error\",\n     main = \"Distribution of error\",\n     xlim = c(-1.5, 1.5))\n\nvX &lt;- seq(-1.5, 1.5, 0.05)\nlines(vX, dnorm(vX, mean = mean(vDiffSk), sd = sd(vDiffSk)), type = \"l\", col = \"red\", lwd = 2)\n\nlegend(\"topright\",\n       legend = c(\"histogram\", \"density\"),\n       col = c(\"cornflowerblue\", \"red\"),\n       lwd = 2)",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Ordinary exam 2023</span>"
    ]
  },
  {
    "objectID": "exam2023.html#problem-1",
    "href": "exam2023.html#problem-1",
    "title": "\n24  Ordinary exam 2023\n",
    "section": "",
    "text": "Use the inversion method to simulate data from two gamma distributions Gamma(\\(k_1, \\theta_1\\)) and Gamma(\\(k_2, \\theta_2\\)) with the parameters \\((k_1, \\theta_1) = (2,1)\\), and \\((k_2, \\theta_2) = (1,2)\\), where \\(k_1\\) and \\(k_2\\) are shape parameters and \\(\\theta_1\\) and \\(\\theta_2\\) are scale parameters. (Hint: You may use R’s built-in inverse cumulative distribution function (quantile function) qgamma().) We wish to simulate \\(N=100\\) samples each of length \\(T=1000\\). Store your gamma random variables in the matrices mG1 and mG2 each of size \\(T \\times N\\).\n\n\n\n\nDefine a new random variable \\(L(k_1, \\theta_1, k_2, \\theta_2) = \\text{Gamma}(k_1, \\theta_1) - \\text{Gamma}(k_2, \\theta_2)\\). Use this representation and the matrices mG1 and mG2 generated in question 1 to generate \\(N=100\\) samples each of length \\(T=1000\\) of random variables \\(L(k_1, \\theta_1, k_2, \\theta_2)\\). Store these random variables in a vector mL of size \\(T \\times N\\).\n\n\n\n\nThe skewness of the random variable \\(L(k_1, \\theta_1, k_2, \\theta_2)\\) is given by \\[ \\text{sk} = \\frac{2k_1\\theta_1^3 - 2k_2\\theta_2^3}{(k_1\\theta_1^2 + k_2\\theta_2^2)^{3/2}} \\] and the empirical skewness of a vector \\(\\mathbf{X}\\) is given by \\[ \\hat{\\text{sk}} = \\frac{\\frac{1}{T}\\sum_{i=1}^T (X_i - \\bar{X})^3}{\\left[\\frac{1}{T}\\sum_{i=1}^T (X_i - \\bar{X})^2\\right]^{3/2}}, \\quad \\bar{X} = \\frac{1}{T}\\sum_{i=1}^T X_i. \\] Write a function, fn_estimateSkewness, which provides estimates \\(\\hat{\\text{sk}}\\) for each column in mL by calculating its empirical skewness and then subtract \\(\\text{sk}\\) calculated with the formulas above, i.e., calculate \\(\\hat{\\text{sk}} - \\text{sk}\\) for each column in mL. Save the resulting 100 estimates in a vector vDiffSk.\n\n\n\n\nCreate a histogram of the estimates contained in the vector vDiffSk. Let the title of the plot be “Distribution of error” and set the label on the x-axis to “Estimates of error”. Add a density plot to the histogram you have just created. Assume the error \\(\\hat{\\text{sk}} - \\text{sk}\\) is Gaussian and use the empirical mean and standard deviation of the vector vDiffSk as parameters of the density. Superimpose the density plot on the histogram with lines().",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Ordinary exam 2023</span>"
    ]
  },
  {
    "objectID": "exam2023.html#problem-2-constrained-optimization-c-and-packaging",
    "href": "exam2023.html#problem-2-constrained-optimization-c-and-packaging",
    "title": "\n24  Ordinary exam 2023\n",
    "section": "\n24.2 Problem 2: (Constrained) Optimization, C++, and Packaging",
    "text": "24.2 Problem 2: (Constrained) Optimization, C++, and Packaging\nNote: This problem is best solved in order from 1 - 6.\nIn this problem, you will work with the Y.R and X.R files provided with the exam. They contain data which you will use to estimate a Poisson generalized linear model by maximum likelihood. The model is as follows: \\[ y_i \\sim \\text{Pois}(\\lambda_i), \\quad \\lambda_i = \\exp(\\mathbf{x}_i'\\boldsymbol{\\theta}), \\quad i=1,\\dots,500. \\] The file Y.R contains the dependent variables \\(y_i\\) stacked in an integer vector, and X.R contains the regressors stacked in a \\(500 \\times 5\\) matrix.\n\nLoad the data files Y.R and X.R into your R workspace using readRDS().\n\nSolution:\n\nset.seed(123)\ngenData &lt;- function() {\n  n &lt;- 500  # Number of observations\n  p &lt;- 5    # Number of regressors (including intercept)\n\n  # Define the true parameter vector theta.\n  # We set theta_1 &gt; 0 to make the hypothesis test H0: theta_1 &lt;= 0 meaningful.\n  theta_true &lt;- c(0.8, -1.2, 0.5, 0.1, 2.0)\n\n  # Create the regressor matrix X\n  # First column is the intercept\n  X &lt;- matrix(rnorm(n * (p - 1)), nrow = n, ncol = p - 1)\n  X &lt;- cbind(1, X) \n  colnames(X) &lt;- paste0(\"X\", 1:p)\n\n  # Calculate the linear predictor and the mean lambda\n  # eta = X * theta\n  eta &lt;- X %*% theta_true\n  # lambda = exp(eta)\n  lambda &lt;- exp(eta)\n\n  # Generate the dependent variable Y from the Poisson distribution\n  Y &lt;- rpois(n, lambda)\n  return(list(X = X, Y = Y))\n}\n\nX &lt;- genData()$X\nY &lt;- genData()$Y\n\nYou want to find the maximum likelihood estimator of \\(\\boldsymbol{\\theta}\\), using the average log-likelihood and average score functions: \\[ \\ell(\\boldsymbol{\\theta} | \\mathbf{y}, \\mathbf{X}) = \\frac{1}{n} \\sum_{i=1}^n y_i (\\mathbf{x}_i'\\boldsymbol{\\theta}) - \\exp(\\mathbf{x}_i'\\boldsymbol{\\theta}) - \\ln(y_i!), \\] \\[ s(\\boldsymbol{\\theta} | \\mathbf{y}, \\mathbf{X}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\exp(\\mathbf{x}_i'\\boldsymbol{\\theta}))\\mathbf{x}_i. \\]\n\nCreate a C++ script, and write a C++ function called factorial_cpp which computes the factorial of an integer. It should accept a single integer iN as an input, and return iN!. The function should check whether the input is negative; if it is, it should generate an informative error message regarding this issue.\n\nSolution:\n\n// [[Rcpp::depends(RcppArmadillo)]]\n#include &lt;RcppArmadillo.h&gt;\n\nusing namespace arma;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\ndouble factorial_cpp(int iN) {\n  if (iN &lt; 0) {\n    Rcpp::stop(\"Input to factorial must be a non-negative integer.\");\n  }\n  if (iN == 0) {\n    return 1.0;\n  }\n  double result = 1.0;\n  for (int i = 1; i &lt;= iN; ++i) {\n    result *= i;\n  }\n  return result;\n}\n\n\nsuppressMessages(library(Rcpp))\n#&gt; Warning: pakke 'Rcpp' blev bygget under R version 4.3.3\nsuppressMessages(library(RcppArmadillo))\n#&gt; Warning: pakke 'RcppArmadillo' blev bygget under R version 4.3.3\nsourceCpp(\"exam2023cpp.cpp\")\n\n\nWrite two C++ functions fLnL_cpp and fScore_cpp which accept as arguments the vector \\(\\boldsymbol{\\theta}\\), integer vector \\(\\mathbf{y}\\), and matrix \\(\\mathbf{X}\\). They should return \\(\\ell(\\boldsymbol{\\theta} | \\mathbf{y}, \\mathbf{X})\\) and \\(s(\\boldsymbol{\\theta} | \\mathbf{y}, \\mathbf{X})\\) respectively. In your R script, include a sourceCpp() command which compiles the C++ script and adds these functions to your environment. Remark: If you cannot solve question 2, you may skip the \\(-\\ln(y_i!)\\) term in the fLnL_cpp function.\n\n\nSolution:\n\n// [[Rcpp::export]]\ndouble fLnL_cpp(vec vParams, vec vY, mat mX) {\n  double dSum = 0.0;\n  int iN = mX.n_rows;\n  \n  for (int i = 0; i &lt; iN; i++) {\n    //dSum = dSum + vY[i] * as_scalar(mX.row(i) * vParams) - exp(as_scalar(mX.row(i) * vParams)) - log(factorial_cpp(vY[i]));\n    dSum = dSum + vY[i] * as_scalar(mX.row(i) * vParams) - exp(as_scalar(mX.row(i) * vParams)) - lgamma(vY[i] + 1.0);\n  }\n  \n  return(dSum / iN);\n}\n\n// [[Rcpp::export]]\nvec fScore_2_cpp(vec vParams, vec vY, mat mX){\n  vec vP = exp(mX*vParams);\n  vec vScore = trans(mX)*(vY - vP) / vY.n_elem;\n  \n  return vScore;\n}\n\n// [[Rcpp::export]]\nvec fScore_cpp(vec vParams, vec vY, mat mX){\n  \n  int n_obs = mX.n_rows; // Number of observations\n  int p = mX.n_cols;     // Number of parameters\n  \n  vec vOut = zeros&lt;vec&gt;(p);\n  \n  for (int i = 0; i &lt; n_obs; i++) {\n    vOut += (vY[i] - exp(as_scalar(mX.row(i) * vParams))) * mX.row(i).t();\n  }\n  \n  return vOut / n_obs;\n}\n\n\nsuppressMessages(library(Rcpp))\nsuppressMessages(library(RcppArmadillo))\nsourceCpp(\"exam2023cpp.cpp\")\n\n\nMaximize the average log-likelihood with the BFGS algorithm, using the optim() function. You should provide your fScore_cpp function as an input to the gr argument, and you may use a vector of zeroes as the initial guess. Report the maximum likelihood estimator of \\(\\boldsymbol{\\theta}\\) and the average log-likelihood evaluated at that point. Remark: If you cannot fully solve question 3, in this question and all subsequent questions, you can earn partial points by implementing fLnL_cpp in R instead of C++, and using gr=NULL in the optim() function instead of providing the score directly.\n\n\nSolution:\n\noptim_res &lt;- optim(c(0,0,0,0,0), fLnL_cpp, fScore_2_cpp, vY = Y, mX = X, method = \"BFGS\", control=list(fnscale=-1))\noptim_res\n#&gt; $par\n#&gt; [1]  3.630563467  0.126546522 -0.008899956  0.282246449  0.236120415\n#&gt; \n#&gt; $value\n#&gt; [1] -114.4235\n#&gt; \n#&gt; $counts\n#&gt; function gradient \n#&gt;       55       19 \n#&gt; \n#&gt; $convergence\n#&gt; [1] 0\n#&gt; \n#&gt; $message\n#&gt; NULL\n\nYou want to test the following hypothesis about the first element of \\(\\boldsymbol{\\theta}\\): \\[ H_0: \\theta_1 \\le 0 \\quad \\text{vs.} \\quad H_1: \\theta_1 &gt; 0. \\] One way to perform this test is with a likelihood ratio test, using the test statistic \\[ LR = -2 \\left[ \\max_{\\boldsymbol{\\theta} \\in (-\\infty, 0] \\times \\mathbb{R}^4} \\{\\ell(\\boldsymbol{\\theta} | \\mathbf{y}, \\mathbf{X})\\} - \\max_{\\boldsymbol{\\theta} \\in \\mathbb{R}^5} \\{\\ell(\\boldsymbol{\\theta} | \\mathbf{y}, \\mathbf{X})\\} \\right]. \\] The first term is obtained by maximizing the log-likelihood subject to the constraint that \\(\\theta_1 \\le 0\\), and the second term is the unconstrained maximum which you found in question 4.\n\nWrite an R function LR_stat which computes the likelihood ratio test statistic above, and does the following:\n\nThe arguments should be \\(\\mathbf{y}\\), and \\(\\mathbf{X}\\), and vectors of initial guesses for the constrained and unconstrained maximization, with vectors of 0 as default.\nIt should check whether \\(\\mathbf{y}\\) is an integer vector. If not, it should stop and generate an informative error message regarding this issue.\nThe constrained maximization should be solved using the optim() function with argument method=\"L-BFGS-B\" and appropriately chosen arguments lower and upper. You should provide your fScore_cpp function as an input to the gr argument.\nThe unconstrained maximization should be solved as in question 4.\nThe output should be a list containing the likelihood ratio test statistic, the maximum likelihood estimator from the unconstrained maximization, and a message (or two) giving information about the convergence of both maximization problems. Run this function and report its outputs.\n\n\n\nSolution:\n\nLR_stat &lt;- function(vY, mX, vInitGuessesCon = rep(0, ncol(mX)), vInitGuessesUncon = rep(0, ncol(mX))) {\n  if (all(vY == floor(vY)) == FALSE) {\n    stop(\"y is not an integer vector. Stopping...\")\n  }\n  \n  # unconstrained max\n  fitUnconMax &lt;- optim(vInitGuessesUncon, fLnL_cpp, fScore_2_cpp, vY = Y, mX = X, method = \"BFGS\", control=list(fnscale=-1))\n  \n  # constrained max\n  fitConMax &lt;- optim(vInitGuessesCon, fLnL_cpp, fScore_2_cpp, vY = Y, mX = X, method = \"L-BFGS-B\", lower = -Inf, upper = c(0, rep(Inf, length(vInitGuessesCon) - 1)), control=list(fnscale=-1))\n  \n  return(list(\n    LR_test_stat = -2 * (fitConMax$value - fitUnconMax$value),\n    MLE_estimator_uncon = fitUnconMax$par,\n    convergence_messages = c(\n      unconstrained = paste0(\"Convergence code:\", fitUnconMax$convergence, \"-\", fitUnconMax$message),\n      constrained = paste0(\"Convergence code:\", fitConMax$convergence, \"-\", fitConMax$message))))\n}\n\nLR_stat(vY = Y, mX = X)\n#&gt; $LR_test_stat\n#&gt; [1] 172.3262\n#&gt; \n#&gt; $MLE_estimator_uncon\n#&gt; [1]  3.630563467  0.126546522 -0.008899956  0.282246449  0.236120415\n#&gt; \n#&gt; $convergence_messages\n#&gt;                                                        unconstrained \n#&gt;                                                \"Convergence code:0-\" \n#&gt;                                                          constrained \n#&gt; \"Convergence code:0-CONVERGENCE: REL_REDUCTION_OF_F &lt;= FACTR*EPSMCH\"\n\n\nCreate an R package that contains the functions from questions 2, 3, and 5. Edit the title description to “This is my exam package”. Export the package as a bundled development version (with file extension .tar.gz), and include it as part of your exam submission. Remark: If you cannot solve 2, 3, or 5, create a package that contains an R and a C++ function with single scalar inputs that always return the number 2023.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Ordinary exam 2023</span>"
    ]
  },
  {
    "objectID": "exam2023re.html",
    "href": "exam2023re.html",
    "title": "\n25  Retake exam 2023\n",
    "section": "",
    "text": "25.1 Problem 1:\nBegin by setting the seed to 134.\nSolution:\nset.seed(134)\n\nfn_denPareto &lt;- function(x, gamma, alpha) {\n  vOut &lt;- ifelse(x &lt; gamma, 0, alpha * gamma^alpha / x^(alpha + 1))\n  return(vOut)\n}\nSolution:\nfn_ParetoSimul &lt;- function(gamma, alpha, samples, length) {\n  \n  U &lt;- runif(samples * length)\n  vSim &lt;- gamma * (1 - U)^(-1/alpha)\n  return(matrix(vSim, nrow = length, ncol= samples))\n  \n}\n\nmX &lt;- fn_ParetoSimul(gamma = 1, alpha = 3, samples = 100, length = 500)\n\nhist(mX[, 1], \n     freq = FALSE,\n     breaks = 41,\n     col = \"cornflowerblue\", \n     xlab = \"\",\n     ylab = \"Density\",\n     main = \"\",\n     xlim = c(min(mX[, 1]), max(mX[, 1])))\n\nvX &lt;- seq(min(mX[, 1]), max(mX[, 1]), by = 0.05)\nlines(vX, fn_denPareto(vX, 1, 3), type = \"l\", col = \"red\", lwd = 2)\n\nlegend(\"topright\",\n       legend = c(\"histogram\", \"theoretical\"),\n       col = c(\"cornflowerblue\", \"red\"),\n       lwd = 2)\nSolution:\nfn_estimateParams &lt;- function(mInput) {\n  fAlphas &lt;- function(vX) {\n    vOut &lt;- length(vX) / sum(log(vX / min(vX)))\n    return(vOut)\n  }\n  \n  vGammas &lt;- apply(mInput, 2, min)\n  vAlphas &lt;- apply(mInput, 2, fAlphas)\n  \n  return(list(\n    alphas = vAlphas,\n    gammas = vGammas\n  ))\n}\n\nvAlpha &lt;- fn_estimateParams(mX)$alphas\nSolution:\nhist(vAlpha, \n     freq = FALSE,\n     breaks = 41,\n     col = \"cornflowerblue\", \n     xlab = \"\",\n     ylab = \"Density\",\n     main = \"\",\n     xlim = c(min(vAlpha), max(vAlpha)))",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Retake exam 2023</span>"
    ]
  },
  {
    "objectID": "exam2023re.html#problem-1",
    "href": "exam2023re.html#problem-1",
    "title": "\n25  Retake exam 2023\n",
    "section": "",
    "text": "The density function of the Pareto distribution is given as \\[ f(X) = \\begin{cases} \\frac{\\alpha \\gamma^\\alpha}{X^{\\alpha+1}}, & \\text{if } X \\ge \\gamma, \\\\ 0, & \\text{if } X &lt; \\gamma, \\end{cases} \\] where \\(\\gamma &gt; 0\\) and \\(\\alpha &gt; 0\\) are the parameters. Write a function fn_denPareto that returns the value \\(f(X)\\) for any \\(X\\) given the parameters \\(\\gamma\\) and \\(\\alpha\\).\n\n\n\n\nUse the inversion method to simulate data with Pareto distribution with the parameters values \\(\\gamma = 1\\) and \\(\\alpha = 3\\). The inverse CDF is given as \\[ F^{-1}(U) = \\gamma (1-U)^{-\\frac{1}{\\alpha}}, \\] where \\(U\\) is a standard uniform random variable. We wish to simulate \\(N=100\\) samples of length \\(T=500\\) each.\n\nWrite a function fn_ParetoSimul, which simulates and returns these samples in the \\(T \\times N\\) matrix mX as output.\nSimulate mX and create a histogram of its first column. Superimpose the theoretical density defined in question 1. in red using lines().\n\n\n\n\n\n\nThe maximum likelihood estimates of the parameters based on a sample \\(X_1, \\dots, X_T\\) are given by \\[ \\hat{\\gamma} = \\min(X_t) \\] \\[ \\hat{\\alpha} = \\frac{T}{\\sum_{t=1}^T \\ln(X_t/\\hat{\\gamma})} \\] Write a function fn_estimateParams, which estimates \\(\\hat{\\gamma}\\) and \\(\\hat{\\alpha}\\) for each column in mX. Save the resulting 100 estimates of \\(\\hat{\\alpha}\\) in a vector vAlpha.\n\n\n\n\nCreate a histogram of the estimates \\(\\hat{\\alpha}\\) contained in the vector vAlpha. Let the title of the plot be “Distribution of estimates” and set the label on the x-axis to “Estimates”.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Retake exam 2023</span>"
    ]
  },
  {
    "objectID": "exam2023re.html#problem-2-constrained-optimization-c-and-packaging",
    "href": "exam2023re.html#problem-2-constrained-optimization-c-and-packaging",
    "title": "\n25  Retake exam 2023\n",
    "section": "\n25.2 Problem 2: (Constrained) Optimization, C++, and Packaging",
    "text": "25.2 Problem 2: (Constrained) Optimization, C++, and Packaging\nNote: This problem is best solved in order from 1 - 6.\nIn this problem, you will work with the Y.R file provided with the exam. It contains data which you will use to estimate an autoregressive model by maximum likelihood. The model is as follows: \\[ y_t = \\mu + \\phi y_{t-1} + \\epsilon_t, \\quad \\epsilon_t \\stackrel{iid}{\\sim} N(0, \\sigma^2), \\quad t=2,\\dots,T, \\quad \\quad (2.1) \\] with initial value \\(y_1 \\sim N\\left(\\frac{\\mu}{1-\\phi}, \\frac{\\sigma^2}{1-\\phi^2}\\right)\\) and sample size \\(T=500\\). Naturally \\(\\sigma^2 &gt; 0\\), and since we’re only interested in stationary models \\(-1 &lt; \\phi &lt; 1\\). For convenience, we stack the parameters into the vector \\(\\boldsymbol{\\theta} = (\\mu, \\phi, \\sigma^2)'\\). Under this model, the joint pdf of the data is \\[ f_{Y_1,\\dots,Y_T}(\\mathbf{y} | \\boldsymbol{\\theta}) = f_{Y_1}(y_1 | \\boldsymbol{\\theta}) \\prod_{t=2}^T f_{Y_t|Y_{t-1}}(y_t|y_{t-1}, \\boldsymbol{\\theta}). \\] where \\[ f_{Y_1}(y_1|\\boldsymbol{\\theta}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2/(1-\\phi^2)}} \\exp\\left(-\\frac{1}{2} \\frac{[y_1 - \\mu/(1-\\phi)]^2}{\\sigma^2/(1-\\phi^2)}\\right), \\] \\[ f_{Y_t|Y_{t-1}}(y_t|y_{t-1}, \\boldsymbol{\\theta}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2} \\frac{[y_t - \\mu - \\phi y_{t-1}]^2}{\\sigma^2}\\right). \\]\n\nLoad the data file Y.R into your R workspace using readRDS(). It contains the variables \\(y_t\\) stacked in a vector.\n\nSolution:\n\nset.seed(123)\n\ngenData &lt;- function() {\n  mu_true &lt;- 0.5\n  phi_true &lt;- 0.7\n  sigma2_true &lt;- 1.5\n  T_val &lt;- 500\n\n  # Ensure parameters meet conditions\n  if (abs(phi_true) &gt;= 1) {\n    stop(\"phi must be between -1 and 1 for stationarity.\")\n  }\n  if (sigma2_true &lt;= 0) {\n    stop(\"sigma2 must be positive.\")\n  }\n\n  # Initialize the time series vector\n  y &lt;- numeric(T_val)\n\n  # 1. Generate y_1 from its stationary distribution\n  # y_1 ~ N(mu / (1 - phi), sigma^2 / (1 - phi^2))\n  mean_y1 &lt;- mu_true / (1 - phi_true)\n  var_y1 &lt;- sigma2_true / (1 - phi_true^2)\n  sd_y1 &lt;- sqrt(var_y1)\n\n  y[1] &lt;- rnorm(1, mean = mean_y1, sd = sd_y1)\n\n  # 2. Generate y_t for t = 2, ..., T\n  # y_t = mu + phi * y_{t-1} + epsilon_t, epsilon_t ~ N(0, sigma^2)\n  sd_epsilon &lt;- sqrt(sigma2_true)\n  for (t in 2:T_val) {\n    epsilon_t &lt;- rnorm(1, mean = 0, sd = sd_epsilon)\n    y[t] &lt;- mu_true + phi_true * y[t-1] + epsilon_t\n  }\n\n  return(y)\n}\n\ny &lt;- genData()\n\n\nCreate a C++ script, and write two C++ functions called f_Y1 and f_cond which compute \\(f_{Y_1}(y_1|\\boldsymbol{\\theta})\\) and \\(f_{Y_t|Y_{t-1}}(y_t|y_{t-1}, \\boldsymbol{\\theta})\\) respectively. In your R script, include a sourceCpp() command which compiles the C++ script and adds these functions to your environment.\n\nYou want to find the maximum likelihood estimator of \\(\\boldsymbol{\\theta}\\), using the average log-likelihood function: \\[ \\ell(\\boldsymbol{\\theta}|\\mathbf{y}) = \\frac{1}{T} \\left( \\ln f_{Y_1}(y_1|\\boldsymbol{\\theta}) + \\sum_{t=2}^T \\ln f_{Y_t|Y_{t-1}}(y_t|y_{t-1}, \\boldsymbol{\\theta}) \\right) \\]\nSolution:\n\n// [[Rcpp::depends(RcppArmadillo)]]\n#include &lt;RcppArmadillo.h&gt;\n\nusing namespace arma;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\ndouble f_Y1(double y1, vec vParams) {\n  double dPi = atan(1)*4;\n  double dMu = vParams[0];\n  double dPhi = vParams[1];\n  double dSigma2 = vParams[2];\n  \n  double dOut = 1 / pow(2 * dPi * dSigma2 / (1 - pow(dPhi, 2)), 0.5) * exp(-0.5 * pow((y1 - dMu / (1 - dPhi)), 2) / (dSigma2 / (1 - dPhi)));\n    \n  return dOut;\n}\n\n// [[Rcpp::export]]\ndouble f_cond(double y1, double yt_1, vec vParams) {\n  double dPi = atan(1)*4;\n  double dMu = vParams[0];\n  double dPhi = vParams[1];\n  double dSigma2 = vParams[2];\n  \n  double dOut = 1 / pow(2 * dPi * dSigma2, 0.5) * exp(-0.5 * pow((y1 - dMu - dPhi * yt_1), 2) / dSigma2);\n  \n  return dOut;\n}\n\n\nsuppressMessages(library(Rcpp))\n#&gt; Warning: pakke 'Rcpp' blev bygget under R version 4.3.3\nsuppressMessages(library(RcppArmadillo))\n#&gt; Warning: pakke 'RcppArmadillo' blev bygget under R version 4.3.3\nsourceCpp(\"exam2023recpp.cpp\")\n\n\nWrite an R function called fLnL_rep which computes the reparametrized average log-likelihood, such that the restrictions \\(\\sigma^2 &gt; 0\\) and \\(-1 &lt; \\phi &lt; 1\\) are satisfied. The function should accept a vector \\(\\boldsymbol{\\theta}\\) and the data \\(\\mathbf{y}\\). It should also appropriately transform \\(\\boldsymbol{\\theta}\\) into \\(\\tilde{\\boldsymbol{\\theta}}\\) which respects the restrictions. It should also use your functions f_Y1 and f_cond to compute \\(\\ell(\\boldsymbol{\\theta}|\\mathbf{y})\\). Remark: If you cannot fully solve question 2., you can earn partial points by implementing the functions f_Y1 and f_cond in R.\n\n\nSolution:\n\nfLnL_rep &lt;- function(vParams, vY) {\n  dMu &lt;- vParams[1]\n  dPhi &lt;- -1 + 2 * (exp(vParams[2]) / (1 + exp(vParams[2])))\n  dSigma2 &lt;- exp(vParams[3])\n  \n  dT &lt;- length(vY)\n  dSum &lt;- log(f_Y1(vY[1], c(dMu, dPhi, dSigma2)))\n  \n  for (t in 2:dT) {\n    dSum &lt;- dSum + log(f_cond(vY[t], vY[t - 1], c(dMu, dPhi, dSigma2)))\n  }\n  \n  return(dSum / dT)\n}\n\n\nMaximize the reparametrized average log-likelihood with the BFGS algorithm, using the optim() function. You may use \\(\\boldsymbol{\\theta}_0 = (0,0,1)\\) as the initial guess. Report the maximum likelihood estimator of \\(\\boldsymbol{\\theta}\\) and the average log-likelihood evaluated at that point. Hint: Due to the reparameterization, you may need to transform the solution again to obtain \\(\\boldsymbol{\\theta}\\).\n\n\nSolution:\n\nvParams &lt;- c(0, 0, 1)\nvParams[2] &lt;- log(((vParams[2] + 1) / 2) / (1 - ((vParams[2] + 1) / 2.0)))\nvParams[3] &lt;- log(vParams[3])\noptim_res &lt;- optim(vParams, fLnL_rep, vY = y, method = \"BFGS\", control=list(fnscale=-1))\nvParams &lt;- optim_res$par\n\ndMu &lt;- vParams[1]\ndPhi &lt;- -1 + 2 * (exp(vParams[2]) / (1 + exp(vParams[2])))\ndSigma2 &lt;- exp(vParams[3])\n\nprint(paste0(\"Est. Mu: \", dMu, \" vs act.: \", 0.5))\n#&gt; [1] \"Est. Mu: 0.67406977684722 vs act.: 0.5\"\nprint(paste0(\"Est. Phi: \", dPhi, \" vs act.: \", 0.7))\n#&gt; [1] \"Est. Phi: 0.62648860056871 vs act.: 0.7\"\nprint(paste0(\"Est. Sigma2: \", dSigma2, \" vs act.: \", 1.5))\n#&gt; [1] \"Est. Sigma2: 1.40416059123645 vs act.: 1.5\"\nprint(paste0(\"Maximized average log-likelihood: \", optim_res$value))\n#&gt; [1] \"Maximized average log-likelihood: -1.58915508954246\"\n\n\nThe file simulateAR1_buggy.cpp contains a C++ function using Rcpp and RcppArmadillo. The code contains multiple errors. Debug the function. Add a short comment before each line where you have made changes. The function should simulate an AR(1) process as in Equation (2.1), with user-provided arguments for \\(\\mu, \\phi, \\sigma^2\\) and \\(T\\). It should check whether these arguments satisfy the restrictions \\(\\sigma^2 &gt; 0\\) and \\(-1 &lt; \\phi &lt; 1\\), and provide an informative error message if not. It should return the simulated time series as a vector.\n\nSolution:\n\narma::vec simulateAR1_buggy(double mu_true = 0.5, double phi_true = 0.7, double sigma2_true = 1.5, double T_val = 500) {\n\n  if (abs(phi_true) &gt;= 1) {\n    stop(\"phi must be between -1 and 1 for stationarity.\");\n  }\n  if (sigma2_true &lt;= 0) {\n    stop(\"sigma2 must be positive.\");\n  }\n  \n  arma::vec y = zeros&lt;vec&gt;(T_val);\n  \n  double mean_y1 = mu_true / (1 - phi_true);\n  double var_y1 = sigma2_true / (1 - pow(phi_true, 2));\n  double sd_y1 = sqrt(var_y1);\n  \n  y[0] = Rf_rnorm(mean_y1, sd_y1);\n    \n  double sd_epsilon = sqrt(sigma2_true);\n  double epsilon_t = 0.0;\n  for (int t = 1; t &lt; T_val; t++) {\n    epsilon_t = Rf_rnorm(0, sd_epsilon);\n    y[t] = mu_true + phi_true * y[t-1] + epsilon_t;\n  }\n      \n  return(y);\n}\n\n\nsuppressMessages(library(Rcpp))\nsuppressMessages(library(RcppArmadillo))\nsourceCpp(\"exam2023recpp.cpp\")\n\nvY &lt;- simulateAR1_buggy(0.5, 0.7, 1.5, 500)\n\nAnd just for fun:\n\nvParams &lt;- c(0, 0, 1)\nvParams[2] &lt;- log(((vParams[2] + 1) / 2) / (1 - ((vParams[2] + 1) / 2.0)))\nvParams[3] &lt;- log(vParams[3])\noptim_res &lt;- optim(vParams, fLnL_rep, vY = vY, method = \"BFGS\", control=list(fnscale=-1))\nvParams &lt;- optim_res$par\n\ndMu &lt;- vParams[1]\ndPhi &lt;- -1 + 2 * (exp(vParams[2]) / (1 + exp(vParams[2])))\ndSigma2 &lt;- exp(vParams[3])\n\nprint(paste0(\"Est. Mu: \", dMu, \" vs act.: \", 0.5))\n#&gt; [1] \"Est. Mu: 0.50608151243852 vs act.: 0.5\"\nprint(paste0(\"Est. Phi: \", dPhi, \" vs act.: \", 0.7))\n#&gt; [1] \"Est. Phi: 0.694273961540576 vs act.: 0.7\"\nprint(paste0(\"Est. Sigma2: \", dSigma2, \" vs act.: \", 1.5))\n#&gt; [1] \"Est. Sigma2: 1.52925097542527 vs act.: 1.5\"\n\n\nCreate an R package that contains the functions from questions 2., 3. and 5., and edit the title description to “This is my exam package”. Export the package as a bundled development version (with file extension .tar.gz), and include it as part of your exam submission. Remark: If you cannot solve 2., 3. or 5., create a package that contains an R and a C++ function with single scalar inputs that always return the number 42.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Retake exam 2023</span>"
    ]
  },
  {
    "objectID": "exam2024.html",
    "href": "exam2024.html",
    "title": "\n26  Ordinary exam 2024\n",
    "section": "",
    "text": "26.1 Problem 1\nBegin by setting the seed to 123.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Ordinary exam 2024</span>"
    ]
  },
  {
    "objectID": "exam2024.html#problem-1",
    "href": "exam2024.html#problem-1",
    "title": "\n26  Ordinary exam 2024\n",
    "section": "",
    "text": "26.1.1 Problem 1.1\n\nWrite a function rndRayleigh to simulate a random variable using the inversion method.\n\nFirst, use the inversion method to simulate data \\(y\\) from the Exponential distribution \\(y \\sim \\text{Exp}(\\lambda)\\) with the parameter \\(\\lambda = 2\\).\nThe cumulative distribution function (CDF) of the exponential distribution is given as: \\[G(y) = 1 - e^{-\\lambda y}.\\]\n\nDefine a new random variable \\(x = \\sqrt{y}\\).\nStore simulated random variables \\(x\\) in the matrix mR of size \\(T \\times N\\), where \\(T = 1000\\) and \\(N = 100\\).\n\n\n\nSolution:\n\nset.seed(123)\n\nrndRayleigh &lt;- function(lambda, size) {\n  \n  # lambda - intensity of the exponential distribution\n  # size - number of i.i.d. random variables to be drawn\n  \n  U &lt;- runif(size)\n  return(-1/lambda * log(U))\n  \n}\nset.seed(123) # set seed\n\n# matrix size and default variables\niT &lt;- 1000\niN &lt;- 100\nlambda &lt;- 2\n\n# simulate data, y\ny &lt;- rndRayleigh(lambda, iT * iN)\n\nx &lt;- sqrt(y)\n\nmR &lt;- matrix(x, iT, iN)\n\n\n26.1.2 Problem 1.2\n\nCalculate empirical moments using a vector-based command (Lecture on Vector-based programming using functions).\n\nDefine a constant \\(\\gamma = \\frac{1}{\\sqrt{2\\lambda}}\\), and parameters \\(\\mu = \\gamma \\sqrt{\\frac{\\pi}{2}}\\) and \\(\\sigma^2 = \\frac{4-\\pi}{2} \\gamma^2\\).\nUsing a vector-based command, calculate the mean \\(\\bar{\\mu}\\) and variance \\(\\bar{\\sigma}^2\\) of each column of the matrix mR, and subtract the values of \\(\\mu\\) and \\(\\sigma^2\\), storing them separately in the vectors vmuError and vsigmaError. Each of these vectors must be of dimension \\(N \\times 1\\).\nCalculate the mean and standard deviation of the elements of the vectors vmuError and vsigmaError.\n\n\n\nSolution:\n\n# defining the constants\ngamma &lt;- 1 / (sqrt(2 * lambda))\nmu &lt;- gamma * sqrt(2 / pi)\nsigma2 &lt;- (4 - pi) / 2 * gamma^2\n\n# mean and variance of each column\nmu_hat &lt;- colMeans(mR)\nsigma2_hat &lt;- apply(mR, 2, var)\n\n# store the errors\nvmuError &lt;- mu_hat - mu\nvsigmaError &lt;- sigma2_hat - sigma2\n\n# calculate mean and std of these\nmean(vmuError)\n#&gt; [1] 0.228063\nmean(vsigmaError)\n#&gt; [1] -0.0006383356\nsqrt(var(vmuError))\n#&gt; [1] 0.009235813\nsqrt(var(vsigmaError))\n#&gt; [1] 0.004542095\n\n\n26.1.3 Problem 1.3\n\nProduce histogram and density plots.\n\nCreate a histogram of the data contained in any of the columns of the matrix mR.\n\nCalculate the density function based on the parameter values \\(\\gamma\\) as follows:\n\\[f(x) = \\frac{x}{\\gamma^2} \\exp\\left(-\\frac{x^2}{2\\gamma^2}\\right).\\]\n\nAdd a plot of this density function to the histogram.\n\n\n\nSolution:\n\n# histogram\nhist(mR, \n     freq = FALSE,\n     breaks = 30,\n     col = \"cornflowerblue\", \n     xlab = \"x\",\n     ylab = \"Density\",\n     main = \"Histogram\",\n     xlim = c(0, 3))\n\n# superimpose the density function\nfDensity &lt;- function(x, gamma) {\n  return((x / gamma^2) * exp(-(x^2 / (2*gamma^2))))\n}\n\ncurve(fDensity(x, gamma),\n      from = 0,\n      to = 3,\n      col = \"red\",      \n      lwd = 2,\n      add = TRUE)\n\nlegend(\"topright\",\n       legend = c(\"histogram\", \"density\"),\n       col = c(\"cornflowerblue\", \"red\"),\n       lwd = 2)\n\n\n\n\n\n\n\n\n26.1.4 Problem 1.4\n\nProduce histogram and density plot using the Box-Muller algorithm.\n\nSimulate two standard random normal variables \\(y_1\\) and \\(y_2\\) using the Box-Muller Algorithm, each of size 1000.\nDefine a new random variable \\(z = \\sqrt{y_1^2 + y_2^2}\\) and store it in a vector vz. The vector is of size \\(1000 \\times 1\\).\nPlot a histogram of the data contained in the vector vz.\n\nAdd a density plot to the histogram, where the density is given by\n\\[f(z) = z \\exp\\left(-\\frac{z^2}{2}\\right).\\]\n\n\n\n\nSolution:\n\nBoxMuller &lt;- function(size = 1) {\n  \n  # size: number of standard bivariate normal random variables to simulate\n  \n  U &lt;- runif(size)\n  V &lt;- runif(size)\n  \n  X &lt;- sqrt(-2*log(U)) * cos(2*pi*V)\n  Y &lt;- sqrt(-2*log(U)) * sin(2*pi*V)\n  \n  return(c(X,Y))\n  \n}\n\ny_1 &lt;- BoxMuller(1000)\ny_2 &lt;- BoxMuller(1000)\n\nvZ &lt;- sqrt(y_1^2 + y_2^2)\n\n# histogram\nhist(vZ, breaks = 35, freq = FALSE,\n     main = \"Theoretical and simulated density\",\n     col = \"cornflowerblue\",\n     xlim = c(-0.5, 5), ylim = c(0, 0.7),\n     xlab = \"z\",\n     cex.main = 0.7)\n\nfDensityNew &lt;- function(z) {\n  return(z * exp(- z^2/2))\n}\nx &lt;- vZ\ncurve(fDensityNew(x),\n      from = -0.5,\n      to = 5,\n      col = \"red\",      \n      lwd = 2,\n      add = TRUE)\n\nlegend(\"topright\", legend = c(\"Simulated\", \"Theoretical\"), lty = c(1, 1), lwd = c(5, 2), col = c(\"cornflowerblue\", \"red\"))",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Ordinary exam 2024</span>"
    ]
  },
  {
    "objectID": "exam2024.html#problem-2-constrained-optimization-c-and-packaging",
    "href": "exam2024.html#problem-2-constrained-optimization-c-and-packaging",
    "title": "\n26  Ordinary exam 2024\n",
    "section": "\n26.2 Problem 2: (Constrained) Optimization, C++, and Packaging",
    "text": "26.2 Problem 2: (Constrained) Optimization, C++, and Packaging\nNote: This problem is best solved in order from 1 - 6.\nIn this problem, you will work with the d.R file provided with the exam. It contains a sample of \\(T=250\\) draws from a binomial distribution with number of trials \\(n=10\\) and unknown probability \\(p\\). The probability density function is \\[f(k, n, p) = P(d_t=k) = \\binom{n}{k} p^k (1-p)^{n-k},\\] where \\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\) denotes the binomial coefficient.\n\n26.2.1 Problem 2.1\n\nLoad the data file d.R into your R workspace using readRDS().\n\nSolution:\n\nvD &lt;- readRDS(\"d.R\")\n\nYou will find the maximum likelihood estimator of \\(p\\) by numerically maximizing the average log-likelihood using Newton’s Method (recall Lecture 8). For this purpose, you will need the average log-likelihood: \\[\\ell(p|n,d) = \\frac{1}{T} \\sum_{t=1}^T \\left[ \\log \\binom{n}{d_t} + d_t \\log(p) + (n-d_t)\\log(1-p) \\right], \\quad (1)\\] the average score: \\[s(p|n,d) = \\frac{1}{T} \\sum_{t=1}^T \\left[ \\frac{d_t}{p} - \\frac{n-d_t}{1-p} \\right], \\quad (2)\\] and the average second derivative: \\[sd(p|n,d) = \\frac{1}{T} \\sum_{t=1}^T \\left[ -\\frac{d_t}{p^2} - \\frac{n-d_t}{(1-p)^2} \\right]. \\quad (3)\\] (Remark: By log, we always mean the natural logarithm.)\n\n26.2.2 Problem 2.2\n\n\nWrite an R function called log_n_choose_k, which computes the first term of in equation (1) above. It should accept two integer arguments \\(n\\) and \\(k\\), and return the log binomial coefficient: \\[\\log \\binom{n}{k} = \\begin{cases} 0 & \\text{if } k=0 \\\\ \\sum_{i=n-k+1}^n \\log(i) - \\sum_{i=1}^k \\log(i) & \\text{otherwise} \\end{cases}\\]\n\n\n\nSolution:\n\n\nlog_n_choose_k &lt;- function(n, k) {\n  if (k == 0) {\n    return(0)\n  } else {\n    dSumPart1 &lt;- 0\n    dSumPart2 &lt;- 0\n    for (i in (n - k + 1):n) {\n      dSumPart1 &lt;- dSumPart1 + log(i)\n    }\n    for (i in 1:k) {\n      dSumPart2 &lt;- dSumPart2 + log(i)\n    }\n    return(dSumPart1 - dSumPart2)\n  }\n}\n\n\nCreate a C++ script and write a C++ function called log_n_choose_k_cpp which does the same as your R function in part (a).\n\nSolution:\n\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\ndouble log_n_choose_k_cpp(int n, int k) {\n  \n  if (k == 0) {\n    return(0);\n  } else {\n    double dSumPart1 = 0;\n    double dSumPart2 = 0;\n    for (int i = (n - k + 1); i &lt;= n; i++) {\n      dSumPart1 = dSumPart1 + log(i);\n    }\n    for (int i = 1; i &lt;= k; i++) {\n      dSumPart2 = dSumPart2 + log(i);\n    }\n    return(dSumPart1 - dSumPart2);\n  }\n}\n\n\nsuppressMessages(library(Rcpp))\n#&gt; Warning: pakke 'Rcpp' blev bygget under R version 4.3.3\nsourceCpp(\"cpp_functions.cpp\")\n\n# naive check that the functions are equal\nlog_n_choose_k(10, 3)\n#&gt; [1] 4.787492\nlog_n_choose_k_cpp(10, 3)\n#&gt; [1] 4.787492\n\n\n26.2.3 Problem 2.3\n\n\nWrite three R functions fLnL, fScore, and fSD which implement equations (1)-(3) respectively. They should accept a decimal number \\(p\\), an integer \\(n\\), and an integer vector \\(d\\) as arguments.\n\n\nSolution:\n\nfLnL &lt;- function(p, n, d) {\n  result &lt;- numeric(length(d))\n  \n  for (i in 1:length(d)) {\n    result[i] &lt;- log_n_choose_k(n, d[i]) + d[i] * log(p) + (n - d[i]) * log(1 - p)\n  }\n  \n  return(mean(result))\n}\n\nfScore &lt;- function(p, n, d) {\n  return(mean(d / p - (n - d) / (1 - p)))\n}\n\nfSD &lt;- function(p, n, d) {\n  return(mean(-(d / p^2) - (n - d) / ((1 - p)^2)))\n}\n\n\nWrite three C++ functions fLnL_cpp, fScore_cpp, and fSD_cpp, which do the same as your R functions in part (a). In your R script, include a sourceCpp() command from the Rcpp package which compiles the C++ script and adds all C++ functions from questions 2 and 3 to your environment. Remark: If you cannot solve question 2, you may skip the \\(\\log \\binom{n}{k}\\) term in the fLnL and fLnL_cpp functions.\n\n\n\n// [[Rcpp::export]]\ndouble fLnL_cpp(double p, int n, vec d) {\n  int len = d.n_elem;\n  vec result(len);\n  \n  for (int i = 0; i &lt; len; i++) {\n    result[i] = log_n_choose_k_cpp(n, d[i]) + d[i] * log(p) + (n - d[i]) * log(1 - p);\n  }\n  return(mean(result));\n}\n\n// [[Rcpp::export]]\ndouble fScore_cpp(double p, int n, vec d) {\n  return(mean(d / p - (n - d) / (1 - p)));\n}\n\n// [[Rcpp::export]]\ndouble fSD_cpp(double p, int n, vec d) {\n  return(mean(-(d / pow(p, 2)) - (n - d) / pow((1 - p), 2)));\n}\n\n\n26.2.4 Problem 2.4\n\nWrite an R function called Newton which maximizes the average log likelihood in equation (1) using Newton’s Method. This function should have the following arguments: a function to be maximized, its first derivative, its second derivative, a starting value, a tolerance threshold, maximum number of iterations, and other arguments which need to be passed the function you’re maximizing and its derivatives (i.e. \\(n=10\\) and \\(d\\)). It should return a list containing the value which maximizes the function, the maximum itself, and it should give an indication whether convergence was achieved or not.\n\nRun this function using fLnL_cpp, fScore_cpp, and fSD_cpp as the inputs, and use starting value \\(p=0.5\\). Compare the optimal value of \\(p\\) you find with the analytical maximum \\(p^* = \\left(\\frac{1}{T} \\sum_{t=1}^T d_t\\right)/n\\). Remark: You may adapt the example code from Lecture 8 if you wish. If you cannot implement the C++ functions in questions 2 or 3, you may use the corresponding R functions to do this optimization instead, for partial credit.\nSolution:\n\nNewton &lt;- function(f, f_prime, f_sec, X0, Tol = 1e-9, max.iter = 1000, ...){\n  dX &lt;- X0\n  fx &lt;- f(dX, ...)\n  fpx &lt;- f_prime(dX, ...)\n  fsx &lt;- f_sec(dX, ...)\n  i &lt;- 0\n  while ((abs(fpx) &gt; Tol) && (i &lt; max.iter)) {\n    dX &lt;- dX - fpx/fsx\n    fx &lt;- f(dX, ...)\n    fpx &lt;- f_prime(dX, ...)\n    fsx &lt;- f_sec(dX, ...)\n    i &lt;- i + 1\n    #cat(\"At iteration\", n, \"the value of x is:\", dX, \"\\n\")\n  }\n  if (i == max.iter) {\n    return(\n      list(\n       maximizing.val = NULL, \n       maximum = NULL, \n       \"Algorithm failed to converge. Maximum iterations reached.\")\n      )\n  } else {\n    return(\n      list(\n       maximizing.val = dX, \n       maximum = f(dX, ...), \n       \"Algorithm converged.\")\n      )\n  }\n}\n\n# Newton max\nNewton(fLnL_cpp, fScore_cpp, fSD_cpp, 0.5, n = 10, d = vD)\n#&gt; $maximizing.val\n#&gt; [1] 0.2032\n#&gt; \n#&gt; $maximum\n#&gt; [1] -1.543041\n#&gt; \n#&gt; [[3]]\n#&gt; [1] \"Algorithm converged.\"\n\n# Analytical max\np.star &lt;- mean(vD) / 10\np.star\n#&gt; [1] 0.2032\n\nSuppose you obtain additional information that the \\(p\\) which maximizes the likelihood lies somewhere between 0.1 and 0.9. You will use reparameterization to enforce this constraint. Consider the transformation \\(p = \\lambda(\\tilde{p}) = 0.1 + 0.8 \\frac{\\exp(\\tilde{p})}{1+\\exp(\\tilde{p})}\\), such that \\(p\\) is between 0.1 and 0.9 for any value of \\(\\tilde{p}\\). This transformation has a corresponding transformed average log-likelihood \\[\\tilde{\\ell}(\\tilde{p}|n,d) = \\ell(\\lambda(\\tilde{p})|n,d) = \\frac{1}{T} \\sum_{t=1}^T \\left[ \\log \\binom{n}{d_t} + d_t \\log(\\lambda(\\tilde{p})) + (n-d_t)\\log(1-\\lambda(\\tilde{p})) \\right]. \\quad (4)\\]\n\n26.2.5 Problem 2.5\n\n\nWrite a new R function fLnL_t, which implements equation (4).\n\n\nSolution:\n\nfLnL_t &lt;- function(p, n, d) {\n  new_p &lt;- 0.1 + 0.8 * (exp(p)) / (1 + exp(p))\n  result &lt;- numeric(length(d))\n  \n  for (i in 1:length(d)) {\n    result[i] &lt;- log_n_choose_k(n, d[i]) + d[i] * log(new_p) + (n - d[i]) * log(1 - new_p)\n  }\n  \n  return(mean(result))\n}\n\n\nMaximize the transformed average log-likelihood using the function optim (or optimize (your choice)), paying close attention to the initial values/bounds that you use. Report the maximum and maximizer you find, making sure that those results are transformed back into the original scale, such that they are comparable with the results you found in question 4.\n\nSolution:\n\ndOptimalP &lt;- optimize(fLnL_t, lower = -1000, upper = 1000, n = 10, d = vD, maximum = TRUE)$objective\n\ndOptimalP.orig.scale &lt;- 0.1 + 0.8 * (exp(dOptimalP)) / (1 + exp(dOptimalP)) \n\ndOptimalP.orig.scale\n#&gt; [1] 0.1942476\n\n\n26.2.6 Problem 2.6\n\nCreate an R package that contains the all functions you wrote for questions 2-5, and edit the Title of your package in the description file to “This is my exam package”. Export the package as a bundled development version (with file extension .tar.gz), and include it as part of your exam submission. Remark: If you cannot solve questions 2-5, create a package that contains an R and a C++ function with single scalar input that always return the number 2024.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Ordinary exam 2024</span>"
    ]
  },
  {
    "objectID": "exam2024retake.html",
    "href": "exam2024retake.html",
    "title": "\n27  Retake exam 2024\n",
    "section": "",
    "text": "27.1 Problem 1",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Retake exam 2024</span>"
    ]
  },
  {
    "objectID": "exam2024retake.html#problem-1",
    "href": "exam2024retake.html#problem-1",
    "title": "\n27  Retake exam 2024\n",
    "section": "",
    "text": "27.1.1 Problem 1.1\n\nMonte Carlo Integration\n\n\nConsider a function\n\\[f(u) = -\\frac{\\ln(1-u)}{\\lambda},\\]\n\n\n\n\nwhere \\(\\lambda &gt; 0\\) and \\(u \\in (0,1)\\). * Consider the following integral\n\\[\\gamma = \\frac{1}{1-\\alpha} \\int_{\\alpha}^{1} f(u) \\, du\\]\n\nEvaluate the above integral for \\(\\alpha = 0.95\\) and \\(\\lambda = 2\\) using the Monte Carlo integration technique.\nCompare the result of your calculations with the following expression\n\n\\[\\gamma = \\frac{-\\ln(1-\\alpha)+1}{\\lambda}.\\]\nSolution:\n\nMonteCarlo.Integration &lt;- function(f, n, a, b) {\n  \n  U &lt;- runif(n, min = a, max = b)\n  return( (b-a)*mean(f(U)) )\n  \n}\n\nset.seed(123)\n\ndAlpha &lt;- 0.95\ndLambda &lt;- 2\n\n# Monte Carlo Integration\ndGammaMI &lt;- MonteCarlo.Integration(function(x) (-(log(1-x)) / dLambda), 100, dAlpha, 1)\ndGammaMI\n#&gt; [1] 0.09927233\n\n# Numerical evaluation\ndGammaNE &lt;- (-log(1-dAlpha) + 1) / dLambda\ndGammaNE\n#&gt; [1] 1.997866\n\n# Thus, we should have this relationship\n# dGammaNE * (1 - dAlpha) approx dGammaMI\ndGammaNE * (1 - dAlpha)\n#&gt; [1] 0.09989331\n\n\n27.1.2 Problem 1.2\n\nmU &lt;- matrix(runif(40000), 200, 200)\nmU &lt;- ifelse(mU &lt; 0.5, -1, 1)\n\nsum(mU == 1)\n#&gt; [1] 19715\nsum(mU == -1)\n#&gt; [1] 20285\n\n\nVectorized if/else statement\n\nGenerate a \\(200 \\times 200\\) matrix where each entry is a standard Uniform random variable.\nAssign the value -1 to those elements of the matrix that are less than 0.5 and 1 to those that are larger than or equal to 0.5.\nCount how many entries are -1 and 1, and report the counts.\n\n\n\nSolution:\n\n27.1.3 Problem 1.3\n\nRecursive functions\n\nConsider the following power function that calculates \\(z^k\\) written as \\[\\text{Power}(z, k) = \\begin{cases} 1 & \\text{if } k=0 \\\\ z \\times \\text{Power}(z, k-1) & \\text{if } k &gt; 0 \\end{cases}\\]\n\nWrite the recursive function.\n\n\n\nSolution:\n\nfPower &lt;- function(z, k) {\n  if (k == 0) {\n    return(1)\n  } else if (k &gt; 0) {\n    return(z * fPower(z, k - 1))\n  } else {\n    return(NULL)\n  }\n}\n# Test\nfPower(2, 4)\n#&gt; [1] 16\n\n\n27.1.4 Problem 1.4\n\nSimulation by Application of the Inversion Method\n\nBegin by setting the seed to 134.\nSuppose \\(\\mu = 0.5\\) is a location parameter and \\(\\gamma = 2\\).\nUse the inversion method to simulate 1000 Cauchy-distributed random variables using the following inverse cumulative distribution function: \\[g(u) = \\mu + \\gamma \\tan\\left(\\pi \\left(u - \\frac{1}{2}\\right)\\right),\\] where \\(\\tan()\\) stands for the trigonometric tangent function.\nPlot a histogram using the variables you have just generated.\n\n\n\nSolution:\n\nset.seed(134)\ndMu &lt;- 0.5\ndGamma &lt;- 2\n\nCauchy.Simulate &lt;- function(mu, lambda, size) {\n  U &lt;- runif(size)\n  return(mu + lambda * tan(pi * (U - 1/2)))\n}\n\nvX &lt;- Cauchy.Simulate(dMu, dGamma, 1000)\nhist(vX, \n     freq = FALSE,\n     breaks = 5000,\n     col = \"cornflowerblue\", \n     xlab = \"x\",\n     ylab = \"Density\",\n     main = \"Histogram\",\n     xlim = c(-20, 20))",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Retake exam 2024</span>"
    ]
  },
  {
    "objectID": "exam2024retake.html#problem-2-constrained-optimization-c-and-packaging",
    "href": "exam2024retake.html#problem-2-constrained-optimization-c-and-packaging",
    "title": "\n27  Retake exam 2024\n",
    "section": "\n27.2 Problem 2: (Constrained) Optimization, C++, and Packaging",
    "text": "27.2 Problem 2: (Constrained) Optimization, C++, and Packaging\nNote: This problem is best solved in order from 1 - 5.\nIn this problem, you will numerically maximize the negative Rosenbrock function\n\\[f(x,y) = -(\\pi - x)^2 - 100(y-x^2)^2, \\quad (1)\\]\nwhere \\(\\pi\\) is the mathematical constant 3.141593… You will use the Steepest Ascent Method (recall Lecture 9). For this purpose, you will need the gradient of \\(f\\):\n\\[\\nabla f(x,y) = \\begin{pmatrix} 2\\pi - 2x - 400x^3 + 400xy \\\\ 200(x^2-y) \\end{pmatrix}. \\quad (2)\\]\n\n27.2.1 Problem 2.1\n\n\nWrite R functions called fRosen and fRosen_grad which implement equations (1) and (2).\n\n\nSolution:\n\nfRosen &lt;- function(vX) {\n  return(-(pi - vX[1])^2 - 100 * (vX[2] - vX[1]^2)^2)\n}\n\nfRosen_grad &lt;- function(vX) {\n  vOutput &lt;- numeric(2)\n  vOutput[1] &lt;- 2 * pi - 2 * vX[1] - 400 * vX[1]^3 + 400 * vX[1] * vX[2]\n  vOutput[2] &lt;- 200 * (vX[1]^2 - vX[2])\n  return(vOutput)\n}\n\n\nCreate a C++ script and write C++ functions called fRosen_cpp and fRosen_grad_cpp which do the same as your R functions in part (a). In your R script, include a sourceCpp() command from the Rcpp package which compiles the C++ script and adds all C++ functions from questions 2 and 3 to your environment.\n\nSolution:\n\n// [[Rcpp::depends(RcppArmadillo)]]\n#include &lt;RcppArmadillo.h&gt;\nusing namespace Rcpp;\nusing namespace arma;\n\n// [[Rcpp::export]]\ndouble fRosen_cpp(vec vX) {\n  double pi = atan(1)*4; //pi\n  return(-pow((pi - vX[0]), 2) - 100 * pow((vX[1] - pow(vX[0], 2)), 2));\n}\n\n// [[Rcpp::export]]\nvec fRosen_grad_cpp(vec vX) {\n  vec vOutput(2);\n  double pi = atan(1)*4; //pi\n  vOutput[0] = 2 * pi - 2 * vX[0] - 400 * pow(vX[0], 3) + 400 * vX[0] * vX[1];\n  vOutput[1] = 200 * (pow(vX[0], 2) - vX[1]);\n  return(vOutput);\n}\n\n\nsuppressMessages(library(Rcpp))\n#&gt; Warning: pakke 'Rcpp' blev bygget under R version 4.3.3\nsuppressMessages(library(RcppArmadillo))\n#&gt; Warning: pakke 'RcppArmadillo' blev bygget under R version 4.3.3\nsourceCpp(\"cpp_functions_retake_2024.cpp\")\n\n# sanity checks\nfRosen(c(0.5, 2))\n#&gt; [1] -313.228\nfRosen_cpp(c(0.5, 2))\n#&gt; [1] -313.228\nfRosen_grad(c(0.5, 2))\n#&gt; [1]  355.2832 -350.0000\nfRosen_grad_cpp(c(0.5, 2))\n#&gt;           [,1]\n#&gt; [1,]  355.2832\n#&gt; [2,] -350.0000\n\n\n27.2.2 Problem 2.2\n\n\nWrite an R function called ascent which implemented the Steepest Ascent Method with the Golden Section Method used to compute the step size. The function should accept as arguments the function to be maximized, its gradient, a starting value, a tolerance level, and a maximum number of iterations. It should output a list containing the function maximum and the vector at which the maximum is attained. In case the algorithm does not converge, the list should contain -Inf as the function maximum and NA as the vector.\n\n\nSolution:\n\n# golden section algorithm from last week\ngsection &lt;- function(f, dX.l, dX.r, dX.m, dTol = 1e-9) {\n  \n  # golden ratio plus one\n  dGR1 &lt;- 1 + (1 + sqrt(5))/2\n  \n  # successively refine x.l, x.r, and x.m\n  f.l &lt;- f(dX.l)\n  f.r &lt;- f(dX.r)\n  f.m &lt;- f(dX.m)\n  while ((dX.r - dX.l) &gt; dTol) { \n    if ((dX.r - dX.m) &gt; (dX.m - dX.l)) { # if the right segment is wider than the left \n      dY &lt;- dX.m + (dX.r - dX.m)/dGR1 # put Y into the right segment according to the golden ratio\n      f.y &lt;- f(dY)\n      if (f.y &gt;= f.m) {\n        dX.l &lt;- dX.m\n        f.l &lt;- f.m\n        dX.m &lt;- dY\n        f.m &lt;- f.y\n      } else {\n        dX.r &lt;- dY\n        f.r &lt;- f.y\n      }\n    } else { #if the left segment is wider than the right\n      dY &lt;- dX.m - (dX.m - dX.l)/dGR1 # put Y into the left segment according to the golden ratio\n      f.y &lt;- f(dY)\n      if (f.y &gt;= f.m) {\n        dX.r &lt;- dX.m\n        f.r &lt;- f.m\n        dX.m &lt;- dY\n        f.m &lt;- f.y\n      } else {\n        dX.l &lt;- dY\n        f.l &lt;- f.y\n      }\n    }\n  }\n  return(dX.m)\n}\n\nline.search &lt;- function(f, vX, vG, dTol = 1e-9, dA.max = 2^5) {\n  # f is a real function that takes a vector of length d\n  # x and y are vectors of length d\n  # line.search uses gsection to find a &gt;= 0 such that\n  # g(a) = f(x + a*y) has a local maximum at a,\n  # within a tolerance of tol\n  # if no local max is found then we use 0 or a.max for a\n  # the value returned is x + a*y\n  if (sum(abs(vG)) == 0){\n    return(vX) # +0*vG\n  } # g(a) constant\n  g &lt;- function(dA){\n    return(f(vX + dA*vG)) \n  }\n  # find a triple a.l &lt; a.m &lt; a.r such that\n  # g(a.l) &lt;= g(a.m) and g(a.m) &gt;= g(a.r)\n  \n  # choose a.l\n  dA.l &lt;- 0\n  g.l &lt;- g(dA.l)\n  # find a.m\n  dA.m &lt;- 1\n  g.m &lt;- g(dA.m)\n  while ((g.m &lt; g.l) & (dA.m &gt; dTol)) {\n    dA.m &lt;- dA.m/2\n    g.m &lt;- g(dA.m)\n  }\n  # if a suitable a.m was not found then use 0 for a, so just return vX as the next step\n  if ((dA.m &lt;= dTol) & (g.m &lt; g.l)){\n    return(vX)\n  } \n  # find a.r\n  dA.r &lt;- 2*dA.m\n  g.r &lt;- g(dA.r)\n  while ((g.m &lt; g.r) & (dA.r &lt; dA.max)) {\n    dA.m &lt;- dA.r\n    g.m &lt;- g.r\n    dA.r &lt;- 2*dA.m\n    g.r &lt;- g(dA.r)\n  }\n  # if a suitable a.r was not found then use a.max for a\n  if ((dA.r &gt;= dA.max) & (g.m &lt; g.r)){\n    return(vX + dA.max*vG)\n  } \n  # apply golden-section algorithm to g to find a\n  dA &lt;- gsection(g, dA.l, dA.r, dA.m)\n  return(vX + dA*vG)\n}\n\nascent &lt;- function(f, grad.f, vX0, dTol = 1e-9, n.max = 100) {\n  vX.old &lt;- vX0\n  vX &lt;- line.search(f, vX0, grad.f(vX0))\n  n &lt;- 1\n  while ((f(vX) - f(vX.old) &gt; dTol) & (n &lt; n.max)) {\n    vX.old &lt;- vX\n    vX &lt;- line.search(f, vX, grad.f(vX))\n    #cat(\"at iteration\", n, \"the coordinates of x are\", vX, \"\\n\")\n    n &lt;- n + 1\n  }\n  if (n == n.max) {\n    return(\n      list(\n       maximizing.val = NA, \n       maximum = -Inf, \n       paste0(\"Algorithm failed to converge. Maximum iterations reached. Last values:\", vX))\n      )\n  } else {\n    return(\n      list(\n       maximizing.val = vX, \n       maximum = f(vX), \n       \"Algorithm converged.\")\n      )\n  }\n}\n\n\nUse this function to attempt to maximize your fRosen_cpp function, using fRosen_grad_cpp as its gradient, and starting value \\((1,1)\\). This function is difficult to optimize, so it will likely fail to converge.\n\nRemark: You may adapt the example code from Lecture 9 if you wish, including the functions gsection and line.search. If you did not solve question 1(b), you can use the R functions instead of the C++ ones for partial credit.\nSolution:\n\nascent(fRosen_cpp, fRosen_grad_cpp, vX0 = c(1, 1))\n#&gt; $maximizing.val\n#&gt; [1] NA\n#&gt; \n#&gt; $maximum\n#&gt; [1] -Inf\n#&gt; \n#&gt; [[3]]\n#&gt; [1] \"Algorithm failed to converge. Maximum iterations reached. Last values:1.20713041591749\"\n#&gt; [2] \"Algorithm failed to converge. Maximum iterations reached. Last values:1.45716385117783\"\n\n\n27.2.3 Problem 2.3\nYou will now write a function which systematically applies the ascent function over a grid of starting values. To define the grid, let vector \\((x_1, x_2, \\dots, x_K)\\) and \\((y_1, y_2, \\dots, y_K)\\) be the grid coordinates. These are typically in ascending order and evenly spaced. The grid consists of all \\(K^2\\) points \\((x_i, y_j)\\), \\(i,j \\in \\{1, \\dots, K\\}\\).\n\n\nWrite a new R function grid_ascent which has inputs similar to the ascent function, but instead of a single initial value, it accepts two coordinate vectors. The function should evaluate the ascent function starting at each grid point. It should return a \\(K \\times K\\) matrix containing the corresponding function maxima found using these starting points.\n\n\nSolution:\n\ngrid_ascent &lt;- function(f, grad.f, vX0, vY0, dTol = 1e-9, n.max = 100) {\n  mOutput &lt;- matrix(NA, length(vX0), length(vY0))\n  \n  for (x in 1:length(vX0)) {\n    for (y in 1:length(vY0)) {\n      vXY0 &lt;- c(vX0[x], vY0[y])\n      vXY.old &lt;- vXY0\n      vXY &lt;- line.search(f, vXY0, grad.f(vXY0))\n      n &lt;- 1\n      while ((f(vXY) - f(vXY.old) &gt; dTol) & (n &lt; n.max)) {\n        vXY.old &lt;- vXY\n        vXY &lt;- line.search(f, vXY, grad.f(vXY))\n        n &lt;- n + 1\n      }\n      if (n == n.max) {\n        mOutput[x, y] &lt;- -Inf\n      } else {\n        mOutput[x, y] &lt;- f(vXY)\n      }\n    }\n  }\n  return(mOutput)\n}\n\n\nChoose an appropriate grid, and use the output of the grid_ascent function to find a starting point from which the ascent function converges to a maximum. To give you an idea where to start, try a grid with \\(x\\) values around 3 and \\(y\\) values around 9. To begin with, try a relatively small \\(K\\) (e.g. \\(K=5\\)) because the run time can be significant for large \\(K\\). If your grid cannot find any valid starting point, i.e. grid_ascent returns a matrix of all -Inf, here are some things you can try: make the grid larger by making the min and max of the coordinates further apart, make the grid finer by increasing \\(K\\), choose a smaller tolerance, or increase the maximum number of iterations. Do these changes gradually, as most of them will make the function run slower.\n\nSolution:\n\ngrid_ascent(fRosen_cpp, fRosen_grad_cpp, vX0 = c(2.5, 2.75, 3, 3.25, 3.5), vY0 = c(8.5, 8.75, 9, 9.25, 9.5))\n#&gt;      [,1]          [,2] [,3]          [,4] [,5]\n#&gt; [1,] -Inf          -Inf -Inf          -Inf -Inf\n#&gt; [2,] -Inf          -Inf -Inf          -Inf -Inf\n#&gt; [3,] -Inf -2.871553e-10 -Inf          -Inf -Inf\n#&gt; [4,] -Inf          -Inf -Inf -1.443747e-09 -Inf\n#&gt; [5,] -Inf          -Inf -Inf          -Inf -Inf\n\n\nWhen you find a grid point from which ascent converges, evaluate ascent at this starting point and report the function maximum and the point at which this maximum is achieved.\n\nSolution:\n\nascent(fRosen_cpp, fRosen_grad_cpp, vX0 = c(3, 8.75))\n#&gt; $maximizing.val\n#&gt;          [,1]\n#&gt; [1,] 3.141604\n#&gt; [2,] 9.869677\n#&gt; \n#&gt; $maximum\n#&gt; [1] -2.871553e-10\n#&gt; \n#&gt; [[3]]\n#&gt; [1] \"Algorithm converged.\"\nascent(fRosen_cpp, fRosen_grad_cpp, vX0 = c(3.25, 9.25))\n#&gt; $maximizing.val\n#&gt;          [,1]\n#&gt; [1,] 3.141555\n#&gt; [2,] 9.869366\n#&gt; \n#&gt; $maximum\n#&gt; [1] -1.443747e-09\n#&gt; \n#&gt; [[3]]\n#&gt; [1] \"Algorithm converged.\"\n\n\n27.2.4 Problem 2.4\n\nMaximize your fRosen_cpp function with the optim function in R, using starting point \\((1,1)\\), fRosen_cpp_cpp for the gr argument, and method = \"BFGS\". Compare the result with your answer in question 3.\n\nSolution:\n\noptim(par = c(1, 1), fn = fRosen_cpp, gr = fRosen_grad_cpp, method = \"BFGS\", control = list(fnscale = -1))\n#&gt; $par\n#&gt; [1] 3.141593 9.869604\n#&gt; \n#&gt; $value\n#&gt; [1] -7.901082e-19\n#&gt; \n#&gt; $counts\n#&gt; function gradient \n#&gt;      154       62 \n#&gt; \n#&gt; $convergence\n#&gt; [1] 0\n#&gt; \n#&gt; $message\n#&gt; NULL\n\n\n27.2.5 Problem 2.5\n\nCreate an R package that contains all the functions you wrote for questions 1-3, and edit the Title of your package in the description file to “This is my exam package”. Export the package as a bundled development version (with file extension .tar.gz), and include it as part of your exam submission.\n\nRemark: If you cannot solve questions 1-3, create a package that contains an R and a C++ function with single scalar input that always return the number 2024.2.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Retake exam 2024</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html",
    "href": "extra_exercises.html",
    "title": "\n28  Extra exercises\n",
    "section": "",
    "text": "29 Exercise Set 1: R Basics, Data Structures, and Control Flow",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section",
    "href": "extra_exercises.html#section",
    "title": "\n28  Extra exercises\n",
    "section": "\n29.1 (1)\n",
    "text": "29.1 (1)\n\n\n\nCreate a vector vNumbers containing integers from 1 to 50.\n\n\nCalculate the sum of all even numbers in vNumbers.\n\n\nCreate a new vector vLog containing the natural logarithm of each number in vNumbers. Handle any potential warnings or errors if numbers were 0 or negative (though not the case here).\n\n\nWhat is the data type of vLog?\n\n\n\nvNumbers &lt;- 1:50\nsum(vNumbers[vNumbers %% 2 == 0])\n#&gt; [1] 650\nvLog &lt;- log(vNumbers)\ntypeof(vLog)\n#&gt; [1] \"double\"",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-1",
    "href": "extra_exercises.html#section-1",
    "title": "\n28  Extra exercises\n",
    "section": "\n29.2 (2)\n",
    "text": "29.2 (2)\n\n\n\nCreate a 5x5 matrix mRandom with elements drawn from a uniform distribution between -10 and 10. Set the seed to 42.\n\n\nReplace all negative values in mRandom with NA.\n\n\nCalculate the sum of each row, ignoring NA values.\n\n\nCalculate the mean of each column, ignoring NA values.\n\n\n\nset.seed(42)\nmRandom &lt;- matrix(runif(25, -10, 10), 5, 5)\nmRandom[mRandom &lt; 0] &lt;- NA\nrowSums(mRandom, na.rm = TRUE)\n#&gt; [1] 25.558958 27.420048 18.471280 18.682163  8.142861\ncolMeans(mRandom, na.rm = TRUE)\n#&gt; [1] 6.620373 3.088707 6.537845 6.523825 8.930609",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-2",
    "href": "extra_exercises.html#section-2",
    "title": "\n28  Extra exercises\n",
    "section": "\n29.3 (3)\n",
    "text": "29.3 (3)\n\n\nWrite an R script that does the following:\n\nInitializes a variable dTotalSum to 0.\nUses a for loop to iterate from 1 to 100.\nIn each iteration, if the current number i is a multiple of 3 or a multiple of 5, add i to dTotalSum.\nPrint the final dTotalSum.\n\n\n\n\ndTotalSum &lt;- 0\nfor (i in 1:100) {\n  if (i %% 3 == 0 | i %% 5 == 0) {\n    dTotalSum &lt;- dTotalSum + i\n  }\n}\ndTotalSum\n#&gt; [1] 2418",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-3",
    "href": "extra_exercises.html#section-3",
    "title": "\n28  Extra exercises\n",
    "section": "\n29.4 (4)\n",
    "text": "29.4 (4)\n\n\nCreate a list lMyList with the following named elements:\n\n\nsName: Your name (as a string).\n\nvLuckyNumbers: A vector of your 3 favorite numbers.\n\nmMatrixA: A 2x2 identity matrix.\n\ndfInnerData: A data frame with two columns: Month (Jan, Feb, Mar) and Rainfall_mm (50, 30, 65).\n\n\nAccess and print the Rainfall_mm for February.\nAdd a new element to the list called sCourse with the value “PQE”.\n\n\nlMyList &lt;- list(\n  sName = \"MyName\",\n  vLuckyNumbers = c(1, 3, 5),\n  mMatrixA = identity(2),\n  dfInnerData = data.frame(\n    Month = c(\"Jan\", \"Feb\", \"Mar\"),\n    Rainfall_mm = c(50, 30, 65)\n  )\n)\nlMyList$dfInnerData$Rainfall_mm[lMyList$dfInnerData$Month == \"Feb\"]\n#&gt; [1] 30\nlMyList$sCourse &lt;- \"PQE\"",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-4",
    "href": "extra_exercises.html#section-4",
    "title": "\n28  Extra exercises\n",
    "section": "\n29.5 (5)\n",
    "text": "29.5 (5)\n\n\n\nCreate a data frame dfStudents with 5 students and 3 variables: StudentID (101 to 105), Grade (A, B, C, A, B), and Age (20, 21, 20, 22, 21).\n\n\nConvert the Grade column to an ordered factor with levels C &lt; B &lt; A.\n\n\nAdd a new column Pass which is TRUE if Grade is A or B, and FALSE otherwise.\n\n\nCreate a subset of dfStudents containing only students older than 20.\n\n\n\ndfStudents &lt;- data.frame(\n  StudentID = 101:105,\n  Grade = c(\"A\", \"B\", \"C\", \"A\", \"B\"),\n  Age = c(20, 21, 20, 22, 21)\n)\ndfStudents$Grade &lt;- factor(dfStudents$Grade, levels = c(\"C\", \"B\", \"A\"), ordered = TRUE)\ndfStudents$Pass &lt;- ifelse(dfStudents$Grade == \"A\" | dfStudents$Grade == \"B\", TRUE, FALSE)\ndfStudents[dfStudents$Age &gt; 20, ]\n#&gt;   StudentID Grade Age Pass\n#&gt; 2       102     B  21 TRUE\n#&gt; 4       104     A  22 TRUE\n#&gt; 5       105     B  21 TRUE",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-5",
    "href": "extra_exercises.html#section-5",
    "title": "\n28  Extra exercises\n",
    "section": "\n29.6 (6)\n",
    "text": "29.6 (6)\n\n\nUsing ifelse(), create a vector vSign of length 20. For each element i from -9 to 10, vSign[i+10] should be -1 if i is negative, 0 if i is zero, and 1 if i is positive. (Adjust indexing as R vectors are 1-indexed).\n\n\nvI &lt;- -9:10\nvSign &lt;- ifelse(vI &lt; 0, -1, ifelse(vI == 0, 0, 1))\nvSign\n#&gt;  [1] -1 -1 -1 -1 -1 -1 -1 -1 -1  0  1  1  1  1  1  1  1  1  1  1",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-6",
    "href": "extra_exercises.html#section-6",
    "title": "\n28  Extra exercises\n",
    "section": "\n29.7 (7)\n",
    "text": "29.7 (7)\n\n\nWrite a while loop that simulates flipping a fair coin (0 for Tails, 1 for Heads) until 3 consecutive Heads are observed. Count and print the total number of flips. Set seed to 101.\n\n\nset.seed(101)\niFlipCount &lt;- 0\niConsecHeads &lt;- 0\nwhile (iConsecHeads &lt; 3) {\n  iFlip &lt;- sample(0:1, 1)\n  iFlipCount &lt;- iFlipCount + 1\n  if (iFlip == 1) {\n    iConsecHeads &lt;- iConsecHeads + 1\n  } else {\n    iConsecHeads &lt;- 0\n  }\n}\niFlipCount\n#&gt; [1] 36",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-7",
    "href": "extra_exercises.html#section-7",
    "title": "\n28  Extra exercises\n",
    "section": "\n29.8 (8)\n",
    "text": "29.8 (8)\n\n\n\nCreate a character vector vsFruits with elements: “apple”, “banana”, “cherry”, NA, “date”.\n\n\nHow many missing values are in vsFruits?\n\n\nCreate a new vector vsFruitsClean by removing NA values from vsFruits.\n\n\nFor each fruit in vsFruitsClean, print the fruit name and its number of characters using nchar().\n\n\n\nvsFruits &lt;- c(\"apple\", \"banana\", \"cherry\", NA, \"date\")\nsum(is.na(vsFruits))\n#&gt; [1] 1\nvsFruitsClean &lt;- vsFruits[which(!is.na(vsFruits))]\nfor (fruit in vsFruitsClean) {\n  print(fruit)\n  print(nchar(fruit))\n}\n#&gt; [1] \"apple\"\n#&gt; [1] 5\n#&gt; [1] \"banana\"\n#&gt; [1] 6\n#&gt; [1] \"cherry\"\n#&gt; [1] 6\n#&gt; [1] \"date\"\n#&gt; [1] 4",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-8",
    "href": "extra_exercises.html#section-8",
    "title": "\n28  Extra exercises\n",
    "section": "\n29.9 (9)\n",
    "text": "29.9 (9)\n\n\nExplain the difference between & and &&, and between | and || in R. Provide a small example where && or || would be preferred over & or | due to short-circuiting, especially when dealing with potentially undefined objects or expensive computations.\n\n\n#`&` and `|` are element-wise. `&&` and `||` are short-circuiting, evaluating only the first element. Example:\n\nx &lt;- NULL\n# No error\nif (FALSE && x[1] &gt; 5) {\n  print(\"Not evaluated\")\n} else {\n  print(\"OK\")\n}\n# Error if x[1] is problematic\nif (FALSE & x[1] &gt; 5) {\n  print(\"Error here\")\n}",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-9",
    "href": "extra_exercises.html#section-9",
    "title": "\n28  Extra exercises\n",
    "section": "\n29.10 (10)\n",
    "text": "29.10 (10)\n\n\nConsider the matrix mA &lt;- matrix(1:12, nrow=4, byrow=TRUE).\nExtract the second row.\nExtract the third column.\nExtract the element in the 1st row and 2nd column.\nCreate a new matrix mB by selecting rows 1 and 3, and columns 2 and 3 from mA.\n\n\nmA &lt;- matrix(1:12, nrow = 4, byrow = TRUE)\nmA[2, ]\n#&gt; [1] 4 5 6\nmA[, 3]\n#&gt; [1]  3  6  9 12\nmA[1, 2]\n#&gt; [1] 2\nmB &lt;- mA[c(1, 3), c(2, 3)]\nmB\n#&gt;      [,1] [,2]\n#&gt; [1,]    2    3\n#&gt; [2,]    8    9",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-10",
    "href": "extra_exercises.html#section-10",
    "title": "\n28  Extra exercises\n",
    "section": "\n30.1 (1)\n",
    "text": "30.1 (1)\n\n\nWrite an R function fCalculateStats that takes a numeric vector as input.\nThe function should return a list containing:\n\n\ndMean: The mean of the vector.\n\ndMedian: The median of the vector.\n\ndSD: The standard deviation of the vector.\n\niNAs: The number of NA values in the vector.\n\n\nThe function should handle NA values gracefully by removing them before calculating mean, median, and SD.\nTest your function with vTest &lt;- c(1, 5, NA, 7, 3, NA, 8).\n\n\nfCalculateStats &lt;- function(vInput) {\n  outputList &lt;- list(\n    dMean = mean(vInput, na.rm = TRUE),\n    dMedian = median(vInput, na.rm = TRUE),\n    dSD = sqrt(var(vInput, na.rm = TRUE)),\n    iNAs = sum(is.na(vInput))\n  )\n  return(outputList)\n}\nvTest &lt;- c(1, 5, NA, 7, 3, NA, 8)\nfCalculateStats(vTest)\n#&gt; $dMean\n#&gt; [1] 4.8\n#&gt; \n#&gt; $dMedian\n#&gt; [1] 5\n#&gt; \n#&gt; $dSD\n#&gt; [1] 2.863564\n#&gt; \n#&gt; $iNAs\n#&gt; [1] 2",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-11",
    "href": "extra_exercises.html#section-11",
    "title": "\n28  Extra exercises\n",
    "section": "\n30.2 (2)\n",
    "text": "30.2 (2)\n\n\nConsider the following R code:\n\n\nx &lt;- 10\nmy_func &lt;- function(y) {\n  x &lt;- 5\n  return(x + y)\n}\nz &lt;- my_func(3)\n\n\nWhat are the values of x and z in the global environment after this code is executed? Explain R’s scoping rules in this context.\n\nThe value of z is 8, and the value of x is 10. x is local within the function.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-12",
    "href": "extra_exercises.html#section-12",
    "title": "\n28  Extra exercises\n",
    "section": "\n30.3 (3)\n",
    "text": "30.3 (3)\n\n\nWrite a function fSumSquaresUpToN that calculates the sum of squares of integers from 1 up to n (i.e., \\(1^2 + 2^2 + ... + n^2\\)).\nImplement this first using a for loop.\nImplement this second using a vectorized approach (without an explicit loop).\nUse system.time() or microbenchmark() to compare the execution time of both versions for n = 10000.\n\n\nsuppressMessages(library(microbenchmark))\n#&gt; Warning: pakke 'microbenchmark' blev bygget under R version 4.3.3\n\nfSumSquaresUpToN &lt;- function(iN) {\n  iSumSquares &lt;- 0\n  for (i in 1:iN) {\n    iSumSquares &lt;- iSumSquares + i^2\n  } \n}\n\nfSumSquaresVector &lt;- function(iN) {\n  return(sum((1:iN)^2))\n}\n\nmicrobenchmark(fSumSquaresVector(10000), fSumSquaresUpToN(10000))\n#&gt; Unit: microseconds\n#&gt;                      expr   min     lq    mean median  uq    max neval\n#&gt;  fSumSquaresVector(10000)  24.4  56.85  73.568  58.45  61 1557.5   100\n#&gt;   fSumSquaresUpToN(10000) 222.8 224.50 249.487 225.95 227 2233.7   100",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-13",
    "href": "extra_exercises.html#section-13",
    "title": "\n28  Extra exercises\n",
    "section": "\n30.4 (4)\n",
    "text": "30.4 (4)\n\n\nWrite a recursive R function fRecursiveFactorial to calculate the factorial of a non-negative integer n. Include basic error checking for negative inputs.\n\n\nfRecursiveFactorial &lt;- function(n) {\n  if (n &lt; 0) stop(\"Input must be non-negative\")\n  if (n == 0) return(1)\n  else return(n * fRecursiveFactorial(n - 1))\n}\nfRecursiveFactorial(5)\n#&gt; [1] 120",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-14",
    "href": "extra_exercises.html#section-14",
    "title": "\n28  Extra exercises\n",
    "section": "\n30.5 (5)\n",
    "text": "30.5 (5)\n\n\nWrite a function fApplyToMatrix that takes a matrix and a function FUN as arguments.\nThe function should apply FUN to each column of the matrix using sapply or apply.\nTest it by creating a 5x3 matrix of random numbers and applying the mean function to its columns.\n\n\nfApplyToMatrix &lt;- function(mInput, FUN) {\n  return(apply(mInput, 2, FUN))\n}\n\nmA &lt;- matrix(rnorm(15), 5, 3)\nfApplyToMatrix(mA, mean)\n#&gt; [1]  0.360807854 -0.005839041 -0.312895340",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-15",
    "href": "extra_exercises.html#section-15",
    "title": "\n28  Extra exercises\n",
    "section": "\n30.6 (6)\n",
    "text": "30.6 (6)\n\n\nExplain the purpose of the ellipsis (...) argument in R functions. Provide an example of a simple function you write that uses ... to pass arguments to an internal R function (e.g., plot or mean).\n\nIt passes additional arguments.\n\nfCalculateMean &lt;- function(vInput, ...) {\n  return(mean(vInput, ...))\n}\nvX &lt;- c(1, 2, NA, 5)\nfCalculateMean(vX, na.rm = TRUE)\n#&gt; [1] 2.666667\n\nmyPlotWrapper &lt;- function(x, y, ...) {\nplot(x, y, main = \"My Plot Wrapper\", ...)\n}\nmyPlotWrapper(1:5, (1:5)^2, xlab = \"X-Values\", col = \"blue\")",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-16",
    "href": "extra_exercises.html#section-16",
    "title": "\n28  Extra exercises\n",
    "section": "\n30.7 (7)\n",
    "text": "30.7 (7)\n\n\nCreate two numeric vectors, vA = 1:1000000 and vB = rnorm(1000000).\nCompare the time it takes to calculate their element-wise product using:\n\nA for loop.\nThe vectorized * operator.\n\n\nUse microbenchmark for the comparison.\n\n\nvA &lt;- 1:1000000\nvB &lt;- rnorm(1000000)\n\nfLoopApproach &lt;- function(vA, vB) {\n  iProduct &lt;- numeric(length(vA))\n  for (i in 1:length(vA)) {\n    iProduct[i] &lt;- vA[i] * vB[i]\n  }\n}\n\nfVectorApproach &lt;- function(vA, vB) {\n  return(vA * vB)\n}\n\n# microbenchmark(fLoopApproach(vA, vB), fVectorApproach(vA, vB))",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-17",
    "href": "extra_exercises.html#section-17",
    "title": "\n28  Extra exercises\n",
    "section": "\n30.8 (8)\n",
    "text": "30.8 (8)\n\n\nYou are given a list lData where each element is a numeric vector of varying lengths, possibly containing NAs.\n\n\nset.seed(123)\nlData &lt;- list(rnorm(10), c(NA, rnorm(5)), rnorm(15, mean=5))\n\n\nWrite code using lapply to return a new list where each element is the mean of the corresponding vector in lData, after removing NAs.\n\n\nlDataClean &lt;- lapply(lData, mean, na.rm = TRUE)\nlDataClean\n#&gt; [[1]]\n#&gt; [1] 0.07462564\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 0.3079017\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 4.753408",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-18",
    "href": "extra_exercises.html#section-18",
    "title": "\n28  Extra exercises\n",
    "section": "\n30.9 (9)\n",
    "text": "30.9 (9)\n\n\nWrite a function fMatrixRedimension that first initializes an empty vector vResult &lt;- c().\nThen, in a loop that runs 10,000 times, it appends the loop index i to vResult using vResult &lt;- c(vResult, i).\nWrite a second function fMatrixPreallocate that first initializes vResult &lt;- numeric(10000).\nThen, in a loop that runs 10,000 times, it assigns vResult[i] &lt;- i.\nCompare the execution time of these two functions. Explain the difference.\n\n\nfMatrixRedimension &lt;- function(iN) {\n  vResult &lt;- c()\n  for (i in 1:iN) {\n    vResult &lt;- c(vResult, i)\n  }\n}\n\nfMatrixPreallocate &lt;- function(iN) {\n  vResult &lt;- numeric(10000)\n  for (i in 1:iN) {\n    vResult[i] &lt;- i\n  }\n}\n\n# microbenchmark(fMatrixRedimension(10000), fMatrixPreallocate(10000))",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-19",
    "href": "extra_exercises.html#section-19",
    "title": "\n28  Extra exercises\n",
    "section": "\n30.10 (10)\n",
    "text": "30.10 (10)\n\n\nWhen might you prefer using ifelse() over a standard if-else structure within a loop? Provide a small example.\n\nUse ifelse() for vectorized conditions. Loop if-else for scalar/complex logic per iteration.\n\nx &lt;- -2:2\nifelse(x &gt; 0, \"positive\", \"negative\")\n#&gt; [1] \"negative\" \"negative\" \"negative\" \"positive\" \"positive\"",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-20",
    "href": "extra_exercises.html#section-20",
    "title": "\n28  Extra exercises\n",
    "section": "\n31.1 (1)\n",
    "text": "31.1 (1)\n\n\n\nSimulating a Biased Die Roll:\n\nWrite an R function fSimulateBiasedDie to simulate rolling a 6-sided die where the probability of rolling a ‘6’ is 0.3, and the probabilities of rolling ‘1’ through ‘5’ are equal (each 0.14).\nThe function should take an integer n_rolls as input and return a vector of n_rolls simulated outcomes.\nUse the discrete random variable simulation method (using runif and comparing to CDF).\nGenerate 10,000 rolls and plot a histogram of the results.\n\n\n\n\nfSimulateBiasedDie &lt;- function(n_rolls) {\n  probs &lt;- c(rep(0.14, 5), 0.3)\n  outcomes &lt;- 1:6\n  CDF &lt;- cumsum(probs)\n  \n  vRolls &lt;- numeric(n_rolls)\n  \n  for (i in 1:n_rolls) {\n    u &lt;- runif(1)\n    vRolls[i] &lt;- outcomes[min(which(u &lt;= CDF))]\n  }\n  return(vRolls)\n}\n\nvOutcomes &lt;- fSimulateBiasedDie(10000)\n\nhist(vOutcomes, breaks = 0.5:6.5)",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-21",
    "href": "extra_exercises.html#section-21",
    "title": "\n28  Extra exercises\n",
    "section": "\n31.2 (2)\n",
    "text": "31.2 (2)\n\n\n\nInverse Transform Sampling for a Custom Distribution:\n\nA random variable X has a probability density function (PDF) \\(f(x) = 3x^2\\) for \\(0 \\le x \\le 1\\), and \\(f(x)=0\\) otherwise.\n\nFind the cumulative distribution function (CDF), \\(F(x)\\).\n\n\nFind the inverse CDF, \\(F^{-1}(u)\\).\n\n\nWrite an R function fSimulateCustom to generate n random samples from this distribution using the inverse transform method.\n\n\nGenerate 10,000 samples and plot a histogram. Superimpose the theoretical PDF on the histogram.\n\n\n\n\nThe CDF is \\(F(x)=x^3\\). So the inverse CDF is \\(F^{-1}(u)=u^{1/3}\\).\n\nfSimulateCustom &lt;- function(size) {\n  U &lt;- runif(size)\n  return(U^(1/3))\n}\n\nset.seed(123)\nvX &lt;- fSimulateCustom(10000)\n\nhist(vX, \n     freq = FALSE,\n     breaks = 15,\n     col = \"cornflowerblue\", \n     xlab = \"\",\n     ylab = \"Density\",\n     main = \"\",\n     xlim = c(0, 1))\n\ncurve(3 * x^2,\n      from = 0,\n      to = 1,\n      col = \"red\",      \n      lwd = 2,\n      add = TRUE)",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-22",
    "href": "extra_exercises.html#section-22",
    "title": "\n28  Extra exercises\n",
    "section": "\n31.3 (3)\n",
    "text": "31.3 (3)\n\n\n\nAcceptance-Rejection Method for a Truncated Normal:\n\nYou want to simulate from a standard normal distribution truncated to the interval [0, 2]. The PDF is \\(f(x) = \\frac{\\phi(x)}{\\Phi(2)-\\Phi(0)}\\) for \\(0 \\le x \\le 2\\), where \\(\\phi\\) is the standard normal PDF and \\(\\Phi\\) is the standard normal CDF.\nUse a uniform distribution on [0, 2] as the proposal distribution \\(g(x) = 1/2\\) for \\(0 \\le x \\le 2\\).\n\nDetermine the constant \\(c\\) such that \\(f(x) \\le c \\cdot g(x)\\) for all \\(x \\in [0, 2]\\).\n\n\nWrite an R function fSimulateTruncNormAR using the acceptance-rejection method to generate n samples from this truncated normal distribution.\n\n\nGenerate 10,000 samples. Plot a histogram and superimpose the theoretical PDF.\n\n\nWhat is the acceptance rate of your sampler?\n\n\n\n\n\n\n\n\\[\nc = \\frac{\\phi(0)}{\\Phi(2)-\\Phi(0)} / \\frac{1}{2-0} \\approx 1.6716\n\\]\n\nfTruncNormSim &lt;- function(size) {\n  \n  c &lt;- (dnorm(0) / (pnorm(2) - pnorm(0))) / 0.5\n  \n  U &lt;- rep(NA, size)\n  Y &lt;- rep(NA, size)\n  X &lt;- rep(NA, size)\n  Unaccepted &lt;- rep(TRUE, size)\n  \n  while (any(Unaccepted)) {\n    \n    UnacceptedCount &lt;- sum(Unaccepted)\n    \n    U &lt;- runif(UnacceptedCount, 0, 1)\n    Y &lt;- runif(UnacceptedCount, 0, 2)\n    \n    Accepted_ThisTime &lt;- Unaccepted[Unaccepted] & (U &lt;= ((dnorm(Y) / (pnorm(2) - pnorm(0))) / runif(Y, 0, 2) / c))\n    \n    X[Unaccepted][Accepted_ThisTime] &lt;- Y[Accepted_ThisTime]\n    Unaccepted[Unaccepted] &lt;- !Accepted_ThisTime\n    \n  }\n  \n  return(X)\n  \n}\n\nset.seed(1)\nX &lt;- fTruncNormSim(10000)\nhist(X, freq = FALSE, col = \"cornflowerblue\", xlim = c(0, 2), breaks = 50, main = \"Truncated Normal (A-R)\")\n\ncurve(dnorm(x) / (pnorm(2) - pnorm(0)), add = TRUE, col = \"red\", from = 0, to = 2)",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-23",
    "href": "extra_exercises.html#section-23",
    "title": "\n28  Extra exercises\n",
    "section": "\n31.4 (4)\n",
    "text": "31.4 (4)\n\n\n\nBox-Muller Transform:\n\n\nWrite an R function fMyBoxMuller that implements the Box-Muller algorithm to generate n_pairs of independent standard normal random variables. The function should return a list or a matrix with two columns/elements.\n\n\nGenerate 10,000 standard normal random variables (i.e., call your function to generate 5,000 pairs). Combine them into a single vector.\n\n\nPlot a histogram of these 10,000 variables and superimpose the standard normal PDF.\n\n\nCreate a scatter plot of the pairs of normal variables generated. What pattern do you expect?\n\n\n\n\n\nfMyBoxMuller &lt;- function(size = 1) {\n  U &lt;- runif(size)\n  V &lt;- runif(size)\n  X &lt;- sqrt(-2*log(U)) * cos(2*pi*V)\n  Y &lt;- sqrt(-2*log(U)) * sin(2*pi*V)\n  return(matrix(c(X,Y), ncol = 2))\n}\n\nmX &lt;- fMyBoxMuller(5000)\nvX &lt;- as.numeric(mX)\n\nhist(vX, \n     freq = FALSE,\n     breaks = 15,\n     col = \"cornflowerblue\", \n     xlab = \"\",\n     ylab = \"Density\",\n     main = \"\",\n     xlim = c(-2, 2))\n\ncurve(dnorm(x),\n      from = -2,\n      to = 2,\n      col = \"red\",      \n      lwd = 2,\n      add = TRUE)\n\n\n\n\n\n\n\n# Expected: circular pattern centered at (0,0)\nplot(mX[, 1], mX[, 2], pch = \".\")",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-24",
    "href": "extra_exercises.html#section-24",
    "title": "\n28  Extra exercises\n",
    "section": "\n31.5 (5)\n",
    "text": "31.5 (5)\n\n\n\nMonte Carlo Integration:\n\n\nEstimate the value of \\(\\int_{0}^{1} e^{x^2} dx\\) using Monte Carlo integration with 10,000 uniform random samples from [0, 1].\n\n\nThe integrate() function in R can compute this numerically. Compare your Monte Carlo estimate to the result from integrate(function(x) exp(x^2), lower = 0, upper = 1).\n\n\nHow could you improve the accuracy of your Monte Carlo estimate?\n\n\n\n\n\nMonteCarlo.Integration &lt;- function(f, n, a, b) {\n  U &lt;- runif(n, min = a, max = b)\n  return( (b-a)*mean(f(U)) )\n}\nset.seed(10086)\nMonteCarlo.Integration(function(x) exp(x^2), 10000, 0, 1)\n#&gt; [1] 1.45782\n\nintegrate(function(x) exp(x^2), lower = 0, upper = 1)\n#&gt; 1.462652 with absolute error &lt; 1.6e-14\n\n# Improve by increasing the number of samples.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-25",
    "href": "extra_exercises.html#section-25",
    "title": "\n28  Extra exercises\n",
    "section": "\n31.6 (6)\n",
    "text": "31.6 (6)\n\n\n\nSimulating a Poisson Distribution:\n\nThe CDF of a Poisson distribution with rate \\(\\lambda\\) is \\(F(k; \\lambda) = \\sum_{i=0}^{k} \\frac{e^{-\\lambda}\\lambda^i}{i!}\\).\nWrite an R function fSimulatePoissonDiscrete using the method for simulating discrete random variables (based on runif and the CDF) to generate n samples from a Poisson distribution with \\(\\lambda = 3\\).\n(Hint: You can use ppois to get the CDF values, or implement the sum yourself).\nGenerate 1,000 samples and compare its histogram to the one generated by rpois(1000, lambda = 3).\n\n\n\n\nfSimulatePoissonDiscrete &lt;- function(F, size, ...) {\n  m &lt;- 0\n  U &lt;- runif(size)\n  X &lt;- rep(NA, size)\n  X[F(0, ...) &gt;= U] &lt;- 0\n  while (any(F(m, ...) &lt; U)) {\n    m &lt;- m + 1\n    X[(F(m, ...) &gt;= U) & (F(m - 1, ...) &lt; U)] &lt;- m\n  }\n  return(X)\n}\n\nfPoissonCDF &lt;- function(size, lambda) {\n  dSum &lt;- 0\n  for (i in 0:size) {\n    dSum &lt;- dSum + (exp(-lambda) * lambda^i) / factorial(i)\n  }\n  return(dSum)\n}\n\nset.seed(10086)\nvX &lt;- fSimulatePoissonDiscrete(fPoissonCDF, size = 1000, lambda = 3)\n\npar(mfrow=c(1,2))\nhist(vX, \n     breaks = 0:max(vX),\n     col = \"cornflowerblue\", \n     main = \"Custom Poisson\")\nhist(rpois(1000, lambda = 3), \n     breaks = 0:max(vX),\n     col = \"cornflowerblue\", \n     main = \"R's rpois\")\n\n\n\n\n\n\npar(mfrow=c(1,1))",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-26",
    "href": "extra_exercises.html#section-26",
    "title": "\n28  Extra exercises\n",
    "section": "\n31.7 (7)\n",
    "text": "31.7 (7)\n\n\n\nSeeding and Reproducibility:\n\n\nGenerate a vector of 5 random numbers from a standard normal distribution.\n\n\nGenerate another vector of 5 random numbers from a standard normal distribution. Are they the same as in (a)?\n\n\nSet the seed to 12345. Generate a vector of 5 random numbers from a standard normal distribution.\n\n\nSet the seed to 12345 again. Generate another vector of 5 random numbers from a standard normal distribution. Are they the same as in (c)? Explain why.\n\n\nHow can you save and restore the RNG state in R? Demonstrate with an example.\n\n\n\n\n\nvRand1 &lt;- rnorm(5)\nvRand2 &lt;- rnorm(5)\nvRand1 - vRand2\n#&gt; [1]  0.02333262 -0.90672903 -2.01327089  1.66283561  0.14078765\nset.seed(12345)\nvRand1 &lt;- rnorm(5)\nset.seed(12345)\nRNG.state &lt;- .Random.seed\nvRand2 &lt;- rnorm(5)\nvRand1 - vRand2\n#&gt; [1] 0 0 0 0 0\n.Random.seed &lt;- RNG.state\nvRand3 &lt;- rnorm(5)\nvRand1 - vRand3\n#&gt; [1] 0 0 0 0 0",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-27",
    "href": "extra_exercises.html#section-27",
    "title": "\n28  Extra exercises\n",
    "section": "\n32.1 (1)\n",
    "text": "32.1 (1)\n\n\n\nLoan Repayment Interest Rate:\n\nRecall the loan repayment problem where \\(f(r) = \\frac{A}{P} - \\frac{r(1+r)^N}{(1+r)^N - 1} = 0\\).\nGiven a loan amount \\(P=200000\\), \\(N=240\\) months (20 years), and a monthly repayment \\(A=1500\\).\n\nWrite an R function for \\(f(r)\\) based on these parameters.\n\n\nPlot \\(f(r)\\) for \\(r \\in [0.001, 0.02]\\) to visually inspect for a root.\n\n\nImplement the Bisection method to find the monthly interest rate \\(r\\). Use an initial interval \\([0.001, 0.01]\\) and a tolerance of \\(10^{-7}\\). Print the root and the number of iterations.\n\n\n(Optional) If you have the numDeriv package, calculate the derivative \\(f'(r)\\) numerically.\n\n\n\n\n\nfRepayment &lt;- function(r, A, P, N) {\n  return(A/P - (r * (1+r)^N) / ((1+r)^N - 1))\n}\nvX &lt;- seq(0.001, 0.02, by = 0.0001)\nplot(vX, fRepayment(vX, 1500, 200000, 240))\n\n\n\n\n\n\n\nbisection &lt;- function(f, dX.l, dX.r, dTol = 10e-7, max.iter = 1000, ...) {\n  \n  #check inputs\n  if (dX.l &gt;= dX.r) {\n    cat(\"error: x.l &gt;= x.r \\n\")\n    return(NULL)\n  }\n  f.l &lt;- f(dX.l, ...)\n  f.r &lt;- f(dX.r, ...)\n  if (f.l == 0) {\n    return(dX.l)\n  } else if (f.r == 0) {\n    return(dX.r)\n  } else if (f.l*f.r &gt; 0) {\n    cat(\"error: f(x.l)*f(x.r) &gt; 0 \\n\")\n    return(NULL)\n  }\n  \n  # successively refine x.l and x.r\n  iter &lt;- 0\n  while ((dX.r - dX.l) &gt; dTol && (iter &lt; max.iter)) {\n    dX.m &lt;- (dX.l + dX.r)/2\n    f.m &lt;- f(dX.m, ...)\n    if (f.m == 0) {\n      return(dX.m)\n    } else if (f.l*f.m &lt; 0) {\n      dX.r &lt;- dX.m\n      f.r &lt;- f.m\n    } else {\n      dX.l &lt;- dX.m\n      f.l &lt;- f.m\n    }\n    iter &lt;- iter + 1\n  }\n  cat(\"at iteration\", iter, \"the root lies between\", dX.l, \"and\", dX.r, \"\\n\")\n  # return approximate root\n  return((dX.l + dX.r)/2)\n}\n\nbisection(fRepayment, dX.l = 0.001 , dX.r = 0.01, dTol = 10e-7, A = 1500, P = 200000, N = 240)\n#&gt; at iteration 14 the root lies between 0.005479126 and 0.005479675\n#&gt; [1] 0.005479401",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-28",
    "href": "extra_exercises.html#section-28",
    "title": "\n28  Extra exercises\n",
    "section": "\n32.2 (2)\n",
    "text": "32.2 (2)\n\n\n\nNewton-Raphson Method:\n\nConsider the function \\(f(x) = x^3 - 2x - 5\\).\n\nAnalytically find the first derivative \\(f'(x)\\).\n\n\nImplement the Newton-Raphson method in R to find a root of \\(f(x)\\). The function should take \\(f, f'\\), an initial guess \\(x_0\\), a tolerance, and max iterations as input.\n\n\nUse your function to find a root starting with \\(x_0 = 2\\). Set tolerance to \\(10^{-6}\\) and max iterations to 100.\n\n\nWhat happens if you start with \\(x_0 = 0\\)?\n\n\n\n\nAnalytical derivative: \\(f'(x)=3x^2-2\\)\n\nf &lt;- function(x) {\n  dOut = x^3 - 2 * x - 5\n  return(dOut)\n}\n\nf_prime &lt;- function(x) {\n  dOut = 3 * x^2 - 2\n  return(dOut)\n}\n\nNR &lt;- function(f, f_prime, dX0, dTol = 1e-9, max.iter = 1000, ...) {\n  dX &lt;- dX0\n  fx &lt;- f(dX, ...)\n  iter &lt;- 0\n  while ((abs(fx) &gt; dTol) && (iter &lt; max.iter)) {\n    dX &lt;- dX - f(dX, ...)/f_prime(dX, ...)\n    fx &lt;- f(dX, ...)\n    iter &lt;- iter + 1\n  }\n  if (abs(fx) &gt; dTol) {\n    cat(\"Algorithm failed to converge\\n\")\n    return(NULL)\n  } else {\n    cat(\"Algorithm converged\\n\")\n    cat(\"At iteration \", iter, \"value of x is: \", dX, \"\\n\")\n    return(dX)\n  }\n}\n\nNR(f, f_prime, dTol = 10e-6, dX0 = 2, max.iter = 100)\n#&gt; Algorithm converged\n#&gt; At iteration  3 value of x is:  2.094551\n#&gt; [1] 2.094551\nNR(f, f_prime, dTol = 10e-6, dX0 = 0, max.iter = 100) # Slower to converge. Could find a different root or may even diverge.\n#&gt; Algorithm converged\n#&gt; At iteration  18 value of x is:  2.094551\n#&gt; [1] 2.094551",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-29",
    "href": "extra_exercises.html#section-29",
    "title": "\n28  Extra exercises\n",
    "section": "\n32.3 (3)\n",
    "text": "32.3 (3)\n\n\n\nSecant Method:\n\nConsider the same function \\(f(x) = x^3 - 2x - 5\\).\n\nImplement the Secant method in R to find a root of \\(f(x)\\). The function should take \\(f\\), two initial guesses \\(x_0, x_1\\), a tolerance, and max iterations.\n\n\nUse your function to find a root starting with \\(x_0 = 2, x_1 = 2.5\\). Set tolerance to \\(10^{-6}\\) and max iterations to 100.\n\n\nCompare the number of iterations taken by Newton-Raphson and Secant method for this problem.\n\n\n\n\n\nf &lt;- function(x) {\n  dOut = x^3 - 2 * x - 5\n  return(dOut)\n}\nfSecant &lt;- function(f, dX0, dX1, dTol = 1e-9, max.iter = 1000, ...) {\n  iter &lt;- 0\n  dX2 &lt;- dX1\n  while ((abs(f(dX2, ...)) &gt; dTol) && (iter &lt; max.iter)) {\n    dX2 &lt;- dX1 - f(dX1, ...) * ((dX0 - dX1) / (f(dX0, ...) - f(dX1, ...)))\n    dX0 &lt;- dX1\n    dX1 &lt;- dX2\n    iter &lt;- iter + 1\n  }\n  if (abs(f(dX2, ...)) &gt; dTol) {\n    cat(\"At iteration \", iter, \"value of x is: \", dX1, \"\\n\")\n    return(list(root = NULL, f.root = NULL, iter = iter, \"Algorithm failed to converge. Maximum iterations reached.\"))\n  } else {\n    cat(\"At iteration \", iter, \"value of x is: \", dX1, \"\\n\")\n    return(list(root = dX2, f.root = f(dX2), iterations = iter, \"Convergence reached.\"))\n  }\n}\n\nroot &lt;- fSecant(f, dX0 = 2, dX1 = 2.5, dTol = 10e-6, max.iter = 100)\n#&gt; At iteration  4 value of x is:  2.094551\nroot # 1 more step than Newton-Raphson\n#&gt; $root\n#&gt; [1] 2.094551\n#&gt; \n#&gt; $f.root\n#&gt; [1] -9.612876e-07\n#&gt; \n#&gt; $iterations\n#&gt; [1] 4\n#&gt; \n#&gt; [[4]]\n#&gt; [1] \"Convergence reached.\"\n\nuniroot(f, interval = c(-3,3))\n#&gt; $root\n#&gt; [1] 2.094555\n#&gt; \n#&gt; $f.root\n#&gt; [1] 3.690185e-05\n#&gt; \n#&gt; $iter\n#&gt; [1] 7\n#&gt; \n#&gt; $init.it\n#&gt; [1] NA\n#&gt; \n#&gt; $estim.prec\n#&gt; [1] 6.103516e-05\n\nvX &lt;- seq(1, 3, 0.01)\nplot(vX, f(vX), type = \"l\")\nabline(h = 0, col = \"red\")\nabline(v = root[\"root\"], col = \"blue\", lty = 2)",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-30",
    "href": "extra_exercises.html#section-30",
    "title": "\n28  Extra exercises\n",
    "section": "\n32.4 (4)\n",
    "text": "32.4 (4)\n\n\n\nUsing uniroot:\n\nFor the function \\(f(x) = \\cos(x) - x\\):\n\nPlot the function over an interval (e.g., \\([- \\pi, \\pi]\\)) to identify a bracketing interval for a root.\n\n\nUse the uniroot function to find the root within your chosen interval.\n\n\nWrite your own simple Bisection method function and compare its result and number of iterations to uniroot for the same interval and a similar tolerance.\n\n\n\n\n\nf &lt;- function(x) {\n  return(cos(x) - x)\n}\n\nvX &lt;- seq(-pi, pi, 0.01)\nplot(vX, f(vX), type = \"l\")\n\n\n\n\n\n\n\nuniroot(f, interval = c(-1,2))\n#&gt; $root\n#&gt; [1] 0.7390929\n#&gt; \n#&gt; $f.root\n#&gt; [1] -1.292031e-05\n#&gt; \n#&gt; $iter\n#&gt; [1] 6\n#&gt; \n#&gt; $init.it\n#&gt; [1] NA\n#&gt; \n#&gt; $estim.prec\n#&gt; [1] 6.103516e-05\n\n# See exercise just before this one\nroot &lt;- fSecant(f, dX0 = -1, dX1 = 2, dTol = 10e-6, max.iter = 100)\n#&gt; At iteration  5 value of x is:  0.7390831\nroot\n#&gt; $root\n#&gt; [1] 0.7390831\n#&gt; \n#&gt; $f.root\n#&gt; [1] 3.363451e-06\n#&gt; \n#&gt; $iterations\n#&gt; [1] 5\n#&gt; \n#&gt; [[4]]\n#&gt; [1] \"Convergence reached.\"",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-31",
    "href": "extra_exercises.html#section-31",
    "title": "\n28  Extra exercises\n",
    "section": "\n32.5 (5)\n",
    "text": "32.5 (5)\n\n\n\nNumerical Derivatives with numDeriv:\n\nConsider the function \\(g(x) = e^{\\sin(x)} \\cdot \\ln(x^2+1)\\) for \\(x &gt; 0\\).\n\nIf the numDeriv package is installed, use the grad function to numerically compute the derivative of \\(g(x)\\) at \\(x=1\\), \\(x=2\\), and \\(x=3\\).\n\n\n(Challenge) Try to find the analytical derivative of \\(g(x)\\) and compare its value at \\(x=2\\) with the numerical estimate.\n\n\n\n\n\ng &lt;- function(x) {\n  return(exp(sin(x)) * log(x^2 + 1))\n}\nsuppressMessages(library(numDeriv))\ngrad(g, 1)\n#&gt; [1] 3.188554\ngrad(g, 2)\n#&gt; [1] 0.3233247\ngrad(g, 3)\n#&gt; [1] -1.934098\n\n# Analytically\ngGrad &lt;- function(x) {\n  return((exp(sin(x))*cos(x)*log(x^2+1) + exp(sin(x))*(2*x/(x^2+1))))\n}\ngGrad(2)\n#&gt; [1] 0.3233247",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-32",
    "href": "extra_exercises.html#section-32",
    "title": "\n28  Extra exercises\n",
    "section": "\n32.6 (6)\n",
    "text": "32.6 (6)\n\n\n\nComparing Root-Finding Methods:\n\nFind a root for \\(h(x) = x \\cdot e^x - 1 = 0\\).\n\nPlot the function to find a suitable starting interval/point.\n\n\nSolve using your Bisection method implementation.\n\n\nSolve using your Newton-Raphson method implementation (you’ll need \\(h'(x)\\)).\n\n\nSolve using your Secant method implementation.\n\n\nSolve using uniroot.\n\n\nBriefly compare the methods in terms of requirements (e.g., derivative, interval), convergence speed (number of iterations), and robustness for this specific problem.\n\n\n\n\n\nh &lt;- function(x) {\n  return(x * exp(x) - 1)\n}\nvX &lt;- seq(-2, 2, 0.01)\nplot(vX, h(vX))\n\n\n\n\n\n\nbisection(h, 0, 2)\n#&gt; at iteration 21 the root lies between 0.5671425 and 0.5671434\n#&gt; [1] 0.567143\n\nhGrad &lt;- function(x) {\n  return(x * exp(x) + exp(x))\n}\n\nNR(h, hGrad, 1.5)\n#&gt; Algorithm converged\n#&gt; At iteration  6 value of x is:  0.5671433\n#&gt; [1] 0.5671433\nfSecant(h, 0, 2)\n#&gt; At iteration  8 value of x is:  0.5671433\n#&gt; $root\n#&gt; [1] 0.5671433\n#&gt; \n#&gt; $f.root\n#&gt; [1] -3.581359e-10\n#&gt; \n#&gt; $iterations\n#&gt; [1] 8\n#&gt; \n#&gt; [[4]]\n#&gt; [1] \"Convergence reached.\"\nuniroot(h, c(0, 2))\n#&gt; $root\n#&gt; [1] 0.5671321\n#&gt; \n#&gt; $f.root\n#&gt; [1] -3.093327e-05\n#&gt; \n#&gt; $iter\n#&gt; [1] 8\n#&gt; \n#&gt; $init.it\n#&gt; [1] NA\n#&gt; \n#&gt; $estim.prec\n#&gt; [1] 6.103516e-05",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-33",
    "href": "extra_exercises.html#section-33",
    "title": "\n28  Extra exercises\n",
    "section": "\n33.1 (1)\n",
    "text": "33.1 (1)\n\n\n\nUnivariate Optimization with Newton’s Method:\n\nConsider the function \\(f(x) = x^4 - 14x^3 + 60x^2 - 70x\\).\n\nFind the first \\(f'(x)\\) and second \\(f''(x)\\) derivatives analytically.\n\n\nImplement Newton’s method for optimization to find a local minimum/maximum of \\(f(x)\\). Your function should search for \\(x\\) where \\(f'(x)=0\\).\n\n\nTry to find a local extremum starting from \\(x_0 = 0\\). What do you find?\n\n\nTry to find a local extremum starting from \\(x_0 = 6\\). What do you find?\n\n\nUse \\(f''(x)\\) to classify the extrema found.\n\n\n\n\nAnalytical derivatives: \\(f'(x)=4x^3-42x^2+120x-70\\) and \\(f''(x)=12x^2-84x+120\\).\n\nf &lt;- function(x) {\n  return(x^4 - 14 * x^3 + 60 * x^2 - 70 * x)\n}\n\nfprime &lt;- function(x) {\n  return(4 * x^3 - 42 * x^2 + 120 * x - 70)\n}\n\nfsecond &lt;- function(x) {\n  return(12 * x^2 - 84 * x + 120)\n}\n\nNM &lt;- function(f, f_prime, f_sec, dX0, dTol = 1e-9, n.max = 1000){\n  dX &lt;- dX0\n  fx &lt;- f(dX)\n  fpx &lt;- f_prime(dX)\n  fsx &lt;- f_sec(dX)\n  n &lt;- 0\n  while ((abs(fpx) &gt; dTol) && (n &lt; n.max)) {\n    dX &lt;- dX - fpx/fsx\n    fx &lt;- f(dX)\n    fpx &lt;- f_prime(dX)\n    fsx &lt;- f_sec(dX)\n    n &lt;- n + 1\n  }\n  if (n == n.max) {\n    cat('newton failed to converge\\n')\n  } else {\n    cat(\"At iteration\", n, \"the value of x is:\", dX, \"\\n\")\n    return(dX)\n  }\n}\nx1 &lt;- NM(f, fprime, fsecond, dX0 = 0)\n#&gt; At iteration 5 the value of x is: 0.7808841\nx2 &lt;- NM(f, fprime, fsecond, dX0 = 3)\n#&gt; At iteration 4 the value of x is: 3.761921\nx3 &lt;- NM(f, fprime, fsecond, dX0 = 6)\n#&gt; At iteration 3 the value of x is: 5.957195\nfsecond(x1) # local minimum\n#&gt; [1] 61.7231\nfsecond(x2) # local maximum\n#&gt; [1] -26.17677\nfsecond(x3) # local minimum\n#&gt; [1] 45.45367\nvX &lt;- seq(-2, 8, 0.1)\nplot(vX, f(vX), type = \"l\")",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-34",
    "href": "extra_exercises.html#section-34",
    "title": "\n28  Extra exercises\n",
    "section": "\n33.2 (2)\n",
    "text": "33.2 (2)\n\n\n\nGolden Section Search:\n\nConsider the function \\(g(x) = -(x-2)^2 + 5 \\sin(x)\\) on the interval \\([0, 4]\\).\n\nPlot the function to visualize its behavior.\n\n\nImplement the Golden Section Search algorithm to find the maximum of \\(g(x)\\) within this interval. Set a tolerance of \\(10^{-5}\\).\n\n\nCompare your result with R’s optimize function (remember to set maximum = TRUE).\n\n\n\n\n\ng &lt;- function(x) {\n  return(-(x - 2)^2 + 5 * sin(x))\n}\n\nvX &lt;- seq(0, 4, 0.1)\nplot(vX, g(vX), type = \"l\")\n\n\n\n\n\n\n\ngsection &lt;- function(f, dX.l, dX.r, dX.m, dTol = 1e-9) {\n  \n  # golden ratio plus one\n  dGR1 &lt;- 1 + (1 + sqrt(5))/2\n  \n  # successively refine x.l, x.r, and x.m\n  f.l &lt;- f(dX.l)\n  f.r &lt;- f(dX.r)\n  f.m &lt;- f(dX.m)\n  while ((dX.r - dX.l) &gt; dTol) { \n    if ((dX.r - dX.m) &gt; (dX.m - dX.l)) { # if the right segment is wider than the left \n      dY &lt;- dX.m + (dX.r - dX.m)/dGR1 # put Y into the right segment according to the golden ratio\n      f.y &lt;- f(dY)\n      if (f.y &gt;= f.m) {\n        dX.l &lt;- dX.m\n        f.l &lt;- f.m\n        dX.m &lt;- dY\n        f.m &lt;- f.y\n      } else {\n        dX.r &lt;- dY\n        f.r &lt;- f.y\n      }\n    } else { #if the left segment is wider than the right\n      dY &lt;- dX.m - (dX.m - dX.l)/dGR1 # put Y into the left segment according to the golden ratio\n      f.y &lt;- f(dY)\n      if (f.y &gt;= f.m) {\n        dX.r &lt;- dX.m\n        f.r &lt;- f.m\n        dX.m &lt;- dY\n        f.m &lt;- f.y\n      } else {\n        dX.l &lt;- dY\n        f.l &lt;- f.y\n      }\n    }\n  }\n  return(dX.m)\n}\ngsection(g, dX.l = 0, dX.r = 4, dX.m = 2, dTol = 10e-5)\n#&gt; [1] 1.693623\noptimize(g, interval = c(0, 4), maximum = TRUE)\n#&gt; $maximum\n#&gt; [1] 1.693664\n#&gt; \n#&gt; $objective\n#&gt; [1] 4.868465",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-35",
    "href": "extra_exercises.html#section-35",
    "title": "\n28  Extra exercises\n",
    "section": "\n33.3 (3)\n",
    "text": "33.3 (3)\n\n\n\nMultivariate Optimization: Steepest Ascent:\n\nConsider the function \\(h(x, y) = -(x-1)^2 - 2(y-2)^2 + xy\\).\n\nAnalytically compute the gradient \\(\\nabla h(x,y)\\).\n\n\nImplement the Steepest Ascent method. For line search, you can use a fixed small step size initially or implement a simple line search (e.g., by testing a few step sizes or adapting the Golden Section search for the 1D problem along the gradient direction).\n\n\nFind the maximum starting from \\((x_0, y_0) = (0,0)\\). Set tolerance for gradient norm to \\(10^{-4}\\).\n\n\n\n\n\nh &lt;- function(vX) {\n  return(-(vX[1] - 1)^2 - 2 * (vX[2] - 2)^2 + vX[1] * vX[2])\n}\n\nhgrad &lt;- function(vX) {\n  h1 &lt;- -2 * (vX[1] - 1) + vX[2]\n  h2 &lt;- -4 * (vX[2] - 2) + vX[1]\n  return(c(h1, h2))\n}\n\n# golden section algorithm from last week\ngsection &lt;- function(f, dX.l, dX.r, dX.m, dTol = 1e-9) {\n  \n  # golden ratio plus one\n  dGR1 &lt;- 1 + (1 + sqrt(5))/2\n  \n  # successively refine x.l, x.r, and x.m\n  f.l &lt;- f(dX.l)\n  f.r &lt;- f(dX.r)\n  f.m &lt;- f(dX.m)\n  while ((dX.r - dX.l) &gt; dTol) { \n    if ((dX.r - dX.m) &gt; (dX.m - dX.l)) { # if the right segment is wider than the left \n      dY &lt;- dX.m + (dX.r - dX.m)/dGR1 # put Y into the right segment according to the golden ratio\n      f.y &lt;- f(dY)\n      if (f.y &gt;= f.m) {\n        dX.l &lt;- dX.m\n        f.l &lt;- f.m\n        dX.m &lt;- dY\n        f.m &lt;- f.y\n      } else {\n        dX.r &lt;- dY\n        f.r &lt;- f.y\n      }\n    } else { #if the left segment is wider than the right\n      dY &lt;- dX.m - (dX.m - dX.l)/dGR1 # put Y into the left segment according to the golden ratio\n      f.y &lt;- f(dY)\n      if (f.y &gt;= f.m) {\n        dX.r &lt;- dX.m\n        f.r &lt;- f.m\n        dX.m &lt;- dY\n        f.m &lt;- f.y\n      } else {\n        dX.l &lt;- dY\n        f.l &lt;- f.y\n      }\n    }\n  }\n  return(dX.m)\n}\n\nline.search &lt;- function(f, vX, vG, dTol = 1e-9, dA.max = 2^5) {\n  # f is a real function that takes a vector of length d\n  # x and y are vectors of length d\n  # line.search uses gsection to find a &gt;= 0 such that\n  # g(a) = f(x + a*y) has a local maximum at a,\n  # within a tolerance of tol\n  # if no local max is found then we use 0 or a.max for a\n  # the value returned is x + a*y\n  if (sum(abs(vG)) == 0){\n    return(vX) # +0*vG\n  } # g(a) constant\n  g &lt;- function(dA){\n    return(f(vX + dA*vG)) \n  }\n  # find a triple a.l &lt; a.m &lt; a.r such that\n  # g(a.l) &lt;= g(a.m) and g(a.m) &gt;= g(a.r)\n  \n  # choose a.l\n  dA.l &lt;- 0\n  g.l &lt;- g(dA.l)\n  # find a.m\n  dA.m &lt;- 1\n  g.m &lt;- g(dA.m)\n  while ((g.m &lt; g.l) & (dA.m &gt; dTol)) {\n    dA.m &lt;- dA.m/2\n    g.m &lt;- g(dA.m)\n  }\n  # if a suitable a.m was not found then use 0 for a, so just return vX as the next step\n  if ((dA.m &lt;= dTol) & (g.m &lt; g.l)){\n    return(vX)\n  } \n  # find a.r\n  dA.r &lt;- 2*dA.m\n  g.r &lt;- g(dA.r)\n  while ((g.m &lt; g.r) & (dA.r &lt; dA.max)) {\n    dA.m &lt;- dA.r\n    g.m &lt;- g.r\n    dA.r &lt;- 2*dA.m\n    g.r &lt;- g(dA.r)\n  }\n  # if a suitable a.r was not found then use a.max for a\n  if ((dA.r &gt;= dA.max) & (g.m &lt; g.r)){\n    return(vX + dA.max*vG)\n  } \n  # apply golden-section algorithm to g to find a\n  dA &lt;- gsection(g, dA.l, dA.r, dA.m)\n  return(vX + dA*vG)\n}\n\nascent &lt;- function(f, grad.f, vX0, dTol = 1e-9, n.max = 100) {\n  vX.old &lt;- vX0\n  vX &lt;- line.search(f, vX0, grad.f(vX0))\n  n &lt;- 1\n  while ((f(vX) - f(vX.old) &gt; dTol) & (n &lt; n.max)) {\n    vX.old &lt;- vX\n    vX &lt;- line.search(f, vX, grad.f(vX))\n    n &lt;- n + 1\n  }\n  cat(\"at iteration\", n, \"the coordinates of x are\", vX, \"\\n\")\n  return(vX)\n}\n\nascent(h, hgrad, vX0 = c(0, 0))\n#&gt; at iteration 16 the coordinates of x are 2.285706 2.57142\n#&gt; [1] 2.285706 2.571420",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-36",
    "href": "extra_exercises.html#section-36",
    "title": "\n28  Extra exercises\n",
    "section": "\n33.4 (4)\n",
    "text": "33.4 (4)\n\n\n\nMultivariate Optimization: Newton’s Method in Higher Dimensions:\n\nFor the same function \\(h(x, y) = -(x-1)^2 - 2(y-2)^2 + xy\\) from Q3:\n\nAnalytically compute the Hessian matrix \\(H(x,y)\\).\n\n\nImplement Newton’s method for multivariate optimization.\n\n\nFind the maximum starting from \\((x_0, y_0) = (0,0)\\). Set tolerance for gradient norm to \\(10^{-4}\\).\n\n\nCompare the number of iterations with the Steepest Ascent method.\n\n\n\n\n\nhcomb &lt;- function(vX) {\n  h &lt;- -(vX[1] - 1)^2 - 2 * (vX[2] - 2)^2 + vX[1] * vX[2]\n  h1 &lt;- -2 * (vX[1] - 1) + vX[2]\n  h2 &lt;- -4 * (vX[2] - 2) + vX[1]\n  h11 &lt;- -2\n  h12 &lt;- 1\n  h22 &lt;- -4\n  return(list(h, c(h1, h2), matrix(c(h11, h12, h12, h22), 2, 2)))\n}\n\nnewton &lt;- function(f3, vX0, dTol = 1e-9, n.max = 100) {\n  # Newton's method for optimisation, starting at x0\n  # f3 is a function that given x returns the list\n  # {f(x), grad f(x), Hessian f(x)}, for some f\n  vX &lt;- vX0\n  f3.x &lt;- f3(vX)\n  n &lt;- 0\n  while ((max(abs(f3.x[[2]])) &gt; dTol) & (n &lt; n.max)) {\n    vX &lt;- vX - solve(f3.x[[3]], f3.x[[2]])\n    #vX &lt;- vX - solve(f3.x[[3]])%*%f3.x[[2]]\n    f3.x &lt;- f3(vX)\n    cat(\"At iteration\", n, \"the coordinates of x are\", vX, \"\\n\")\n    n &lt;- n + 1\n  }\n  if (n == n.max) {\n    cat('newton failed to converge\\n')\n  } else {\n    return(vX)\n  }\n}\n\nn1 &lt;- newton(hcomb, vX0 = c(0, 0))\n#&gt; At iteration 0 the coordinates of x are 2.285714 2.571429\nn1\n#&gt; [1] 2.285714 2.571429\neigen(hcomb(n1)[[3]]) ## maximum\n#&gt; eigen() decomposition\n#&gt; $values\n#&gt; [1] -1.585786 -4.414214\n#&gt; \n#&gt; $vectors\n#&gt;            [,1]       [,2]\n#&gt; [1,] -0.9238795 -0.3826834\n#&gt; [2,] -0.3826834  0.9238795",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-37",
    "href": "extra_exercises.html#section-37",
    "title": "\n28  Extra exercises\n",
    "section": "\n33.5 (5)\n",
    "text": "33.5 (5)\n\n\n\nUsing optim in R:\n\nConsider the Rosenbrock function (often used for testing optimization algorithms): \\(f(x,y) = (a-x)^2 + b(y-x^2)^2\\). Let \\(a=1\\) and \\(b=100\\). We want to find its minimum.\n\nWrite an R function for the Rosenbrock function.\n\n\n\nUse the optim function to find the minimum. Try the following methods:\n\n\n“Nelder-Mead”\n“BFGS”\n“L-BFGS-B” (try with and without bounds, e.g., \\(x \\in [-2, 2], y \\in [-1, 3]\\))\n\n\nStart from an initial guess of \\((-1.2, 1)\\).\n\nFor the “BFGS” method, if you have the numDeriv package, provide the gradient numerically using grad. Does it converge faster or to a better point?\n\n\n\n\n\nsuppressMessages(library(numDeriv))\nfRosenbrock &lt;- function(vX) {\n  a &lt;- 1\n  b &lt;- 100\n  return((a-vX[1])^2 + b * (vX[2]-vX[1]^2)^2)\n}\n\noptim(c(-1.2, -1), fRosenbrock, method = \"Nelder-Mead\")$par\n#&gt; [1] 1.003505 1.006996\noptim(c(-1.2, -1), fRosenbrock, method = \"BFGS\")$par\n#&gt; [1] 0.9998000 0.9996001\noptim(c(-1.2, -1), fRosenbrock, method = \"L-BFGS-B\")$par\n#&gt; [1] 0.9998032 0.9996078\noptim(c(-1.2, -1), fRosenbrock, method = \"L-BFGS-B\", lower=c(-2,-1), upper=c(2,3))$par\n#&gt; [1] 0.9998006 0.9996012\ngrad_rosen &lt;- function(par) numDeriv::grad(fRosenbrock, par)\noptim(c(-1.2, -1), fRosenbrock, gr = grad_rosen, method = \"BFGS\")$par\n#&gt; [1] 1 1",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-38",
    "href": "extra_exercises.html#section-38",
    "title": "\n28  Extra exercises\n",
    "section": "\n33.6 (6)\n",
    "text": "33.6 (6)\n\n\n\nOptimization Pitfalls:\n\nCreate a simple R function \\(f(x) = \\cos(2\\pi x) + 0.1x^2\\).\n\nPlot this function over \\(x \\in [-3, 3]\\).\n\n\nUse optimize() to find the global minimum in this interval.\n\n\nNow, use optimize() to find a local minimum in the interval \\([-0.5, 0.5]\\) and another in \\([0.5, 1.5]\\). Discuss how starting points or intervals affect finding local vs. global optima.\n\n\n\n\n\nf &lt;- function(x) {\n  return(cos(2 * pi * x) + 0.1 * x^2)\n}\n\nvX &lt;- seq(-3, 3, 0.01)\nplot(vX, f(vX), type = \"l\")\n\n\n\n\n\n\n\noptimize(f, interval = c(-3, 3))\n#&gt; $minimum\n#&gt; [1] 1.492439\n#&gt; \n#&gt; $objective\n#&gt; [1] -0.7761343\noptimize(f, interval = c(-0.5, 0.5))\n#&gt; $minimum\n#&gt; [1] -0.4974641\n#&gt; \n#&gt; $objective\n#&gt; [1] -0.975126\noptimize(f, interval = c(0.5, 1.5))\n#&gt; $minimum\n#&gt; [1] 0.5000661\n#&gt; \n#&gt; $objective\n#&gt; [1] -0.9749933",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-39",
    "href": "extra_exercises.html#section-39",
    "title": "\n28  Extra exercises\n",
    "section": "\n33.7 (7)\n",
    "text": "33.7 (7)\n\n\n\nConstrained Optimization via Reparameterization:\n\nYou want to maximize \\(f(p) = p^2(1-p)^3\\) subject to \\(0 &lt; p &lt; 1\\).\n\nUse the transformation \\(p = \\frac{e^{\\tilde{p}}}{1+e^{\\tilde{p}}}\\) to convert this into an unconstrained problem for \\(\\tilde{p}\\). Write the new function \\(g(\\tilde{p}) = f(\\lambda(\\tilde{p}))\\).\n\n\nUse optim with “BFGS” to find the maximum of \\(g(\\tilde{p})\\).\n\n\nTransform the optimal \\(\\tilde{p}^*\\) back to \\(p^*\\) and report the maximum value \\(f(p^*)\\).\n\n\n(Optional) Analytically find the maximum and compare.\n\n\n\n\n\nf &lt;- function(p) {\n  return(p^2 * (1 - p)^3)\n}\n\ng &lt;- function(p_tilde) {\n  p &lt;- exp(p_tilde) / (1 + exp(p_tilde))\n  return(f(p))\n}\n\noptim_res &lt;- optim(0, g, method = \"BFGS\", , control=list(fnscale=-1))\np_tilde &lt;- optim_res$par\np_optim &lt;- exp(p_tilde) / (1 + exp(p_tilde))\np_optim\n#&gt; [1] 0.4\nf(p_optim)\n#&gt; [1] 0.03456",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-40",
    "href": "extra_exercises.html#section-40",
    "title": "\n28  Extra exercises\n",
    "section": "\n33.8 (8)\n",
    "text": "33.8 (8)\n\n\n\nLikelihood Maximization:\n\nSuppose you have \\(n=10\\) observations \\(y = (1, 0, 1, 1, 0, 1, 0, 0, 1, 1)\\) from a Bernoulli distribution with unknown probability \\(p\\). The log-likelihood function is \\(L(p|y) = \\sum y_i \\log(p) + \\sum (1-y_i) \\log(1-p)\\).\n\nWrite an R function for the negative log-likelihood (since optim minimizes).\n\n\nUse optim with method “L-BFGS-B” and appropriate bounds for \\(p\\) (e.g., \\([0.001, 0.999]\\)) to find the Maximum Likelihood Estimate (MLE) of \\(p\\).\n\n\nCompare this to the analytical MLE, which is \\(\\bar{y}\\).\n\n\n\n\n\nneg_log &lt;- function(p, y) {\n  dSum1 &lt;- 0\n  dSum2 &lt;- 0\n  for (i in 1:length(y)) {\n    dSum1 &lt;- dSum1 - y[i] * log(p)\n    dSum2 &lt;- dSum2 - (1 - y[i]) * log(1 - p)\n  }\n  return(dSum1 + dSum2)\n}\n\nneg_log_lik_bern &lt;- function(p, data) {\nif (p &lt;= 0 || p &gt;= 1) return(Inf) # Bounds for log\n-sum(data * log(p) + (1 - data) * log(1 - p))\n}\n\nvY &lt;- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1)\noptim(0.5, neg_log, y = vY, method = \"L-BFGS-B\", lower = 0.001, upper = 0.999)$par\n#&gt; [1] 0.5999997\noptim(0.5, neg_log_lik_bern, data = vY, method = \"L-BFGS-B\", lower = 0.001, upper = 0.999)$par\n#&gt; [1] 0.5999997\n# Analytical MLE\nmean(vY)\n#&gt; [1] 0.6",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-41",
    "href": "extra_exercises.html#section-41",
    "title": "\n28  Extra exercises\n",
    "section": "\n34.1 (1)\n",
    "text": "34.1 (1)\n\n\n\nBasic Rcpp Function:\n\nWrite a C++ function using Rcpp (via cppFunction or sourceCpp) named sum_greater_than_C that takes a numeric vector x and a scalar threshold as input.\nThe function should return the sum of all elements in x that are strictly greater than threshold.\nTest your C++ function in R and compare its output with an equivalent R implementation for x = rnorm(1000) and threshold = 0.\n\n\n\n\nsuppressMessages(library(Rcpp))\n#&gt; Warning: pakke 'Rcpp' blev bygget under R version 4.3.3\n\ncppFunction('\ndouble sum_greater_than_C(NumericVector x, double threshold) {\n  double total = 0;\n  for (int i = 0; i &lt; x.size(); i++) {\n    if (x[i] &gt; threshold) {\n      total += x[i];\n    }\n  }\n  return total;\n}')\n\nvX &lt;- rnorm(1000)\nthreshold &lt;- 0\nsum_greater_than_C(vX, threshold)\n#&gt; [1] 420.1322\nsum(vX[vX&gt;0])\n#&gt; [1] 420.1322",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-42",
    "href": "extra_exercises.html#section-42",
    "title": "\n28  Extra exercises\n",
    "section": "\n34.2 (2)\n",
    "text": "34.2 (2)\n\n\n\nFibonacci Sequence in C++:\n\n\nWrite a recursive C++ function fib_cpp(n) to compute the n-th Fibonacci number.\n\n\nWrite an iterative (loop-based) C++ function fib_iter_cpp(n) to compute the n-th Fibonacci number.\n\n\nCompare the execution speed of fib_cpp(25), fib_iter_cpp(25), and an R recursive Fibonacci function for \\(n=25\\) using microbenchmark.\n\n\n\n\n\nsuppressMessages(library(Rcpp))\nsuppressMessages(library(microbenchmark))\n\ncppFunction('\nint fib_cpp(int n) {\n  if (n &lt;= 1) return(0);\n  return fib_cpp(n - 1) + fib_cpp(n - 2);\n}')\n\ncppFunction('\nint fib_iter_cpp(int n) {\n  if (n &lt;= 1) return(0);\n  int a = 0, b = 1, temp;\n  for (int i = 2; i &lt;= n; i++) {\n    temp = a + b;\n    a = b;\n    b = temp;\n  }\n  return b;\n}')\n\nfib_r &lt;- function(n) {\n  if (n &lt;= 1) {\n    return(0)\n  } else {\n    return(fib_r(n - 1) + fib_r(n - 2))\n  }\n}\n\nmicrobenchmark(fib_cpp(25), fib_iter_cpp(25), fib_r(25))\n#&gt; Unit: nanoseconds\n#&gt;              expr      min       lq     mean   median       uq       max neval\n#&gt;       fib_cpp(25)   150000   152550   182500   162300   184400   1006200   100\n#&gt;  fib_iter_cpp(25)      700     1000    13350     2900    10400    811600   100\n#&gt;         fib_r(25) 87031600 88521350 92179973 89966200 92497400 134088200   100",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-43",
    "href": "extra_exercises.html#section-43",
    "title": "\n28  Extra exercises\n",
    "section": "\n34.3 (3)\n",
    "text": "34.3 (3)\n\n\n\nRcppArmadillo: Matrix Operations:\n\n\nCreate an R script that defines a C++ function arma_matrix_ops using RcppArmadillo.\n\nThis function should take two matrices, mA and mB, as input (use arma::mat).\nIt should return a list containing:\n\n\nmProd: The standard matrix product \\(mA \\times mB\\).\n\nmInvA: The inverse of mA (if possible, handle potential errors if not invertible, e.g. by trying and catching or checking determinant if simple).\n\nvEigenvalsA: The eigenvalues of mA (use arma::eig_sym if symmetric, or arma::eig_gen if general; assume real eigenvalues for simplicity or return complex as string if needed).\n\n\n\nIn R, create two 3x3 matrices, test your function, and verify the product with R’s %*%.\n\n\n\n\n\nsuppressMessages(library(Rcpp))\nsuppressMessages(library(RcppArmadillo))\n#&gt; Warning: pakke 'RcppArmadillo' blev bygget under R version 4.3.3\n\nsourceCpp(\"extra_cpp.cpp\")\n#&gt; \n#&gt; &gt; vX &lt;- 1:10\n#&gt; \n#&gt; &gt; dS &lt;- 5\n#&gt; \n#&gt; &gt; res &lt;- cpp_vector_scalar_mult(vX, dS)\n#&gt; \n#&gt; &gt; print(res)\n#&gt;       [,1]\n#&gt;  [1,]    5\n#&gt;  [2,]   10\n#&gt;  [3,]   15\n#&gt;  [4,]   20\n#&gt;  [5,]   25\n#&gt;  [6,]   30\n#&gt;  [7,]   35\n#&gt;  [8,]   40\n#&gt;  [9,]   45\n#&gt; [10,]   50\n\nmA &lt;- matrix(1:9, 3, 3)\nmB &lt;- matrix(9:1, 3, 3)\narma_matrix_ops(mA, mB)\n#&gt; Matrix mA is singular.\n#&gt; $mProd\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]   90   54   18\n#&gt; [2,]  114   69   24\n#&gt; [3,]  138   84   30\n#&gt; \n#&gt; $vEigenvalsA\n#&gt;                  [,1]\n#&gt; [1,]  1.611684e+01+0i\n#&gt; [2,] -1.116844e+00+0i\n#&gt; [3,] -5.700691e-16+0i\nmA %*% mB\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]   90   54   18\n#&gt; [2,]  114   69   24\n#&gt; [3,]  138   84   30",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-44",
    "href": "extra_exercises.html#section-44",
    "title": "\n28  Extra exercises\n",
    "section": "\n34.4 (4)\n",
    "text": "34.4 (4)\n\n\n\nSimulating GARCH(1,1) process in C++:\n\nThe GARCH(1,1) process is defined by: \\(r_t = \\sigma_t \\epsilon_t\\), where \\(\\epsilon_t \\sim N(0,1)\\) \\(\\sigma_t^2 = \\omega + \\alpha r_{t-1}^2 + \\beta \\sigma_{t-1}^2\\)\n\nWrite a C++ function using RcppArmadillo named simulate_garch_cpp that takes parameters \\(\\omega, \\alpha, \\beta\\), number of time steps T, and an initial \\(\\sigma_0^2\\) as input.\nThe function should simulate and return a list containing two vectors: vR (the returns \\(r_t\\)) and vSigma2 (the variances \\(\\sigma_t^2\\)).\n(Hint: Use R::rnorm(0,1) for \\(\\epsilon_t\\). Initialize \\(r_0\\) using \\(\\sigma_0\\)).\nTest by simulating 1000 steps with \\(\\omega=0.1, \\alpha=0.05, \\beta=0.9, \\sigma_0^2=1\\). Plot the simulated returns and variances.\n\n\n\n\nsuppressMessages(library(Rcpp))\nsuppressMessages(library(RcppArmadillo))\n\nsourceCpp(\"extra_cpp.cpp\")\n#&gt; \n#&gt; &gt; vX &lt;- 1:10\n#&gt; \n#&gt; &gt; dS &lt;- 5\n#&gt; \n#&gt; &gt; res &lt;- cpp_vector_scalar_mult(vX, dS)\n#&gt; \n#&gt; &gt; print(res)\n#&gt;       [,1]\n#&gt;  [1,]    5\n#&gt;  [2,]   10\n#&gt;  [3,]   15\n#&gt;  [4,]   20\n#&gt;  [5,]   25\n#&gt;  [6,]   30\n#&gt;  [7,]   35\n#&gt;  [8,]   40\n#&gt;  [9,]   45\n#&gt; [10,]   50\n\niT &lt;- 1000\ndOmega &lt;- 0.1\ndAlpha &lt;- 0.05\ndBeta &lt;- 0.9\ndSigmaInit &lt;- 1.0\n\nlSim &lt;- simulate_garch_cpp(iT, dOmega, dAlpha, dBeta, dSigmaInit)\n\nplot(1:1000, lSim[[\"vSigma2\"]], type = \"l\", lty = 1, xlab = \"Time\")\nlines(1:1000, lSim[[\"vR\"]],col=\"green\")",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-45",
    "href": "extra_exercises.html#section-45",
    "title": "\n28  Extra exercises\n",
    "section": "\n34.5 (5)\n",
    "text": "34.5 (5)\n\n\n\nCalling R functions from C++:\n\n\nWrite a simple R function my_r_summary &lt;- function(x) list(mean=mean(x), sd=sd(x)).\n\n\nWrite a C++ function call_r_from_cpp that takes a NumericVector x and an R Function r_func as input.\n\nInside the C++ function, call r_func with x and return the result.\n\nTest this by calling call_r_from_cpp(rnorm(100), my_r_summary) from R.\n\n\n\n\n\nmy_r_summary &lt;- function(x) list(mean=mean(x), sd=sd(x))\n\ncppFunction('\nSEXP call_r_from_cpp(NumericVector x, Function r_func) {\n  return r_func(x);\n}')\n\ncall_r_from_cpp(rnorm(100), my_r_summary)\n#&gt; $mean\n#&gt; [1] -0.10438\n#&gt; \n#&gt; $sd\n#&gt; [1] 1.056556",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-46",
    "href": "extra_exercises.html#section-46",
    "title": "\n28  Extra exercises\n",
    "section": "\n34.6 (6)\n",
    "text": "34.6 (6)\n\n\n\nSourceCpp and Embedded R code:\n\nCreate a .cpp file.\nInside it, define a simple C++ function cpp_vector_scalar_mult that takes an arma::vec v and a double s and returns their element-wise product v * s. Make sure to include RcppArmadillo.h and necessary using namespace directives, and the [[Rcpp::export]] tag.\nIn the same .cpp file, use the /*** R ... */ block to write R code that:\n\nDefines a sample vector and a scalar in R.\nCalls your cpp_vector_scalar_mult function.\nPrints the result.\n\n\nUse sourceCpp() in an R script to compile and run the C++ function and the embedded R code.\n\n\n\n\n// [[Rcpp::depends(RcppArmadillo)]]\n#include &lt;RcppArmadillo.h&gt;\nusing namespace Rcpp;\nusing namespace arma;\n\n//[[Rcpp::export]]\nvec cpp_vector_scalar_mult(vec v, double s) {\n  return v * s;\n}\n\n/*** R\nvX &lt;- 1:10\ndS &lt;- 5\nres &lt;- cpp_vector_scalar_mult(vX, dS)\nprint(res)\n*/",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-47",
    "href": "extra_exercises.html#section-47",
    "title": "\n28  Extra exercises\n",
    "section": "\n35.1 (1)\n",
    "text": "35.1 (1)\n\n\n\nSimple R Package:\n\nCreate a new R package using RStudio named “MyUtils”.\n\nInside the R/ directory, create an R script file (e.g., arithmetic.R). Add a simple R function fAddTwoNumbers(a, b) that returns their sum.\n\n\n\nEdit the DESCRIPTION file:\n\n\nGive it a Title: “My First Utility Package”.\nSet yourself as the Author and Maintainer.\nAdd a brief Description.\n\n\n\nDocument your fAddTwoNumbers function. Create an .Rd file in the man/ directory (e.g., fAddTwoNumbers.Rd). Include sections for \\name, \\alias, \\title, \\description, \\usage, \\arguments, \\value, and a simple \\examples block.\n\n\nInstall and restart your package (Ctrl+Shift+L or Build -&gt; Install and Restart).\n\n\nTest your function: MyUtils::fAddTwoNumbers(5, 3) and check its help page ?fAddTwoNumbers.\n\n\n\n\n\nMyUtils::fAddTwoNumbers(5, 2)\n#&gt; [1] 7",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-48",
    "href": "extra_exercises.html#section-48",
    "title": "\n28  Extra exercises\n",
    "section": "\n35.2 (2)\n",
    "text": "35.2 (2)\n\n\n\nPackage with C++ Function (Rcpp):\n\nExtend the “MyUtils” package (or create a new one “MyCppUtils”).\n\nIn the src/ directory, create a C++ file (e.g., cpp_helpers.cpp).\n\n\nIn this file, write a C++ function fGeometricSumCpp(a, r, n) that calculates the sum of the first n terms of a geometric series: \\(a + ar + ar^2 + ... + ar^{n-1}\\). Use a loop. Ensure it’s exported to R using [[Rcpp::export]] and includes &lt;Rcpp.h&gt;.\n\n\n\nModify the DESCRIPTION file:\n\n\nAdd Rcpp to Imports.\nAdd Rcpp to LinkingTo.\n\n\n\nAdd an R wrapper function in an R/ script (e.g., wrappers.R) called fGeometricSum(a, r, n) that simply calls your C++ function fGeometricSumCpp(a, r, n).\n\n\nDocument the R wrapper function fGeometricSum in the man/ directory.\n\n\nInstall and restart. Test MyCppUtils::fGeometricSum(1, 0.5, 10).",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-49",
    "href": "extra_exercises.html#section-49",
    "title": "\n28  Extra exercises\n",
    "section": "\n35.3 (3)\n",
    "text": "35.3 (3)\n\n\n\nPackage with RcppArmadillo Function:\n\nExtend one of your previous packages (or create “MyArmaUtils”).\n\nIn the src/ directory, create/modify a C++ file.\n\n\nWrite a C++ function fSolveSystemArma(mA, vB) that takes an arma::mat mA and arma::vec vB and solves the linear system \\(mA \\cdot x = vB\\) for \\(x\\) using arma::solve(). Return \\(x\\). Export it. Make sure to include &lt;RcppArmadillo.h&gt; and the [[Rcpp::depends(RcppArmadillo)]] attribute.\n\n\n\nModify DESCRIPTION:\n\n\nEnsure Rcpp, RcppArmadillo are in Imports.\nEnsure Rcpp, RcppArmadillo are in LinkingTo.\n\n\n\nWrite an R wrapper for fSolveSystemArma.\n\n\nDocument the R wrapper.\n\n\nInstall, restart, and test with a sample 3x3 matrix and vector.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-50",
    "href": "extra_exercises.html#section-50",
    "title": "\n28  Extra exercises\n",
    "section": "\n35.4 (4)\n",
    "text": "35.4 (4)\n\n\n\nPackage Structure and NAMESPACE:\n\n\nFor any of the packages you’ve started, after installing and restarting (which RStudio often handles via devtools::load_all()), inspect the NAMESPACE file that RStudio/roxygen2 (if used) might generate.\n\n\nWhat is the purpose of the NAMESPACE file in an R package?\n\n\nIf you were not using roxygen2 (which RStudio uses by default for new packages), how would you manually ensure your R functions and C++ functions (via their R wrappers) are exported for users of your package? (Hint: export() in NAMESPACE).",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-51",
    "href": "extra_exercises.html#section-51",
    "title": "\n28  Extra exercises\n",
    "section": "\n35.5 (5)\n",
    "text": "35.5 (5)\n\n\n\nBuilding and Sharing (Conceptual):\n\n\nFor your “MyUtils” package, use RStudio’s “Build” pane to “Build Source Package”. What is the file extension of the output?\n\n\nIf you wanted to share this package with someone who also has R development tools, which file would you send them?\n\n\nIf you wanted to submit this package to CRAN (hypothetically), what are some of the key checks and requirements CRAN would have? (Refer to lecture notes/Hadley Wickham’s book concepts).",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-52",
    "href": "extra_exercises.html#section-52",
    "title": "\n28  Extra exercises\n",
    "section": "\n36.1 (1)\n",
    "text": "36.1 (1)\n\n\n\nBasic Parallel lapply:\n\n\nCreate a list of 10 numeric vectors, each containing 1,000,000 random numbers from a \\(N(0,1)\\) distribution.\n\n\nWrite a function fSlowFunction(v) that calculates mean(log(abs(v) + 1)) and then pauses for 0.1 seconds using Sys.sleep(0.1).\n\n\nUse the standard lapply to apply fSlowFunction to your list of vectors. Time this operation using system.time().\n\n\nNow, use the parallel package. Detect the number of cores on your machine. Create a cluster using max(1, detectCores() - 1) cores.\n\n\nUse parLapply from the parallel package to apply fSlowFunction to your list. Time this operation. Remember to export necessary objects/functions to the cluster if they are not in the base environment or loaded packages on workers.\n\n\nCompare the execution times. Stop the cluster.\n\n\n\n\n\nlistV &lt;- list(\n  v1 = rnorm(1000000),\n  v2 = rnorm(1000000),\n  v3 = rnorm(1000000),\n  v4 = rnorm(1000000),\n  v5 = rnorm(1000000),\n  v6 = rnorm(1000000),\n  v7 = rnorm(1000000),\n  v8 = rnorm(1000000),\n  v9 = rnorm(1000000),\n  v10 = rnorm(1000000)\n)\n\nfSlowFunction &lt;- function(v) {\n  Sys.sleep(0.1)\n  return(mean(log(abs(v) + 1)))\n}\n\nsystem.time({lapply(listV, fSlowFunction)})\n#&gt;   bruger   system forløbet \n#&gt;     0.32     0.00     1.42\n\nsuppressMessages(library(parallel))\ncluster &lt;- makeCluster(max(1, detectCores() - 1))\n\nsystem.time(parLapply(cluster, listV, fSlowFunction))\n#&gt;   bruger   system forløbet \n#&gt;     0.07     0.02     0.39\nstopCluster(cluster)",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-53",
    "href": "extra_exercises.html#section-53",
    "title": "\n28  Extra exercises\n",
    "section": "\n36.2 (2)\n",
    "text": "36.2 (2)\n\n\n\nParallel Simulation and Aggregation:\n\nYou want to run 1000 simulations. In each simulation, you:\n\nGenerate 500 random numbers from an Exponential distribution with rate = 0.5.\nCalculate the mean of these 500 numbers.\n\n\n\nWrite an R function fOneSim() that performs one such simulation and returns the mean.\n\n\nUse lapply or a for loop to run 1000 simulations and collect the means. Time it.\n\n\nUse parLapply (or mclapply on non-Windows) to run these 1000 simulations in parallel. Time it and compare.\n\n\nPlot a histogram of the 1000 means. What does the Central Limit Theorem suggest about the shape of this distribution?\n\n\n\n\n\nExponential.Simulate &lt;- function(lambda, size) {\n  U &lt;- runif(size)\n  return(mean(-1/lambda * log(U)))\n}\n\nset.seed(10086)\n\nX &lt;- Exponential.Simulate(0.5, 500)\n\nsystem.time(lResult &lt;- lapply(1:500, function(x) Exponential.Simulate(0.5, 500)))\n#&gt;   bruger   system forløbet \n#&gt;     0.02     0.00     0.02\n\nsuppressMessages(library(parallel))\ncluster &lt;- makeCluster(max(1, detectCores() - 1))\nclusterExport(cluster, \"Exponential.Simulate\")\nsystem.time(means_par &lt;- parLapply(cluster, 1:500, function(x) Exponential.Simulate(0.5, 500)))\n#&gt;   bruger   system forløbet \n#&gt;     0.00     0.00     0.01\nstopCluster(cluster)\n\n# CLT suggests distribution of sample means will be approximately normal\nhist(unlist(means_par))",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-54",
    "href": "extra_exercises.html#section-54",
    "title": "\n28  Extra exercises\n",
    "section": "\n36.3 (3)\n",
    "text": "36.3 (3)\n\n\n\nparSapply vs sapply:\n\nConsider the task of calculating sqrt(i) for i from 1 to 1,000,000, but make each calculation artificially slow by adding Sys.sleep(0.00001).\n\nImplement this using sapply. Time it.\n\n\nImplement this using parSapply. Time it and compare. Remember cluster setup and shutdown.\n\n\n\n\n\nsystem.time(sapply(1:1000000, function(x) {sqrt(x); Sys.sleep(0.00001)}))\n#&gt;   bruger   system forløbet \n#&gt;     0.80     0.01     0.84\n\nsuppressMessages(library(parallel))\ncluster &lt;- makeCluster(max(1, detectCores() - 1))\nsystem.time(parLapply(cluster, 1:1000000, function(x) {sqrt(x); Sys.sleep(0.00001)}))\n#&gt;   bruger   system forløbet \n#&gt;     0.08     0.00     0.58\nstopCluster(cluster)",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_exercises.html#section-55",
    "href": "extra_exercises.html#section-55",
    "title": "\n28  Extra exercises\n",
    "section": "\n36.4 (4)\n",
    "text": "36.4 (4)\n\n\n\nConsiderations for Parallelization:\n\n\nWhen is parallelization most beneficial? (Think about task granularity and communication overhead).\n\n\nGive an example of a task that is “embarrassingly parallel” and one that is difficult or impossible to parallelize effectively.\n\n\nWhat are some potential pitfalls or complexities when writing parallel code (e.g., random number generation in parallel, race conditions - though less of an issue with parLapply’s model)?\n\n\n\n\n\n# A: Task granularity high, communication overhead low.\n# B: Embarassingly paralllel: Monte Carlo sims.\n#    Difficult: Highly sequential algorithms like some recursive functions.\n# C) RNG state per worker, data transfer, debugging",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Extra exercises</span>"
    ]
  },
  {
    "objectID": "advanced_exercises.html",
    "href": "advanced_exercises.html",
    "title": "\n29  Advanced exercises\n",
    "section": "",
    "text": "30 Exercise Set 9: Advanced R, Data Manipulation, and Debugging",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Advanced exercises</span>"
    ]
  },
  {
    "objectID": "advanced_exercises.html#section",
    "href": "advanced_exercises.html#section",
    "title": "\n29  Advanced exercises\n",
    "section": "\n30.1 (1)\n",
    "text": "30.1 (1)\n\n\n\nData Frame Manipulation with dplyr (if allowed, otherwise base R):\n\nLoad the built-in iris dataset.\n\nFilter the dataset to include only Species == \"setosa\" and Sepal.Length &gt; 5.0.\n\n\nCreate a new column Petal.Area calculated as Petal.Length * Petal.Width.\n\n\nFor each Species, calculate the mean Sepal.Length and mean Petal.Area.\n\n\nArrange the results from (c) in descending order of mean Sepal.Length.\n\n(If dplyr is not allowed, use base R functions like subset(), transform(), aggregate(), order()).\n\n\n\n\ndf_subset &lt;- subset(iris, Species == \"setosa\" & Sepal.Length &gt; 5.0)\ndf &lt;- iris\ndf &lt;- transform(df, Petal.Area = Petal.Length * Petal.Width)\nagg_df &lt;- aggregate(cbind(Sepal.Length, Petal.Area) ~ Species, data = df, FUN = mean)\nagg_df[order(agg_df$Sepal.Length, decreasing = TRUE), ]\n#&gt;      Species Sepal.Length Petal.Area\n#&gt; 3  virginica        6.588    11.2962\n#&gt; 2 versicolor        5.936     5.7204\n#&gt; 1     setosa        5.006     0.3656",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Advanced exercises</span>"
    ]
  },
  {
    "objectID": "advanced_exercises.html#section-1",
    "href": "advanced_exercises.html#section-1",
    "title": "\n29  Advanced exercises\n",
    "section": "\n30.2 (2)\n",
    "text": "30.2 (2)\n\n\n\nWorking with Dates and Times (lubridate if allowed, otherwise base R):\n\n\nCreate a character vector of dates: vsDateStrings &lt;- c(\"2023-01-15\", \"2023/03/22\", \"07-Apr-2023\").\n\n\nConvert vsDateStrings into R Date objects.\n\n\nFor each date, find the day of the week.\n\n\nCalculate the number of days between the first and last date in your converted date objects.\n\n\n\n\n\nvsDateStrings &lt;- c(\"2023-01-15\", \"2023/03/22\", \"07-Apr-2023\")\ndate_objects &lt;- as.Date(vsDateStrings, tryFormats = c(\"%Y-%m-%d\", \"%Y/%m/%d\", \"%d-%b-%Y\"))\nweekdays(date_objects)\n#&gt; [1] \"søndag\" NA       NA\ndiff(range(date_objects))\n#&gt; Time difference of NA days",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Advanced exercises</span>"
    ]
  },
  {
    "objectID": "advanced_exercises.html#section-2",
    "href": "advanced_exercises.html#section-2",
    "title": "\n29  Advanced exercises\n",
    "section": "\n30.3 (3)\n",
    "text": "30.3 (3)\n\n\n\nError Handling and Debugging:\n\nConsider the following (buggy) R function:\n\n\n\n\nfBuggyMean &lt;- function(vData, bRemoveNA) {\n          if (bRemoveNA = TRUE) { # Intentional bug: assignment instead of comparison\n            vData &lt;- vData[!is.na(vData)]\n          }\n          total_sum &lt;- 0\n          for (val in vData) {\n            total_sum &lt;- total_sum + val\n          }\n          return(total_sum / length(vData)) # Potential bug: division by zero if vData is empty\n        }\n\n\n\nTest fBuggyMean(c(1, 2, NA, 4), bRemoveNA = TRUE). What is the output and why is it incorrect?\n\n\nTest fBuggyMean(c(NA, NA), bRemoveNA = TRUE). What happens?\n\n\nUse R’s debugging tools (browser(), debug(), or RStudio’s debugger) to step through the function with the input from (a) and identify the logical error in the if condition.\n\n\nCorrect the function. Also, add error handling using tryCatch() or if conditions to prevent division by zero if the vector (after NA removal) is empty, returning NaN or a message in such cases.\n\n\n\nfBuggyMean_corrected &lt;- function(vData, bRemoveNA) {\n  if (isTRUE(bRemoveNA)) { # Corrected comparison\n    vData &lt;- vData[!is.na(vData)]\n  }\n  if (length(vData) == 0) { # Handle empty vector\n    return(NaN)\n  }\n  total_sum &lt;- 0\n  for (val in vData) { # Loop is fine, sum() is better\n    total_sum &lt;- total_sum + val\n  }\n  return(total_sum / length(vData))\n}\nfBuggyMean_corrected(c(1,2,NA,4), bRemoveNA=TRUE)\n#&gt; [1] 2.333333\nfBuggyMean_corrected(c(NA,NA), bRemoveNA=TRUE)\n#&gt; [1] NaN",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Advanced exercises</span>"
    ]
  },
  {
    "objectID": "advanced_exercises.html#section-3",
    "href": "advanced_exercises.html#section-3",
    "title": "\n29  Advanced exercises\n",
    "section": "\n30.4 (4)\n",
    "text": "30.4 (4)\n\n\n\nString Manipulation (stringr if allowed, otherwise base R):\n\nYou have a vector of product codes: vsCodes &lt;- c(\"PROD-A-123-X\", \"PROD-B-45-Y\", \"ITEM-C-6789-Z\").\n\nExtract the middle part (A, B, C) from each code.\n\n\nExtract the numeric part (123, 45, 6789) from each code and convert it to a numeric vector.\n\n\nReplace “PROD” with “PRODUCT” in all codes.\n\n\n\n\n\nvsCodes &lt;- c(\"PROD-A-123-X\", \"PROD-B-45-Y\", \"ITEM-C-6789-Z\")\nsapply(strsplit(vsCodes, \"-\"), function(x) x[2])\n#&gt; [1] \"A\" \"B\" \"C\"\nas.numeric(sapply(strsplit(vsCodes, \"-\"), function(x) x[3]))\n#&gt; [1]  123   45 6789\ngsub(\"PROD\", \"PRODUCT\", vsCodes)\n#&gt; [1] \"PRODUCT-A-123-X\" \"PRODUCT-B-45-Y\"  \"ITEM-C-6789-Z\"",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Advanced exercises</span>"
    ]
  },
  {
    "objectID": "advanced_exercises.html#section-4",
    "href": "advanced_exercises.html#section-4",
    "title": "\n29  Advanced exercises\n",
    "section": "\n30.5 (5)\n",
    "text": "30.5 (5)\n\n\n\nWriting Efficient R Code - Revisiting Loops:\n\nYou need to calculate a cumulative product of a vector vX = 1:1000.\n\nImplement this using a for loop.\n\n\nImplement this using R’s built-in cumprod() function.\n\n\nCompare their performance using microbenchmark. Explain why the built-in function is significantly faster.\n\n\n\n\n\nvX_cumprod &lt;- 1:100 # Smaller for quick demo\nfCumprodLoop &lt;- function(v) {\n  res &lt;- numeric(length(v))\n  if (length(v) &gt; 0) res[1] &lt;- v[1]\n  if (length(v) &gt; 1) {\n    for (i in 2:length(v)) res[i] &lt;- res[i - 1] * v[i]\n  }\n  return(res)\n}\nmicrobenchmark::microbenchmark(fCumprodLoop(vX_cumprod), cumprod(vX_cumprod))\n#&gt; Unit: nanoseconds\n#&gt;                      expr  min   lq  mean median    uq     max neval\n#&gt;  fCumprodLoop(vX_cumprod) 9700 9900 70143  10000 10150 6008600   100\n#&gt;       cumprod(vX_cumprod)  500  600   805    600   700    4100   100",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Advanced exercises</span>"
    ]
  },
  {
    "objectID": "advanced_exercises.html#section-5",
    "href": "advanced_exercises.html#section-5",
    "title": "\n29  Advanced exercises\n",
    "section": "\n31.1 (1)\n",
    "text": "31.1 (1)\n\n\n\nRcppArmadillo: More Linear Algebra:\n\nWrite a C++ function arma_advanced_linalg using RcppArmadillo that takes a square arma::mat M.\nThe function should return a list containing:\n\n\ndDeterminant: The determinant of M (arma::det(M)).\n\ndTrace: The trace of M (arma::trace(M)).\n\nmCholesky: The Cholesky decomposition of M, if M is positive definite. (Use arma::chol(M). You might want to wrap this in a try-catch block in C++ or check arma::is_sympd(M) first, returning an empty matrix or NA if not applicable).\n\n\nTest with a 3x3 symmetric positive definite matrix and a non-symmetric matrix.\n\n\n\n\n// [[Rcpp::depends(RcppArmadillo)]]\n#include &lt;RcppArmadillo.h&gt;\n// [[Rcpp::export]]\nRcpp::List arma_advanced_linalg(arma::mat M) {\n  double dDet = arma::det(M);\n  double dTrace = arma::trace(M);\n  arma::mat mChol;\n  bool chol_success = false;\n  if(M.is_sympd()) { // Check if symmetric positive definite for Cholesky\n     chol_success = arma::chol(mChol, M);\n  }\n  \n  return Rcpp::List::create(\n    Rcpp::Named(\"dDeterminant\") = dDet,\n    Rcpp::Named(\"dTrace\") = dTrace,\n    Rcpp::Named(\"mCholesky\") = (chol_success ? Rcpp::wrap(mChol) : Rcpp::wrap(arma::mat()))\n  );\n}",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Advanced exercises</span>"
    ]
  },
  {
    "objectID": "advanced_exercises.html#section-6",
    "href": "advanced_exercises.html#section-6",
    "title": "\n29  Advanced exercises\n",
    "section": "\n31.2 (2)\n",
    "text": "31.2 (2)\n\n\n\nStructuring C++ code in a Package:\n\nImagine you are building a package “StatTools” that includes several statistical computations.\n\n\nCreate two C++ functions:\n\n\n\nweighted_mean_cpp(arma::vec x, arma::vec weights): calculates the weighted mean.\n\nvariance_cpp(arma::vec x): calculates the sample variance.\n\n\n\nPlace these in a single .cpp file within the src/ directory of your package.\n\n\nWrite R wrappers for both in an R/ script.\n\n\nEnsure both are exported and documented.\n\n\nUpdate DESCRIPTION if necessary (e.g., if you used RcppArmadillo). Install and test.\n\n\n\n\n\n// [[Rcpp::depends(RcppArmadillo)]]\n#include &lt;RcppArmadillo.h&gt;\n\n// [[Rcpp::export]]\ndouble weighted_mean_cpp(arma::vec x, arma::vec weights) {\n  return arma::sum(x % weights) / arma::sum(weights); // % is element-wise product\n}\n\n// [[Rcpp::export]]\ndouble variance_cpp(arma::vec x) {\n  return arma::var(x, 0); // 0 for sample variance (N-1 denominator)\n}",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Advanced exercises</span>"
    ]
  },
  {
    "objectID": "advanced_exercises.html#section-7",
    "href": "advanced_exercises.html#section-7",
    "title": "\n29  Advanced exercises\n",
    "section": "\n31.3 (3)\n",
    "text": "31.3 (3)\n\n\n\nUsing C++ Standard Library Features within Rcpp:\n\nWrite a C++ function cpp_unique_sorted(Rcpp::NumericVector x) that returns a sorted vector containing only the unique elements of x.\nHint: You can use std::sort and std::unique from the C++ &lt;algorithm&gt; header, then resize the Rcpp vector. Remember to include &lt;algorithm&gt;.\nCompare its output and speed to sort(unique(x_r_vector)) in R.\n\n\n\n\n#include &lt;Rcpp.h&gt;\n#include &lt;algorithm&gt; // For std::sort, std::unique\n#include &lt;vector&gt;    // For std::vector\n\n// [[Rcpp::export]]\nRcpp::NumericVector cpp_unique_sorted(Rcpp::NumericVector x) {\n  if (x.size() == 0) {\n    return Rcpp::NumericVector(0);\n  }\n  // Convert to std::vector for std algorithms\n  std::vector&lt;double&gt; std_x = Rcpp::as&lt;std::vector&lt;double&gt;&gt;(x);\n  \n  std::sort(std_x.begin(), std_x.end());\n  // std::unique moves unique elements to the front and returns an iterator \n  // to the end of the unique range.\n  std_x.erase(std::unique(std_x.begin(), std_x.end()), std_x.end());\n  \n  return Rcpp::wrap(std_x); // Convert back to Rcpp::NumericVector\n}",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Advanced exercises</span>"
    ]
  },
  {
    "objectID": "advanced_exercises.html#section-8",
    "href": "advanced_exercises.html#section-8",
    "title": "\n29  Advanced exercises\n",
    "section": "\n31.4 (4)\n",
    "text": "31.4 (4)\n\n\n\nPackage Vignettes (Conceptual/Simple Implementation):\n\n\nWhat is the purpose of a vignette in an R package?\n\n\n\nIn one of your previously created packages (e.g., “MyUtils”), create a simple vignette.\n\n\nUse RStudio: File &gt; New File &gt; R Markdown… &gt; From Template &gt; Package Vignette (devtools).\nWrite a short introduction to your package and demonstrate the use of one of its functions.\nBuild the vignette (RStudio’s Build pane -&gt; More -&gt; Build Vignettes).\nHow would a user typically access this vignette after installing your package? (Hint: vignette(\"vignette_name\", package=\"PackageName\")).\n\n\n\n\n\na: Vignettes: long-form documentation, tutorials. e: browseVignettes(\"MyUtils\") or vignette(\"my-vignette-name\", package=\"MyUtils\")",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Advanced exercises</span>"
    ]
  },
  {
    "objectID": "advanced_exercises.html#section-9",
    "href": "advanced_exercises.html#section-9",
    "title": "\n29  Advanced exercises\n",
    "section": "\n32.1 (1)\n",
    "text": "32.1 (1)\n\n\n\nOptimization with Noisy Objective Function:\n\nSuppose you want to minimize \\(g(x) = (x-2)^2\\), but you can only observe \\(h(x) = g(x) + \\epsilon\\), where \\(\\epsilon \\sim N(0, 0.1^2)\\) (observation noise).\n\nWrite an R function for \\(h(x)\\).\n\n\nCan you directly use optimize on \\(h(x)\\) to reliably find the minimum of \\(g(x)\\)? Try it a few times. What happens?\n\n\nA common strategy is to average multiple evaluations of \\(h(x)\\) to reduce noise. Create a new function h_avg(x, n_reps) that computes mean(replicate(n_reps, h(x))).\n\n\nUse optimize on h_avg(x, n_reps=100). Is the result more stable/accurate for finding the minimum of \\(g(x)\\)?\n\n\n\n\n\nh &lt;- function(x) {\n  return((x - 2)^2 + rnorm(1, 0, 0.1))\n}\n\noptimize(h, interval = c(-4, 4))$minimum\n#&gt; [1] 2.108014\noptimize(h, interval = c(-4, 4))$minimum\n#&gt; [1] 2.225827\noptimize(h, interval = c(-4, 4))$minimum\n#&gt; [1] 1.928427\noptimize(h, interval = c(-4, 4))$minimum\n#&gt; [1] 1.9597\n\nh_avg &lt;- function(x, n_reps) {\n  return(mean(replicate(n_reps, h(x))))\n}\n\noptimize(h_avg, interval = c(-4,4), n_reps = 100)\n#&gt; $minimum\n#&gt; [1] 1.983309\n#&gt; \n#&gt; $objective\n#&gt; [1] 0.008184381",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Advanced exercises</span>"
    ]
  },
  {
    "objectID": "advanced_exercises.html#section-10",
    "href": "advanced_exercises.html#section-10",
    "title": "\n29  Advanced exercises\n",
    "section": "\n32.2 (2)\n",
    "text": "32.2 (2)\n\n\n\nSimulated Annealing (Conceptual/Simple Implementation):\n\nGoal: Find the global minimum of a “wiggly” function, e.g., \\(f(x) = (x^2-1)^2 + \\cos(5\\pi x)\\) over \\(x \\in [-2, 2]\\).\nSimulated Annealing idea:\n\nStart at a random point \\(x_c\\).\nPropose a new point \\(x_n\\) (e.g., \\(x_n = x_c + N(0, \\text{stepsize})\\).).\nIf \\(f(x_n) &lt; f(x_c)\\), move to \\(x_n\\).\nIf \\(f(x_n) \\ge f(x_c)\\), move to \\(x_n\\) with probability \\(P = \\exp(-\\frac{f(x_n) - f(x_c)}{T})\\), where \\(T\\) is the “temperature”.\nDecrease \\(T\\) slowly (e.g., \\(T_{new} = T \\times \\text{coolingrate}\\)). Repeat.\n\n\n\nPlot \\(f(x)\\).\n\n\nImplement a basic simulated annealing function. Choose a starting T (e.g., 10), a cooling rate (e.g., 0.99), a step size for proposals, and a number of iterations.\n\n\nRun it. Does it find a good minimum? Experiment with parameters. (This is more about the concept than a perfect implementation).\n\n\n\n\n\nf &lt;- function(x) {\n  return((x^2 - 1)^2 + cos(5 * pi * x))\n}\n\nfSimAnn &lt;- function(f, dLower, dUpper, dStart = (dUpper - dLower)/2, dCool, dStepSize, n.iter = 100, dT) {\n  # start at a random point x_c\n  x_c &lt;- dStart\n  T_new &lt;- dT\n  for (i in 1:n.iter) {\n    x_n &lt;- x_c + rnorm(1, 0, dStepSize)\n    if (f(x_n) &lt; f(x_c)) {\n      x_c &lt;- x_n\n    } else if (runif(1) &lt; exp(-(f(x_n) - f(x_c)) / T_new)) {\n      x_c &lt;- x_n\n    }\n    T_new &lt;- T_new * dCool\n  }\n  return(x_c)\n}\n\nvX &lt;- seq(-4, 4, 0.01)\nplot(vX, f(vX), type = \"l\")\n\n\n\n\n\n\nfSimAnn(f, -2, 2, dCool = 0.99, dT = 10, dStepSize = 0.1, n.iter = 5000)\n#&gt; [1] -1.000039",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Advanced exercises</span>"
    ]
  },
  {
    "objectID": "advanced_exercises.html#section-11",
    "href": "advanced_exercises.html#section-11",
    "title": "\n29  Advanced exercises\n",
    "section": "\n32.3 (3)\n",
    "text": "32.3 (3)\n\n\n\nBootstrap Standard Errors for a Custom Statistic:\n\nSuppose you have data vData &lt;- rexp(50, rate = 0.2). You are interested in the statistic \\(\\theta = 1/\\text{mean}(vData)\\).\n\nCalculate \\(\\hat{\\theta}\\) from vData.\n\n\n\nWrite a bootstrap procedure:\n\n\nSet a number of bootstrap replications, B (e.g., 1000).\nFor each replication b from 1 to B:\n\nResample vData with replacement to get vData_b.\nCalculate \\(\\hat{\\theta}_b = 1/\\text{mean}(vData_b)\\).\n\n\nThe bootstrap standard error of \\(\\hat{\\theta}\\) is the standard deviation of the \\(\\hat{\\theta}_b\\) values.\n\n\n\nImplement this and report the bootstrap standard error for \\(\\hat{\\theta}\\).\n\n\n(Optional) Can this be parallelized? How?\n\n\n\n\n\nvData &lt;- rexp(50, rate = 0.2)\n# Theta-hat\n1/mean(vData)\n#&gt; [1] 0.1987081\n\n# bootstrap\nB &lt;- 1000\nvB &lt;- numeric(B)\nfor (b in 1:B) {\n  vB[b] &lt;- 1/mean(sample(vData, length(vData), replace = TRUE))\n}\n\nmean(vB)\n#&gt; [1] 0.2035953\nsqrt(var(vB))\n#&gt; [1] 0.02796418",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Advanced exercises</span>"
    ]
  },
  {
    "objectID": "advanced_exercises.html#section-12",
    "href": "advanced_exercises.html#section-12",
    "title": "\n29  Advanced exercises\n",
    "section": "\n33.1 (1)\n",
    "text": "33.1 (1)\n\n\n\nProblem: Estimating Parameters of a Mixture Distribution via Likelihood Maximization\n\nYou observe data \\(y_1, \\dots, y_n\\) which you believe comes from a mixture of two normal distributions: \\(f(y | \\pi, \\mu_1, \\sigma_1, \\mu_2, \\sigma_2) = \\pi \\cdot N(y | \\mu_1, \\sigma_1^2) + (1-\\pi) \\cdot N(y | \\mu_2, \\sigma_2^2)\\) where \\(\\pi\\) is the mixing proportion (\\(0 &lt; \\pi &lt; 1\\)), and \\(N(y|\\mu, \\sigma^2)\\) is the PDF of a normal distribution.\nAssume \\(\\sigma_1 = 1\\) and \\(\\sigma_2 = 1\\) are known. You need to estimate \\(\\theta = (\\pi, \\mu_1, \\mu_2)\\).\nGiven data: set.seed(42); y_data &lt;- c(rnorm(30, -2, 1), rnorm(70, 2, 1))\n\n\nWrite an R function for the log-likelihood of this mixture model, given y_data and parameters pi, mu1, mu2. (Remember to sum the logs of the individual likelihoods).\n\n\nReparameterize \\(\\pi\\) using the logit transform: \\(\\pi = \\frac{e^{\\text{logit-pi}}}{1+e^{\\text{logit-pi}}}\\) to make it unconstrained. Modify your log-likelihood function to take logit_pi, mu1, mu2.\n\n\nUse optim with the “BFGS” method to find the MLEs for logit_pi, mu1, mu2. Choose reasonable starting values.\n\n\nTransform logit_pi_hat back to \\(\\hat{\\pi}\\). Report \\(\\hat{\\pi}, \\hat{\\mu_1}, \\hat{\\mu_2}\\).\n\n\n(Optional C++) Write the log-likelihood calculation (for one data point) in C++ and try to use it within your R optimization (this is advanced, involving passing C++ pointers or using Rcpp sugar for calling R functions that wrap C++). Or, simply write the full log-likelihood sum in C++ and call it from R.\n\n\n\n\n\nset.seed(42)\ny_data &lt;- c(rnorm(30, -2, 1), rnorm(70, 2, 1))\n\nlog_likelihood &lt;- function(data, params, sigma_1, sigma_2) {\n  logit_pi &lt;- params[1]\n  mu1 &lt;- params[2]\n  mu2 &lt;- params[3]\n  \n  pi &lt;- exp(logit_pi) / (1 + exp(logit_pi))\n  \n  return(-sum(log(pi * dnorm(data, mu1, sigma_1) + (1 - pi) * dnorm(data, mu2, sigma_2))))\n}\n\nlogit_pi_hat &lt;- optim(c(0.5, 0, 0), log_likelihood, data = y_data, sigma_1 = 1, sigma_2 = 1, method = \"BFGS\")\npi_hat &lt;- exp(logit_pi_hat$par[1]) / (1 + exp(logit_pi_hat$par[1]))\npi_hat\n#&gt; [1] 0.7133243\nlogit_pi_hat$par[2]\n#&gt; [1] 1.988721\nlogit_pi_hat$par[3]\n#&gt; [1] -2.044431",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Advanced exercises</span>"
    ]
  },
  {
    "objectID": "advanced_exercises.html#section-13",
    "href": "advanced_exercises.html#section-13",
    "title": "\n29  Advanced exercises\n",
    "section": "\n33.2 (2)\n",
    "text": "33.2 (2)\n\n\n\nProblem: Simulating a Queueing System and Root-Finding for Stability\n\nConsider a simple M/M/1 queue. Customers arrive according to a Poisson process with rate \\(\\lambda\\). Service times are exponential with rate \\(\\mu\\). The system is stable if \\(\\rho = \\lambda/\\mu &lt; 1\\).\n\nSimulation Part:\n\n\nWrite an R function simulate_queue(lambda, mu, n_customers) that simulates the arrival and service of n_customers.\n\nIt should track arrival times and service completion times. For simplicity, assume the first customer arrives at time 0.\nReturn the average waiting time in the queue (time from arrival until service starts) for the customers.\n(Hint: \\(Arrival_{i+1} = Arrival_i + Exp(\\lambda)\\). \\(ServiceStart_i = max(Arrival_i, Completion_{i-1})\\). \\(Completion_i = ServiceStart_i + Exp(\\mu)\\)).\n\nRun this simulation for \\(\\lambda=0.8, \\mu=1\\) for 1000 customers. Report average waiting time. Repeat a few times. Does it vary?\n\n\n\n\nRoot-Finding Part:\n\nThe theoretical average waiting time in an M/M/1 queue is \\(W_q = \\frac{\\lambda}{\\mu(\\mu-\\lambda)}\\) for \\(\\rho &lt; 1\\).\n\nSuppose \\(\\mu=1\\) and you want to find the arrival rate \\(\\lambda\\) that results in an average waiting time \\(W_q = 5\\).\n\nDefine a function \\(g(\\lambda) = \\frac{\\lambda}{1(1-\\lambda)} - 5\\).\nUse uniroot to find \\(\\lambda \\in [0, 0.99]\\) such that \\(g(\\lambda)=0\\).\n\n\n\n(Package Creation) Create a small R package “QueueSim” containing your simulate_queue function. Document it and build the package.\n\n\n\n\n\nsimulate_queue &lt;- function(lambda, mu, n_customers) {\n  if (n_customers == 0) return(0)\n  arrival_times &lt;- numeric(n_customers)\n  service_start_times &lt;- numeric(n_customers)\n  service_end_times &lt;- numeric(n_customers)\n  service_times &lt;- rexp(n_customers, lambda)\n  \n  # First customer\n  arrival_times[1] &lt;- 0\n  service_start_times[1] &lt;- arrival_times[1]\n  service_end_times[1] &lt;- service_start_times[1] + service_times[1]\n  \n  if (n_customers &gt; 1) {\n    inter_arrival_times &lt;- rexp(n_customers - 1, rate = lambda)\n    for (i in 2:n_customers) {\n      arrival_times[i] &lt;- arrival_times[i - 1] + inter_arrival_times[i - 1]\n      service_start_times[i] &lt;- max(arrival_times[i], service_end_times[i - 1])\n      service_end_times[i] &lt;- service_start_times[i] + service_times[i]\n    }\n  }\n  waiting_times &lt;- service_start_times - arrival_times\n  return(mean(waiting_times))\n}\n\nsimulate_queue(0.8, 1, 1000)\n#&gt; [1] 61.33853\nsimulate_queue(0.8, 1, 1000)\n#&gt; [1] 12.78504\nsimulate_queue(0.8, 1, 1000)\n#&gt; [1] 71.37086\nsimulate_queue(0.8, 1, 1000)\n#&gt; [1] 35.86602\n\ng_lambda &lt;- function(lambda) lambda / (1 * (1 - lambda)) - 5\nuniroot(g_lambda, interval = c(1e-6, 0.999))$root\n#&gt; [1] 0.8333285",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Advanced exercises</span>"
    ]
  },
  {
    "objectID": "advanced_exercises.html#section-14",
    "href": "advanced_exercises.html#section-14",
    "title": "\n29  Advanced exercises\n",
    "section": "\n33.3 (3)\n",
    "text": "33.3 (3)\n\n\n\nProblem: Path Dependent Option Pricing using Monte Carlo and C++ Speedup\n\nAn Asian call option’s payoff depends on the average price of the underlying asset over a period. Payoff = \\(\\max(0, \\text{AvgPrice} - K)\\), where \\(K\\) is the strike price.\nAsset price \\(S_t\\) follows a geometric Brownian motion: \\(dS_t = r S_t dt + \\sigma S_t dW_t\\), which discretizes to \\(S_{t+\\Delta t} = S_t \\exp((r - 0.5\\sigma^2)\\Delta t + \\sigma \\sqrt{\\Delta t} Z_t)\\), where \\(Z_t \\sim N(0,1)\\).\nParameters: \\(S_0=100, K=100, r=0.05, \\sigma=0.2, T=1\\) (1 year). Discretize into \\(M=250\\) time steps (\\(\\Delta t = T/M\\)).\n\nR Implementation:\n\n\nWrite an R function simulate_one_path_R(S0, r, sigma, T, M) that simulates one price path and returns the average price over the \\(M\\) steps.\n\n\n\nWrite an R function price_asian_option_R(S0, K, r, sigma, T, M, n_sims) that:\n\n\nRuns n_sims simulations of the average price using simulate_one_path_R.\nCalculates the payoff for each simulation.\nReturns the discounted average payoff: \\(e^{-rT} \\cdot \\text{mean}(\\text{payoffs})\\).\n\n\n\nPrice the option with n_sims = 10000. Time it.\n\n\n\n\nC++ Implementation:\n\n\nWrite a C++ function simulate_one_path_Cpp(S0, r, sigma, T, M) (using Rcpp/RcppArmadillo for random normals like R::rnorm(0,1) or arma’s randn).\n\n\nWrite a C++ function price_asian_option_Cpp(S0, K, r, sigma, T, M, n_sims) that does the same as its R counterpart, calling simulate_one_path_Cpp.\n\n\nPrice the option using the C++ version with n_sims = 10000. Time it and compare with R.\n\n\n\n\n(Parallel) Use parallel::parLapply to run the R simulations (from part b) in parallel. Compare the time. (Export necessary functions/variables).\n\n\n\n\n\nsimulate_one_path_R &lt;- function(S0, r, sigma, T_val, M) {\n  dt &lt;- T_val / M\n  St_path &lt;- numeric(M + 1)\n  St_path[1] &lt;- S0\n  for (i in 1:M) {\n    Zt &lt;- rnorm(1)\n    St_path[i+1] &lt;- St_path[i] * exp((r - 0.5*sigma^2)*dt + sigma*sqrt(dt)*Zt)\n  }\n  return(mean(St_path[-1])) \n}\n\nprice_asian_option &lt;- function(S0, K, r, sigma, T_val, M, n_sims) {\n  payoffs &lt;- numeric(n_sims)\n  for (i in 1:n_sims) {\n    payoffs[i] &lt;- max(0, simulate_one_path_R(S0, r, sigma, T_val, M) - K)\n  }\n  # alternative\n  # avg_prices &lt;- replicate(n_sims, simulate_one_path_R(S0, r, sigma, T_val, M))\n  # payoffs &lt;- pmax(0, avg_prices - K)\n  #\n  return(exp(-r * T_val) * mean(payoffs))\n}\n\nprice_asian_option(100, 100, 0.05, 0.2, 1, 250, 1000)\n#&gt; [1] 5.762387",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Advanced exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html",
    "href": "extra_extra_exercises.html",
    "title": "\n30  Extra extra exercises\n",
    "section": "",
    "text": "31 Exercise Set 13: R Fundamentals and Data Structures (Review)",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#section",
    "href": "extra_extra_exercises.html#section",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n31.1 (1)\n",
    "text": "31.1 (1)\n\n\n\nCreate a sequence of numbers from 10 down to -10, decreasing by 0.5, and store it in a vector vSeq.\n\n\nHow many elements are in vSeq?\n\n\nCreate a logical vector bIsPositive which is TRUE for elements of vSeq greater than 0, and FALSE otherwise.\n\n\nCalculate the product of all elements in vSeq that are strictly between 1 and 5 (exclusive of 1 and 5).\n\n\n\nvSeq &lt;- seq(10, -10, -0.5)\nlength(vSeq)\n#&gt; [1] 41\nbIsPositive &lt;- vSeq &gt; 0\nprod(vSeq[vSeq &gt; 1 & vSeq &lt; 5])\n#&gt; [1] 1417.5",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#section-1",
    "href": "extra_extra_exercises.html#section-1",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n31.2 (2)\n",
    "text": "31.2 (2)\n\n\n\nConstruct a 4x4 matrix mIdentity which is an identity matrix.\n\n\nConstruct a 4x4 matrix mValues where elements are integers from 1 to 16, filled row-wise.\n\n\nCompute the matrix product mC = mIdentity %*% mValues. What is mC?\n\n\nReplace the diagonal elements of mValues with 0.\n\n\n\nmIdentity &lt;- diag(4)\nmValues &lt;- matrix(1:16, 4, 4, byrow = TRUE)\nmC &lt;- mIdentity %*% mValues\nmC\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    1    2    3    4\n#&gt; [2,]    5    6    7    8\n#&gt; [3,]    9   10   11   12\n#&gt; [4,]   13   14   15   16\ndiag(mValues) &lt;- 0\nmValues\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    0    2    3    4\n#&gt; [2,]    5    0    7    8\n#&gt; [3,]    9   10    0   12\n#&gt; [4,]   13   14   15    0",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#section-2",
    "href": "extra_extra_exercises.html#section-2",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n31.3 (3)\n",
    "text": "31.3 (3)\n\n\nWrite an R script that:\n\nInitializes a counter iCount to 0 and a sum dRunningSum to 0.\nUses a repeat loop. Inside the loop:\n\nGenerate a random integer between 1 and 20 (inclusive).\nAdd this integer to dRunningSum.\nIncrement iCount.\nIf dRunningSum exceeds 100 or iCount reaches 15, break the loop.\n\n\nPrint the final dRunningSum and iCount. Set seed to 777.\n\n\n\n\nset.seed(777)\n\niCount &lt;- 0\ndRunningSum &lt;- 0\n\nrepeat {\n  dRunningSum &lt;- dRunningSum + sample(1:20, 1)\n  iCount &lt;- iCount + 1\n  \n  if (dRunningSum &gt; 100 || iCount &gt; 15) {\n    break\n  }\n}\n\nprint(paste(\"Sum:\", dRunningSum, \"Count:\", iCount))\n#&gt; [1] \"Sum: 106 Count: 11\"",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#section-3",
    "href": "extra_extra_exercises.html#section-3",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n31.4 (4)\n",
    "text": "31.4 (4)\n\n\nCreate a list lExperiment with the following structure:\n\n\nsExpID: “EXP007”\n\ndParams: A named numeric vector with alpha = 0.05, beta = 1.2.\n\nlResults: An inner list containing:\n\n\nvObservations: c(10.2, 11.1, NA, 9.8, 10.5, NA, 10.9)\n\n\nsStatus: “Preliminary”\n\n\n\n\n\nAccess the beta parameter.\n\n\nCalculate the mean of vObservations, ignoring NAs.\n\n\nChange sStatus to “Completed”.\n\n\n\nlExperiment &lt;- list(\n  sExpID = \"EXP007\",\n  dParams = c(alpha = 0.05, beta = 1.2),\n  lResults = list(\n    vObservations = c(10.2, 11.1, NA, 9.8, 10.5, NA, 10.9),\n    sStatus = \"Preliminary\"\n  )\n)\n\nlExperiment[[\"dParams\"]][[\"beta\"]]\n#&gt; [1] 1.2\nmean(lExperiment[[\"lResults\"]][[\"vObservations\"]], na.rm = TRUE)\n#&gt; [1] 10.5\nlExperiment[[\"lResults\"]][[\"sStatus\"]] &lt;- \"Completed\"\nlExperiment[[\"lResults\"]][[\"sStatus\"]]\n#&gt; [1] \"Completed\"",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#section-4",
    "href": "extra_extra_exercises.html#section-4",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n31.5 (5)\n",
    "text": "31.5 (5)\n\n\n\nCreate a data frame dfSales with columns Month (character: “Jan”, “Feb”, “Mar”, “Apr”), ProductA_Units (numeric: 100, 120, 90, 110), ProductB_Units (numeric: 80, 85, 95, 70).\n\n\nAdd a new column Total_Units which is the sum of ProductA_Units and ProductB_Units.\n\n\nCreate a new column ProductA_Share which is ProductA_Units / Total_Units.\n\n\nFind the month(s) where ProductB_Units were greater than 90.\n\n\n\ndfSales &lt;- data.frame(\n  Month = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\"),\n  ProductA_Units = c(100, 120, 90, 110),\n  ProductB_Units = c(80, 85, 95, 70)\n)\ndfSales &lt;- transform(dfSales, ProductA_Share = ProductA_Units / (ProductA_Units + ProductB_Units))\ndfSales$Month[dfSales$ProductB_Units &gt; 90]\n#&gt; [1] \"Mar\"",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#section-5",
    "href": "extra_extra_exercises.html#section-5",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n31.6 (6)\n",
    "text": "31.6 (6)\n\n\nUsing ifelse(), create a character vector vCategory based on a numeric vector vScores = c(45, 88, 62, 95, 70, 55).\n\nvCategory should be “Fail” if score &lt; 50, “Pass” if 50 &lt;= score &lt; 70, “Merit” if 70 &lt;= score &lt; 90, and “Distinction” if score &gt;= 90. (Hint: you might need nested ifelse statements).\n\n\nvScores &lt;- c(45, 88, 62, 95, 70, 55)\nvCategory &lt;- ifelse(vScores &lt; 50, \"Fail\", ifelse(vScores &lt; 70, \"Pass\", ifelse(vScores &lt; 90, \"Merit\", \"Distinction\")))\nvCategory\n#&gt; [1] \"Fail\"        \"Merit\"       \"Pass\"        \"Distinction\" \"Merit\"      \n#&gt; [6] \"Pass\"",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#section-6",
    "href": "extra_extra_exercises.html#section-6",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n31.7 (7)\n",
    "text": "31.7 (7)\n\n\nWrite a for loop that iterates 10 times. In each iteration k:\n\nIf k is even, print k is even.\nIf k is odd and also a multiple of 3, print k is odd and a multiple of 3.\nOtherwise (if k is odd and not a multiple of 3), print k is odd.\n\n\n\n\nfor (k in 1:10) {\n  if (k %% 2 == 0) {\n    print(paste0(k, \" is even.\"))\n  } else if (k %% 3 == 0) {\n    print(paste0(k, \" is odd and a multiple of 3.\"))\n  } else {\n    print(paste0(k, \" is odd.\"))\n  }\n}\n#&gt; [1] \"1 is odd.\"\n#&gt; [1] \"2 is even.\"\n#&gt; [1] \"3 is odd and a multiple of 3.\"\n#&gt; [1] \"4 is even.\"\n#&gt; [1] \"5 is odd.\"\n#&gt; [1] \"6 is even.\"\n#&gt; [1] \"7 is odd.\"\n#&gt; [1] \"8 is even.\"\n#&gt; [1] \"9 is odd and a multiple of 3.\"\n#&gt; [1] \"10 is even.\"",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#section-7",
    "href": "extra_extra_exercises.html#section-7",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n31.8 (8)\n",
    "text": "31.8 (8)\n\n\n\nCreate a character vector vsSentences = c(\"R is fun\", \"Data analysis with R\", \"Missing data NA\", \"Another sentence\").\n\n\nUse grep() or grepl() to find which sentences contain the word “R” (case sensitive).\n\n\nUse sub() or gsub() to replace “R” with “R Language” in all sentences.\n\n\nSplit the second sentence (“Data analysis with R”) into individual words.\n\n\n\nvsSentences &lt;- c(\"R is fun\", \"Data analysis with R\", \"Missing data NA\", \"Another sentence\")\ngrep(\"R\", vsSentences)\n#&gt; [1] 1 2\nsub(\"R\", \"R Language\", vsSentences)\n#&gt; [1] \"R Language is fun\"             \"Data analysis with R Language\"\n#&gt; [3] \"Missing data NA\"               \"Another sentence\"\nstrsplit(vsSentences[2], \" \")\n#&gt; [[1]]\n#&gt; [1] \"Data\"     \"analysis\" \"with\"     \"R\"",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#section-8",
    "href": "extra_extra_exercises.html#section-8",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n32.1 (1)\n",
    "text": "32.1 (1)\n\n\nWrite an R function fNormalizeVector that takes a numeric vector vX as input.\nThe function should return a “normalized” vector where each element x_i is transformed to (x_i - mean(vX)) / sd(vX).\nThe function should handle cases where sd(vX) is 0 (e.g., if all elements are the same) by returning a vector of zeros or NAs, with a warning.\nTest with c(1,2,3,4,5) and c(5,5,5,5).\n\n\nfNormalizeVector &lt;- function(vX) {\n  if (sd(vX) == 0) {\n    warning(\"Standard deviation is 0 or all NA. Returning NAs or zeros.\")\n    return(rep(NA, length(vX))) # Or zeros: numeric(length(vX)\n  } else {\n    return((vX - mean(vX)) / sd(vX))\n  }\n}\n\nfNormalizeVector(c(1,2,3,4,5))\n#&gt; [1] -1.2649111 -0.6324555  0.0000000  0.6324555  1.2649111\nfNormalizeVector(c(5,5,5,5))\n#&gt; Warning in fNormalizeVector(c(5, 5, 5, 5)): Standard deviation is 0 or all NA.\n#&gt; Returning NAs or zeros.\n#&gt; [1] NA NA NA NA",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#section-9",
    "href": "extra_extra_exercises.html#section-9",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n32.2 (2)\n",
    "text": "32.2 (2)\n\n\nWhat is lexical scoping in R? Provide a simple example with a function defined inside another function, where the inner function uses a variable from the outer function’s environment.\n\n\nfOuter &lt;- function(a) {\n      b &lt;- a * 2\n      fInner &lt;- function(c) {\n        return(b + c) # 'b' is from fOuter's environment\n      }\n      return(fInner)\n    }\nmyInnerFunc &lt;- fOuter(10)\nresult &lt;- myInnerFunc(5)\nresult\n#&gt; [1] 25\n\nExplain what result will be and why.\nResult will be 25. fInner “closes over” the environment of fOuter where b (which is 20) was defined.",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#section-10",
    "href": "extra_extra_exercises.html#section-10",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n32.3 (3)\n",
    "text": "32.3 (3)\n\n\nWrite a function fCountValuesInRanges that takes a numeric vector vData and a vector of vBreakpoints (sorted) as input.\nThe function should count how many values in vData fall into each interval defined by vBreakpoints. For example, if vBreakpoints = c(0, 10, 20, 30), the intervals are (-Inf, 0], (0, 10], (10, 20], (20, 30], (30, Inf).\nImplement this using:\n\n\nA loop structure.\n\n\nR’s cut() or findInterval() function.\n\n\n\nCompare their outputs for vData = rnorm(1000, mean=15, sd=10) and vBreakpoints = c(0, 10, 20, 30).\n\n\n# a\nfCountValuesInRanges &lt;- function(vData, vBreakpoints) {\n  full_breaks &lt;- c(-Inf, sort(unique(vBreakpoints)), Inf)\n  counts &lt;- numeric(length(full_breaks) - 1)\n  for (val in vData) {\n    for (j in 1:(length(full_breaks) - 1)) {\n      if (val &gt; full_breaks[j] && val &lt;= full_breaks[j + 1]) {\n        counts[j] &lt;- counts[j] + 1\n        break\n      }\n    }\n  }\n  names(counts) &lt;- cut((full_breaks[-1] + full_breaks[-length(full_breaks)])/2 , breaks=full_breaks, include.lowest=TRUE, right=TRUE) # approx interval names\n  return(counts)\n}\n\nfCountValuesInRangesCut &lt;- function(vData, vBreakpoints) {\n  full_breaks &lt;- c(-Inf, sort(unique(vBreakpoints)), Inf)\n  return(table(cut(vData, breaks = full_breaks, include.lowest = TRUE, right = TRUE)))\n}\n\nvData &lt;- rnorm(1000, mean = 15, sd = 10)\nvBreakpoints &lt;- c(0, 10, 20, 30)\nfCountValuesInRanges(vData, vBreakpoints)\n#&gt;  [-Inf,0]    (0,10]   (10,20]   (20,30] (30, Inf] \n#&gt;        81       258       394       209        58\nfCountValuesInRangesCut(vData, vBreakpoints)\n#&gt; \n#&gt;  [-Inf,0]    (0,10]   (10,20]   (20,30] (30, Inf] \n#&gt;        81       258       394       209        58",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#section-11",
    "href": "extra_extra_exercises.html#section-11",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n32.4 (4)\n",
    "text": "32.4 (4)\n\n\nWrite a recursive R function fRecursiveSum that computes the sum of elements in a numeric vector vX without using the built-in sum() function or any loops. (Hint: sum of vX is vX[1] + sum of vX[-1]). Define the base case.\n\n\nfRecursiveSum &lt;- function(vX) {\n  if (length(vX) == 0) return(0)\n  return(vX[1] + fRecursiveSum(vX[-1]))\n}\nfRecursiveSum(1:5)\n#&gt; [1] 15",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#section-12",
    "href": "extra_extra_exercises.html#section-12",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n32.5 (5)\n",
    "text": "32.5 (5)\n\n\nCreate a list of matrices, where each matrix has 3 columns but a random number of rows (between 5 and 10) and contains random integers.\n\n\nset.seed(1)\nlMatrices &lt;- lapply(1:5, function(i) matrix(sample(1:50, sample(5:10,1)*3, replace=TRUE), ncol=3))\n\n\nUse lapply (or sapply if appropriate) to return a vector where each element is the sum of the diagonal elements (trace) of the corresponding matrix in lMatrices.\n\n\nsapply(lMatrices, function(x) sum(diag(x)))\n#&gt; [1]  64  67  92  68 125",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#section-13",
    "href": "extra_extra_exercises.html#section-13",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n32.6 (6)\n",
    "text": "32.6 (6)\n\n\nExplain the difference between apply(), lapply(), sapply(), and tapply(). For each, provide a very brief example of a situation where it would be the most appropriate choice.\n\napply(): applies function over margins of an array/matrix. Ex: apply(myMatrix, 1, sum) for row sums. lapply(): applies function to each element of a list, returns a list. Ex: lapply(myList, mean). sapply(): similar to lapply, but tries to simplify result to vec- tor/matrix. Ex: sapply(myList, length). tapply(): applies function to subsets of a vector, grouped by another vector (factor). Ex: tapply(iris$Sepal.Length, iris$Species, mean)",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#section-14",
    "href": "extra_extra_exercises.html#section-14",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n32.7 (7)\n",
    "text": "32.7 (7)\n\n\nConsider a task: for each number i from 1 to 1,000,000, if i is even, calculate i/2, if i is odd, calculate 3*i+1.\nImplement this using:\n\n\nA for loop storing results in a pre-allocated vector.\n\n\nA vectorized approach using ifelse().\n\n\n\nCompare their performance using microbenchmark.\n\n\nsuppressMessages(library(microbenchmark))\n#&gt; Warning: pakke 'microbenchmark' blev bygget under R version 4.3.3\n\nfLoopApproach &lt;- function(x) {\n  vResults &lt;- numeric(x)\n  for (i in 1:x) {\n    if (i %% 2 == 0) {\n      vResults[i] &lt;- i / 2\n    } else {\n      vResults[i] &lt;- 3 * i + 1\n    }\n  }\n  return(vResults)\n}\n\nfVectorizedApproach &lt;- function(x) {\n  return(ifelse(x %% 2 == 0, x / 2, 3 * x + 1))\n}\n\nmicrobenchmark(fLoopApproach(100000), fVectorizedApproach(100000))\n#&gt; Unit: microseconds\n#&gt;                        expr     min       lq      mean   median       uq\n#&gt;        fLoopApproach(1e+05) 18080.7 18672.05 19641.000 19553.15 19841.95\n#&gt;  fVectorizedApproach(1e+05)     1.4     2.10    30.531     2.85    14.55\n#&gt;      max neval\n#&gt;  28494.3   100\n#&gt;   2250.7   100",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#section-15",
    "href": "extra_extra_exercises.html#section-15",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n32.8 (8)\n",
    "text": "32.8 (8)\n\n\nYou have a data frame:\n\n\ndf &lt;- data.frame(\n      group = rep(c(\"A\", \"B\", \"C\"), each = 4),\n      value = rnorm(12)\n    )\n\n\nUsing tapply (or aggregate or dplyr if allowed), calculate the range (max - min) of value for each group.\n\n\ntapply(df$value, df$group, function(x) diff(range(x)))\n#&gt;        A        B        C \n#&gt; 2.854650 1.063354 2.165598",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#likelihood-of-a-poisson-process",
    "href": "extra_extra_exercises.html#likelihood-of-a-poisson-process",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n33.1 (1) Likelihood of a Poisson Process\n",
    "text": "33.1 (1) Likelihood of a Poisson Process\n\n\nA Poisson process models the number of events occurring in a fixed interval of time or space, given a constant average rate \\(\\lambda\\). The probability of observing \\(k\\) events is \\(P(K=k | \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\).\nSuppose you observe the following number of events in \\(n=5\\) independent intervals: vCounts = c(2, 3, 1, 2, 4).\n\nWrite an R function fPoissonLogLik that takes a rate parameter dLambda and the vector vCounts as input, and returns the total log-likelihood for the observed counts.\n\n\nThe MLE for \\(\\lambda\\) is simply the sample mean of the counts. Calculate this \\(\\hat{\\lambda}_{MLE}\\).\n\n\nPlot the log-likelihood function for \\(\\lambda \\in [0.1, 5]\\). Mark the MLE on your plot.\n\n\nUse optimize() to find the value of \\(\\lambda\\) that maximizes your fPoissonLogLik function (remember optimize minimizes by default). Compare it to your analytical MLE.\n\n\n(C++) Write a C++ function poisson_log_lik_point_cpp(int k, double lambda) that calculates the log-likelihood for a single count k and a given lambda. Then, write another C++ function total_poisson_log_lik_cpp(Rcpp::IntegerVector counts, double lambda) that iterates through the counts vector and sums the log-likelihoods using your point function. Test this against your R version.\n\n\n\nfPoissonLogLik &lt;- function(dLambda, vCounts) {\n  dSum &lt;- 0\n  for (i in 1:length(vCounts)) {\n    dSum &lt;- dSum + vCounts[i] * log(dLambda) - dLambda - factorial(vCounts[i])\n  }\n  return(dSum)\n}\n\nvCounts &lt;- c(2, 3, 1, 2, 4)\n\nvX &lt;- seq(0.1, 5, 0.01)\nplot(vX, fPoissonLogLik(vX, vCounts), type = \"l\")\nabline(v = mean(vCounts), col = \"red\")\n\n\n\n\n\n\n\nmean(vCounts)\n#&gt; [1] 2.4\noptimize(fPoissonLogLik, interval = c(0.1, 5), vCounts = vCounts, maximum = TRUE)\n#&gt; $maximum\n#&gt; [1] 2.400006\n#&gt; \n#&gt; $objective\n#&gt; [1] -36.49438\n\n\n// [[Rcpp::depends(RcppArmadillo)]]\n#include &lt;RcppArmadillo.h&gt;\nusing namespace Rcpp;\nusing namespace arma;\n\n//[[Rcpp::export]]\ndouble poisson_log_lik_cpp(int k, double lambda) {\n  return (k * log(lambda) - lambda - tgamma(k + 1));\n}\n\n//[[Rcpp::export]]\ndouble total_poisson_log_lik_cpp(Rcpp::IntegerVector counts, double lambda) {\n  double dSum = 0.0;\n  for (int i = 0; i &lt; counts.size(); i++) {\n    dSum += poisson_log_lik_cpp(counts[i], lambda);\n  }\n  return dSum;\n}\n\n\nsuppressMessages(library(Rcpp))\n#&gt; Warning: pakke 'Rcpp' blev bygget under R version 4.3.3\nsourceCpp(\"extra_extra_cpp.cpp\")\n\ntotal_poisson_log_lik_cpp(vCounts, mean(vCounts))\n#&gt; [1] -36.49438\nfPoissonLogLik(mean(vCounts), vCounts)\n#&gt; [1] -36.49438",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#optimization-of-a-bimodal-likelihood-reparameterization",
    "href": "extra_extra_exercises.html#optimization-of-a-bimodal-likelihood-reparameterization",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n33.2 (2) Optimization of a Bimodal Likelihood (Reparameterization)\n",
    "text": "33.2 (2) Optimization of a Bimodal Likelihood (Reparameterization)\n\n\nConsider a custom likelihood function (unnormalized PDF) that is known to be bimodal for a parameter \\(\\theta\\): \\(L(\\theta | \\text{data}) \\propto \\exp(-(\\theta^2 - 4)^2 + 0.5\\theta)\\).\n\nWrite an R function fBimodalNegLogLik for the negative log-likelihood (ignoring any normalizing constant for the data part, just use the function of \\(\\theta\\)).\n\n\nPlot this negative log-likelihood function for \\(\\theta \\in [-3, 3]\\) to observe its shape and local minima.\n\n\nUse optim() with method “BFGS” and several different starting values (e.g., -2, 0, 2) to find local minima. Do you find different minima?\n\n\nSuppose you know one mode is positive and one is negative. If you want to find the positive mode, how could you use reparameterization to restrict optim’s search? For example, use \\(\\theta = \\exp(\\tilde{\\theta})\\) to ensure \\(\\theta &gt; 0\\). Define the new negative log-likelihood in terms of \\(\\tilde{\\theta}\\) and optimize for \\(\\tilde{\\theta}\\). Transform the result back to \\(\\theta\\).\n\n\nSimilarly, to find the negative mode, you might use \\(\\theta = -\\exp(\\tilde{\\theta})\\). Implement this.\n\n\n\nfBimodalNegLogLik &lt;- function(dTheta) {\n  return(-(dTheta^2 - 4)^2 + 0.5 * dTheta)\n}\n\nvX &lt;- seq(-3, 3, 0.01)\nplot(vX, fBimodalNegLogLik(vX), type = \"l\")\n\n\n\n\n\n\n\noptim(-2, fBimodalNegLogLik, method = \"BFGS\")$par\n#&gt; [1] -1.147232e+15\noptim(0, fBimodalNegLogLik, method = \"BFGS\")$par\n#&gt; [1] -0.03125764\noptim(2, fBimodalNegLogLik, method = \"BFGS\")$par\n#&gt; [1] -1.160693e+34\n\nfBimodalNegLogLik_Pos &lt;- function(dTheta_tilde) {\n  dTheta &lt;- exp(dTheta_tilde)\n  return(-(dTheta^2 - 4)^2 + 0.5 * dTheta)\n}\n\ndTheta_tilde_optim &lt;- optim(0, fBimodalNegLogLik_Pos, method = \"BFGS\")$par\ndTheta_optim &lt;- exp(dTheta_tilde_optim)\ndTheta_optim\n#&gt; [1] 3.726639e-06\n\nfBimodalNegLogLik_Neg &lt;- function(dTheta_tilde) {\n  dTheta &lt;- -exp(dTheta_tilde)\n  return(-(dTheta^2 - 4)^2 + 0.5 * dTheta)\n}\n\ndTheta_tilde_optim &lt;- optim(0, fBimodalNegLogLik_Neg, method = \"BFGS\")$par\ndTheta_optim &lt;- -exp(dTheta_tilde_optim)\ndTheta_optim\n#&gt; [1] -1.01302e-05",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#likelihood-for-linear-regression-with-known-variance",
    "href": "extra_extra_exercises.html#likelihood-for-linear-regression-with-known-variance",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n33.3 (3) Likelihood for Linear Regression with Known Variance\n",
    "text": "33.3 (3) Likelihood for Linear Regression with Known Variance\n\n\nConsider a simple linear regression \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\), where \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). Assume \\(\\sigma^2 = 1\\) is known.\nParameters to estimate are \\(\\theta = (\\beta_0, \\beta_1)\\).\nData: set.seed(123); x_data = 1:20; y_data = 0.5 + 2*x_data + rnorm(20, 0, 1);\n\n\nWrite an R function fRegressNegLogLik(vBeta, vY, vX) that calculates the negative log-likelihood. vBeta is a vector c(beta0, beta1). (Hint: \\(L(\\beta_0, \\beta_1 | y, x, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - (\\beta_0 + \\beta_1 x_i))^2}{2\\sigma^2}\\right)\\)).\n\n\nUse optim() (e.g., method “Nelder-Mead” or “BFGS”) to find the MLEs for \\(\\beta_0\\) and \\(\\beta_1\\). Use starting values like c(0,0).\n\n\nCompare your estimates with the coefficients obtained from lm(y_data ~ x_data). They should be similar (though lm also estimates \\(\\sigma^2\\) if not fixed).\n\n\n(Conceptual) If \\(\\sigma^2\\) were also unknown, how would your parameter vector \\(\\theta\\) and your reparameterization strategy for \\(\\sigma^2\\) (to ensure positivity) change?\n\n\n\nset.seed(12)\nx_data &lt;- 1:20\ny_data &lt;- 0.5 + 2 * x_data + rnorm(20, 0, 1)\n\nfRegressNegLogLik &lt;- function(vBeta, vY, vX, dSigma2 = 1) {\n  dSum &lt;- 0\n  for (i in 1:length(vY)) {\n    dSum &lt;- dSum + (vY[i] - (vBeta[1] + vBeta[2] * vX[i]))^2 / (2 * dSigma2)\n  }\n  return(dSum)\n}\n\noptim(c(0, 0), fRegressNegLogLik, vY = y_data, vX = x_data, method = \"BFGS\")$par\n#&gt; [1] -0.3311065  2.0476093\n\ncoef(lm(y_data ~ x_data))\n#&gt; (Intercept)      x_data \n#&gt;  -0.3311065   2.0476093",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#optimization-with-constraints-using-constroptim-or-penalty",
    "href": "extra_extra_exercises.html#optimization-with-constraints-using-constroptim-or-penalty",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n33.4 (4) Optimization with Constraints using constrOptim (or Penalty)\n",
    "text": "33.4 (4) Optimization with Constraints using constrOptim (or Penalty)\n\n\nMinimize \\(f(x, y) = (x-1)^2 + (y-2)^2\\) subject to the linear constraint \\(x + y \\ge 4\\).\n\nMethod 1: constrOptim (if familiar or allowed as it’s base R but more advanced)\n\n\nThe constraint can be written as \\(1x + 1y - 4 \\ge 0\\). constrOptim requires constraints in the form \\(U\\theta - C \\ge 0\\). Define \\(U\\) and \\(C\\).\n\n\nUse constrOptim to solve this. You’ll need to provide the objective function f, its gradient (optional, constrOptim can approximate), ui (U matrix), and ci (C vector). Choose a starting point that satisfies the constraint e.g., (3,3).\n\n\n\n\nMethod 2: Penalty Function\n\n\nDefine a penalized objective function \\(f_p(x, y) = f(x,y) + P \\cdot \\max(0, 4 - (x+y))^2\\), where \\(P\\) is a large penalty constant (e.g., 1000).\n\n\nUse optim with “BFGS” or “Nelder-Mead” to minimize \\(f_p(x,y)\\) without explicit constraints.\n\n\nCompare results from both methods if possible. The analytical solution is \\((x,y) = (1.5, 2.5)\\).\n\n\n\n\n\nf &lt;- function(vX) {\n  return((vX[1] - 1)^2 + (vX[2] - 2)^2)\n}\n\np &lt;- function(vX) {\n  return(max(0, 4 - (vX[1] + vX[2]))^2)\n}\n\nf_p &lt;- function(vX, ...) {\n  return(f(vX) + 1000 * p(vX))\n}\n\noptim(c(3, 3), f_p, method = \"BFGS\")$par\n#&gt; [1] 1.499802 2.499780",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "extra_extra_exercises.html#profile-likelihood",
    "href": "extra_extra_exercises.html#profile-likelihood",
    "title": "\n30  Extra extra exercises\n",
    "section": "\n33.5 (5) Profile Likelihood\n",
    "text": "33.5 (5) Profile Likelihood\n\n\nRevisit the Poisson likelihood from Q1 (vCounts = c(2, 3, 1, 2, 4)). Suppose we have a more complex model with two parameters, \\(\\lambda_1\\) and \\(\\lambda_2\\), and the log-likelihood is \\(LL(\\lambda_1, \\lambda_2)\\).\nOften, we are interested in inference for one parameter (e.g., \\(\\lambda_1\\)) while treating the other as a nuisance parameter. The profile log-likelihood for \\(\\lambda_1\\) is defined as \\(PL(\\lambda_1) = \\max_{\\lambda_2} LL(\\lambda_1, \\lambda_2)\\).\nLet \\(LL(\\lambda_1, \\lambda_2) = \\sum_{i=1}^3 \\log(\\text{dpois}(vCounts[i], \\lambda_1)) + \\sum_{i=4}^5 \\log(\\text{dpois}(vCounts[i], \\lambda_2))\\). (This is an artificial separation for demonstration).\n\nWrite an R function fCombinedLogLik(vLambda, vC) where vLambda = c(lambda1, lambda2).\n\n\nWrite an R function fProfileLogLik_L1(dLambda1_fixed, vC) that, for a fixed dLambda1_fixed, uses optimize to find the dLambda2 that maximizes fCombinedLogLik (with that dLambda1_fixed), and returns this maximized log-likelihood value.\n\n\nCreate a sequence of dLambda1_fixed values (e.g., from 0.5 to 3.5). For each value, calculate fProfileLogLik_L1.\n\n\nPlot the profile log-likelihood \\(PL(\\lambda_1)\\) against dLambda1_fixed. The maximum of this plot gives an estimate for \\(\\lambda_1\\). From this plot, can you estimate an approximate confidence interval for \\(\\lambda_1\\)? (e.g., points where \\(PL(\\lambda_1) &gt; \\max(PL) - \\text{qchisq}(0.95, 1)/2\\)).\n\n\n\nvCounts &lt;- c(2, 3, 1, 2, 4)\n\nfCombinedLogLik &lt;- function(vLambda, vC) {\n  dSum1 &lt;- 0.0\n  dSum2 &lt;- 0.0\n  for (i in 1:3) {\n    dSum1 &lt;- dSum1 + log(dpois(vCounts[i], vLambda[1]))\n  }\n  for (i in 4:5) {\n    dSum2 &lt;- dSum2 + log(dpois(vCounts[i], vLambda[2]))\n  }\n  return(dSum1 + dSum2)\n}\n\nfProfileLogLik_L1 &lt;- function(dLambda1_fixed, vC) {\n  # Objective for optimize: function of lambda2\n  obj_for_lambda2 &lt;- function(dLambda2) {\n    fCombinedLogLik(c(dLambda1_fixed, dLambda2), vC)\n  }\n  # Optimize returns list, $maximum is the value of objective function\n  optimize(obj_for_lambda2, interval=c(0.01, 10), maximum=TRUE)$objective\n}\n\nlambda1_seq &lt;- seq(0.5, 4.5, 0.1)\nprofile_loglik_values &lt;- sapply(lambda1_seq, fProfileLogLik_L1, vC=vCounts)\n\nplot(lambda1_seq, profile_loglik_values, type=\"l\", xlab=\"Lambda1\", ylab=\"Profile Log-Likelihood\")\nabline(h = max(profile_loglik_values) - qchisq(0.95,1)/2, col=\"red\", lty=2)",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Extra extra exercises</span>"
    ]
  },
  {
    "objectID": "likelihood_exercises.html",
    "href": "likelihood_exercises.html",
    "title": "\n31  Likelihood Exercises\n",
    "section": "",
    "text": "32 Exercise Set 16: Advanced Econometric Likelihoods and Optimization",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Likelihood Exercises</span>"
    ]
  },
  {
    "objectID": "likelihood_exercises.html#probit-model-likelihood-medium-hard",
    "href": "likelihood_exercises.html#probit-model-likelihood-medium-hard",
    "title": "\n31  Likelihood Exercises\n",
    "section": "\n32.1 (1) Probit Model Likelihood (Medium-Hard)\n",
    "text": "32.1 (1) Probit Model Likelihood (Medium-Hard)\n\n\nA Probit model for a binary outcome \\(y_i \\in \\{0,1\\}\\) is given by \\(P(y_i=1 | \\mathbf{x}_i) = \\Phi(\\mathbf{x}_i'\\boldsymbol{\\beta})\\), where \\(\\Phi(\\cdot)\\) is the standard normal CDF, \\(\\mathbf{x}_i\\) is a vector of explanatory variables (including an intercept), and \\(\\boldsymbol{\\beta}\\) is a vector of parameters.\nThe log-likelihood for \\(n\\) observations is \\(LL(\\boldsymbol{\\beta}) = \\sum_{i=1}^n [y_i \\log(\\Phi(\\mathbf{x}_i'\\boldsymbol{\\beta})) + (1-y_i) \\log(1-\\Phi(\\mathbf{x}_i'\\boldsymbol{\\beta}))]\\).\nData:\n\n\nset.seed(123)\nN &lt;- 200\nX1 &lt;- rnorm(N)\nX2 &lt;- runif(N)\nX_matrix &lt;- cbind(1, X1, X2) # Design matrix with intercept\nbeta_true &lt;- c(-0.5, 1.2, -0.8)\nlinear_predictor &lt;- X_matrix %*% beta_true\nprob_y1 &lt;- pnorm(linear_predictor)\nY_binary &lt;- rbinom(N, 1, prob_y1)\n\n\n\nWrite an R function fProbitNegLogLik(vBeta, mX, vY) that calculates the negative log-likelihood for the Probit model. vBeta is the parameter vector, mX is the design matrix, and vY is the binary outcome vector.\n\n\nUse optim() with method “BFGS” to find the MLEs for \\(\\boldsymbol{\\beta}\\). Choose appropriate starting values (e.g., a vector of zeros).\n\n\nCompare your estimated \\(\\boldsymbol{\\beta}\\) with the coefficients from glm(Y_binary ~ X1 + X2, family = binomial(link = \"probit\")).\n\n\n(Hard) Analytically derive the gradient of the Probit log-likelihood with respect to \\(\\boldsymbol{\\beta}\\). Recall that \\(\\frac{d\\Phi(z)}{dz} = \\phi(z)\\) (standard normal PDF). The gradient for one observation is \\(\\frac{y_i - \\Phi(\\mathbf{x}_i'\\boldsymbol{\\beta})}{\\Phi(\\mathbf{x}_i'\\boldsymbol{\\beta})(1-\\Phi(\\mathbf{x}_i'\\boldsymbol{\\beta}))} \\phi(\\mathbf{x}_i'\\boldsymbol{\\beta}) \\mathbf{x}_i\\). Write an R function for this gradient (summed over observations) and use it in optim() via the gr argument. Does it converge faster or to a more precise solution?\n\n\n(C++) Implement the Probit negative log-likelihood calculation in C++ (e.g., probit_neg_log_lik_cpp(arma::vec beta, arma::mat X, arma::vec Y)). You can use R::pnorm and R::dnorm for \\(\\Phi\\) and \\(\\phi\\). Compare its speed to the R version for a single evaluation.\n\n\n\n#data\nset.seed(123)\nN &lt;- 200\nX1 &lt;- rnorm(N)\nX2 &lt;- runif(N)\nX_matrix &lt;- cbind(1, X1, X2) # matrix with intercept\nbeta_true &lt;- c(-0.5, 1.2, -0.8)\nlinear_predictor &lt;- X_matrix %*% beta_true\nprob_y1 &lt;- pnorm(linear_predictor)\nY_binary &lt;- rbinom(N, 1, prob_y1)\n\n#a \nfProbitNegLogLik &lt;- function(vBeta, mX, vY) {\n  n &lt;- length(vY)\n  dSum &lt;- 0\n  for (i in 1:n) {\n    dSum &lt;- dSum + vY[i] * log(pnorm(mX %*% vBeta))[i] + (1 - vY[i]) * log(1 - pnorm(mX %*% vBeta))[i]\n  }\n  return(-dSum)\n  # alternative\n  # return(-sum(vY * log(pnorm(mX %*% vBeta)) + (1 - vY) * log(1 - pnorm(mX %*% vBeta))))\n}\n\n#b\noptim(c(0, 0, 0), fProbitNegLogLik, mX = X_matrix, vY = Y_binary, method = \"BFGS\")$par\n#&gt; [1] -0.6124990  1.6593570 -0.7099176\n\n#c\nglm(Y_binary ~ X1 + X2, family = binomial(link = \"probit\"))$coefficients\n#&gt; (Intercept)          X1          X2 \n#&gt;  -0.6125001   1.6593555  -0.7099154\n\n#d\nfProbitGrad &lt;- function(vBeta, mX, vY) {\n  n &lt;- length(vY)\n  dSum &lt;- 0\n  dProb &lt;- pnorm(mX %*% vBeta)\n  for (i in 1:n) {\n    dSum &lt;- dSum + (vY[i] - dProb[i]) / (dProb[i] * (1 - dProb)[i]) * dnorm(mX %*% vBeta)[i] * mX[i, ]\n  }\n  return(-dSum)\n}\n\noptim(c(0, 0, 0), fProbitNegLogLik, gr = fProbitGrad, mX = X_matrix, vY = Y_binary, method = \"BFGS\")$par\n#&gt; [1] -0.6124989  1.6593568 -0.7099176\n\n\n// [[Rcpp::depends(RcppArmadillo)]]\n#include &lt;RcppArmadillo.h&gt;\nusing namespace Rcpp;\nusing namespace arma;\n\n// [[Rcpp::export]]\ndouble probit_neg_log_lik_cpp(vec beta, mat X, vec Y) {\n  int n = Y.size();\n  double dSum = 0.0;\n  vec linear_term = X * beta;\n  vec dProb = zeros&lt;vec&gt;(X.n_rows);\n    \n  for (int i = 0; i &lt; n; i++) {\n    dProb(i) = R::pnorm(linear_term(i), 0.0, 1.0, 1, 0);\n    dSum += Y[i] * log(dProb[i]) + (1 - Y[i]) * log(1 - dProb[i]);\n  }\n  return(-dSum);\n}\n\n\n#e\nsuppressMessages(library(Rcpp))\n#&gt; Warning: pakke 'Rcpp' blev bygget under R version 4.3.3\nsuppressMessages(library(RcppArmadillo))\n#&gt; Warning: pakke 'RcppArmadillo' blev bygget under R version 4.3.3\nsourceCpp(\"likelihood_exercises.cpp\")\n\nprobit_neg_log_lik_cpp(beta_true, X_matrix, Y_binary)\n#&gt; [1] 66.87324\nfProbitNegLogLik(beta_true, X_matrix, Y_binary)\n#&gt; [1] 66.87324",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Likelihood Exercises</span>"
    ]
  },
  {
    "objectID": "likelihood_exercises.html#garch11-model-likelihood-hard",
    "href": "likelihood_exercises.html#garch11-model-likelihood-hard",
    "title": "\n31  Likelihood Exercises\n",
    "section": "\n32.2 (2) GARCH(1,1) Model Likelihood (Hard)\n",
    "text": "32.2 (2) GARCH(1,1) Model Likelihood (Hard)\n\n\nRecall the GARCH(1,1) model: \\(r_t = \\sigma_t \\epsilon_t\\), where \\(\\epsilon_t \\sim N(0,1)\\) (iid) \\(\\sigma_t^2 = \\omega + \\alpha r_{t-1}^2 + \\beta \\sigma_{t-1}^2\\)\n\nParameters are \\(\\boldsymbol{\\theta} = (\\omega, \\alpha, \\beta)\\). Constraints: \\(\\omega &gt; 0, \\alpha \\ge 0, \\beta \\ge 0\\), and \\(\\alpha + \\beta &lt; 1\\) (for stationarity).\nThe conditional log-likelihood for \\(r_t\\) given past information (and \\(\\sigma_t^2\\)) is \\(-\\frac{1}{2} (\\log(2\\pi) + \\log(\\sigma_t^2) + \\frac{r_t^2}{\\sigma_t^2})\\). The total log-likelihood is the sum over \\(t=1, \\dots, T\\).\nFor \\(t=1\\), an initial \\(\\sigma_1^2\\) is needed. Often, the unconditional variance \\(\\sigma^2 = \\frac{\\omega}{1-\\alpha-\\beta}\\) is used if the process is stationary, or simply \\(r_0^2\\) is set to the sample variance of \\(r_t\\) and \\(\\sigma_1^2\\) is calculated using the GARCH equation from \\(r_0^2\\) and a starting \\(\\sigma_0^2\\) (e.g., also sample variance).\nData: Use S&P 500 daily log-returns. You can download data using quantmod package if allowed, or use a pre-saved dataset. For this exercise, let’s simulate some GARCH-like data:\n\n\nset.seed(456)\nT_garch &lt;- 500\nomega_true &lt;- 0.1; alpha_true &lt;- 0.1; beta_true &lt;- 0.85\nsigma2_garch &lt;- numeric(T_garch)\nr_garch &lt;- numeric(T_garch)\nsigma2_garch[1] &lt;- omega_true / (1 - alpha_true - beta_true) # Unconditional variance\nr_garch[1] &lt;- sqrt(sigma2_garch[1]) * rnorm(1)\nfor(t_idx in 2:T_garch) {\n  sigma2_garch[t_idx] &lt;- omega_true + alpha_true * r_garch[t_idx-1]^2 + beta_true * sigma2_garch[t_idx-1]\n  r_garch[t_idx] &lt;- sqrt(sigma2_garch[t_idx]) * rnorm(1)\n}\n\n\n\nWrite an R function fGARCH11NegLogLik(vParams, vReturns, sigma2_initial) that computes the negative log-likelihood. vParams is c(omega, alpha, beta). Handle the recursive calculation of \\(\\sigma_t^2\\).\n\n\n\nReparameterize the parameters to enforce constraints:\n\n\n\\(\\omega = \\exp(\\tilde{\\omega})\\)\n\\(\\alpha = \\exp(\\tilde{\\alpha}) / (1 + \\exp(\\tilde{\\alpha}) + \\exp(\\tilde{\\beta}))\\)\n\n\\(\\beta = \\exp(\\tilde{\\beta}) / (1 + \\exp(\\tilde{\\alpha}) + \\exp(\\tilde{\\beta}))\\) (This ensures \\(\\omega&gt;0, \\alpha&gt;0, \\beta&gt;0, \\alpha+\\beta &lt; 1\\). A simpler common reparam is \\(\\alpha=\\exp(\\tilde{\\alpha})/(1+\\exp(\\tilde{\\alpha}))\\) and \\(\\beta=\\exp(\\tilde{\\beta})/(1+\\exp(\\tilde{\\beta}))\\), then check \\(\\alpha+\\beta&lt;1\\) inside LL and return large penalty if not met, or use L-BFGS-B on transformed \\(\\alpha, \\beta\\) in [0,1] and \\(\\omega&gt;0\\), with an additional check for sum &lt; 1). Modify your likelihood function to take reparameterized inputs \\(\\tilde{\\boldsymbol{\\theta}} = (\\tilde{\\omega}, \\tilde{\\alpha}, \\tilde{\\beta})\\).\n\n\n\nUse optim() with “BFGS” to find the MLEs of \\(\\tilde{\\boldsymbol{\\theta}}\\) using r_garch and an appropriate sigma2_initial (e.g., sample variance of r_garch).\n\n\nTransform the estimated \\(\\tilde{\\boldsymbol{\\theta}}^*\\) back to \\(\\boldsymbol{\\theta}^* = (\\omega^*, \\alpha^*, \\beta^*)\\). Compare with omega_true, alpha_true, beta_true.\n\n\n(Extremely Hard C++) Implement the GARCH(1,1) negative log-likelihood calculation in C++ using RcppArmadillo. This will involve a loop to calculate the \\(\\sigma_t^2\\) sequence. Compare its speed for a single evaluation against the R version.\n\n\n\n#data \n\nset.seed(456)\nT_garch &lt;- 500\nomega_true &lt;- 0.1; alpha_true &lt;- 0.1; beta_true &lt;- 0.85\nsigma2_garch &lt;- numeric(T_garch)\nr_garch &lt;- numeric(T_garch)\nsigma2_garch[1] &lt;- omega_true / (1 - alpha_true - beta_true) # unconditional variance\nr_garch[1] &lt;- sqrt(sigma2_garch[1]) * rnorm(1)\nfor (t_idx in 2:T_garch) {\n  sigma2_garch[t_idx] &lt;- omega_true + alpha_true * r_garch[t_idx-1]^2 + beta_true * sigma2_garch[t_idx-1]\n  r_garch[t_idx] &lt;- sqrt(sigma2_garch[t_idx]) * rnorm(1)\n}\n\n#a\nfGARCH11NegLogLik &lt;- function(vParams, vReturns, sigma2_initial) {\n  dOmega &lt;- vParams[1]\n  dAlpha &lt;- vParams[2]\n  dBeta &lt;- vParams[3]\n  \n  dSum &lt;- 0\n  vSigma2 &lt;- numeric(length(vReturns))\n  vSigma2[1] &lt;- sigma2_initial\n  for (t in 2:length(vReturns)) {\n    vSigma2[t] &lt;- dOmega + dAlpha * vReturns[t-1]^2 + dBeta * vSigma2[t-1]\n    dSum &lt;- dSum - 0.5*(log(2*pi) + log(vSigma2[t]) + vReturns[t]^2 / vSigma2[t])\n  }\n  return(-dSum)\n}\n\n#b\nfGARCH11NegLogLikRepar &lt;- function(vParams, vReturns, sigma2_initial) {\n  dOmega &lt;- exp(vParams[1])\n  dAlpha &lt;- exp(vParams[2]) / (1 + exp(vParams[2]) + exp(vParams[3]))\n  dBeta &lt;- exp(vParams[3]) / (1 + exp(vParams[2]) + exp(vParams[3]))\n  \n  dSum &lt;- 0\n  vSigma2 &lt;- numeric(length(vReturns))\n  vSigma2[1] &lt;- sigma2_initial\n  for (t in 2:length(vReturns)) {\n    vSigma2[t] &lt;- dOmega + dAlpha * vReturns[t-1]^2 + dBeta * vSigma2[t-1]\n    dSum &lt;- dSum - 0.5*(log(2*pi) + log(vSigma2[t]) + vReturns[t]^2 / vSigma2[t])\n  }\n  return(-dSum)\n}\n\n#c\ndPar_tilde &lt;- optim(c(0, 0, 0), fGARCH11NegLogLikRepar, sigma2_initial = var(r_garch), vReturns = r_garch, method = \"BFGS\")$par\n\n\n#d\ndOmega_star &lt;- exp(dPar_tilde[1])\ndAlpha_star &lt;- exp(dPar_tilde[2]) / (1 + exp(dPar_tilde[2]) + exp(dPar_tilde[3]))\ndBeta_star &lt;- exp(dPar_tilde[3]) / (1 + exp(dPar_tilde[2]) + exp(dPar_tilde[3]))\n\nprint(paste0(\"Omega est: \", dOmega_star, \" vs. true: \", omega_true))\n#&gt; [1] \"Omega est: 0.188330580842483 vs. true: 0.1\"\nprint(paste0(\"Alpha est: \", dAlpha_star, \" vs. true: \", alpha_true))\n#&gt; [1] \"Alpha est: 0.120768680865144 vs. true: 0.1\"\nprint(paste0(\"Beta est: \", dBeta_star, \" vs. true: \", beta_true))\n#&gt; [1] \"Beta est: 0.768485839334849 vs. true: 0.85\"\n\n\ndouble fGARCH11NegLogLik_cpp(vec vParams, vec vReturns, double sigma2_initial) {\n  double dOmega = vParams[0];\n  double dAlpha = vParams[1];\n  double dBeta = vParams[2];\n  int n = vReturns.size();\n  \n  double dSum = 0;\n  vec vSigma2 = zeros&lt;vec&gt;(vReturns.size());\n  vSigma2[0] = sigma2_initial;\n  double dPi = atan(1)*4;\n  \n  for (int t = 1; t &lt; n; t++) {\n    vSigma2[t] = dOmega + dAlpha * pow(vReturns[t-1], 2) + dBeta * vSigma2[t-1];\n    dSum = dSum - 0.5*(log(2*dPi) + log(vSigma2[t]) + pow(vReturns[t], 2) / vSigma2[t]);\n  }\n  return(-dSum);\n}\n\n\nsuppressMessages(library(Rcpp))\nsuppressMessages(library(RcppArmadillo))\nsourceCpp(\"likelihood_exercises.cpp\")\n\nfGARCH11NegLogLik(c(omega_true, alpha_true, beta_true), r_garch, var(r_garch))\n#&gt; [1] 830.2781\nfGARCH11NegLogLik_cpp(c(omega_true, alpha_true, beta_true), r_garch, var(r_garch))\n#&gt; [1] 830.2781",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Likelihood Exercises</span>"
    ]
  },
  {
    "objectID": "likelihood_exercises.html#heckman-selection-model-likelihood-extremely-hard",
    "href": "likelihood_exercises.html#heckman-selection-model-likelihood-extremely-hard",
    "title": "\n31  Likelihood Exercises\n",
    "section": "\n32.3 (3) Heckman Selection Model Likelihood (Extremely Hard)\n",
    "text": "32.3 (3) Heckman Selection Model Likelihood (Extremely Hard)\n\n\nThe Heckman model addresses sample selection bias. It involves two equations:\n\nSelection equation (Probit): \\(z_i^* = \\mathbf{w}_i'\\boldsymbol{\\gamma} + u_i\\), where \\(z_i=1\\) if \\(z_i^* &gt; 0\\) (observed), \\(0\\) otherwise. \\(u_i \\sim N(0,1)\\).\nOutcome equation (Linear regression, observed only if \\(z_i=1\\)): \\(y_i = \\mathbf{x}_i'\\boldsymbol{\\beta} + \\epsilon_i\\), where \\(\\epsilon_i \\sim N(0, \\sigma^2)\\).\n\n\n\n\\(u_i\\) and \\(\\epsilon_i\\) are bivariate normal with correlation \\(\\rho\\). \\(corr(u_i, \\epsilon_i) = \\rho\\).\n\n\nThe log-likelihood for one observation \\(i\\) (where \\(y_i\\) is observed if \\(z_i=1\\)) is:\n\nIf \\(z_i=0\\): \\(LL_i = \\log(1 - \\Phi(\\mathbf{w}_i'\\boldsymbol{\\gamma}))\\)\n\nIf \\(z_i=1\\): \\(LL_i = \\log \\left( \\frac{1}{\\sigma} \\phi\\left(\\frac{y_i - \\mathbf{x}_i'\\boldsymbol{\\beta}}{\\sigma}\\right) \\Phi\\left(\\frac{\\mathbf{w}_i'\\boldsymbol{\\gamma} + \\frac{\\rho}{\\sigma}(y_i - \\mathbf{x}_i'\\boldsymbol{\\beta})}{\\sqrt{1-\\rho^2}}\\right) \\right)\\)\n\n\n\nParameters are \\(\\boldsymbol{\\theta} = (\\boldsymbol{\\gamma}', \\boldsymbol{\\beta}', \\sigma, \\rho)'\\). Constraints: \\(\\sigma &gt; 0, -1 &lt; \\rho &lt; 1\\).\nData: You would typically simulate this.\n\n\nset.seed(789)\nN_heck &lt;- 300\nW1 &lt;- rnorm(N_heck); W_matrix &lt;- cbind(1, W1) # Selection vars\nX1 &lt;- rnorm(N_heck); X_matrix_outcome &lt;- cbind(1, X1) # Outcome vars\ngamma_true &lt;- c(0.5, -1); beta_true &lt;- c(1, 2); sigma_true &lt;- 1.5; rho_true &lt;- 0.6\n    \n# Simulate errors\nerrors &lt;- MASS::mvrnorm(N_heck, mu=c(0,0), Sigma=matrix(c(1, rho_true*sigma_true, rho_true*sigma_true, sigma_true^2),2,2))\nu_i &lt;- errors[,1]\nepsilon_i &lt;- errors[,2]\n    \nz_star &lt;- W_matrix %*% gamma_true + u_i\nZ_select &lt;- ifelse(z_star &gt; 0, 1, 0)\n    \nY_outcome &lt;- X_matrix_outcome %*% beta_true + epsilon_i\nY_observed &lt;- ifelse(Z_select == 1, Y_outcome, NA) # Only observe Y if Z_select is 1\ndf_heckman &lt;- data.frame(Y_observed, X1, W1, Z_select)\ndf_heckman_obs &lt;- subset(df_heckman, Z_select == 1) # data for outcome eq.\n\n\n\nWrite an R function fHeckmanNegLogLik(vParams, dfData, mSelectionVars, mOutcomeVars) that calculates the total negative log-likelihood. vParams contains all parameters. dfData should contain the outcome \\(y\\), the selection indicator \\(z\\), and covariates. mSelectionVars and mOutcomeVars are column names/indices for \\(\\mathbf{w}\\) and \\(\\mathbf{x}\\).\n\n\nImplement reparameterizations for \\(\\sigma = \\exp(\\tilde{\\sigma})\\) and \\(\\rho = \\tanh(\\tilde{\\rho}) = \\frac{e^{\\tilde{\\rho}} - e^{-\\tilde{\\rho}}}{e^{\\tilde{\\rho}} + e^{-\\tilde{\\rho}}}\\). Modify your likelihood function.\n\n\nUse optim() (“BFGS” or “Nelder-Mead” due to complexity) to estimate the reparameterized parameters using the simulated data. This is notoriously difficult to optimize; choose starting values carefully (e.g., from separate Probit and OLS on selected sample, \\(\\tilde{\\sigma}=\\log(\\text{sd-res-ols})\\), \\(\\tilde{\\rho}=0\\)).\n\n\nTransform estimates back and compare to true values.\n\n(This problem is very challenging due to the complexity of the likelihood and potential for numerical instability. Focus on correctly writing the likelihood parts).\n\n\n#data\nset.seed(789)\nN_heck &lt;- 300\nW1 &lt;- rnorm(N_heck); W_matrix &lt;- cbind(1, W1) # Selection vars\nX1 &lt;- rnorm(N_heck); X_matrix_outcome &lt;- cbind(1, X1) # Outcome vars\ngamma_true &lt;- c(0.5, -1); beta_true &lt;- c(1, 2); sigma_true &lt;- 1.5; rho_true &lt;- 0.6\n    \n# Simulate errors\nerrors &lt;- MASS::mvrnorm(N_heck, mu=c(0,0), Sigma=matrix(c(1, rho_true*sigma_true, rho_true*sigma_true, sigma_true^2),2,2))\nu_i &lt;- errors[,1]\nepsilon_i &lt;- errors[,2]\n    \nz_star &lt;- W_matrix %*% gamma_true + u_i\nZ_select &lt;- ifelse(z_star &gt; 0, 1, 0)\n    \nY_outcome &lt;- X_matrix_outcome %*% beta_true + epsilon_i\nY_observed &lt;- ifelse(Z_select == 1, Y_outcome, NA) # Only observe Y if Z_select is 1\ndf_heckman &lt;- data.frame(Y_observed, X1, W1, Z_select)\ndf_heckman_obs &lt;- subset(df_heckman, Z_select == 1) # data for outcome eq.\n\n#a + b\nfHeckmanNegLogLik &lt;- function(vParams, dfData, mSelectionVars, mOutcomeVars) {\n  #vParams contains all parameters.\n  #dfData should contain the outcome y, the selection indicator z, and covariates.\n  #mSelectionVars and mOutcomeVars are column names/indices for w and x.\n  vGamma &lt;- vParams[1:2]\n  vBeta &lt;- vParams[3:4]\n  dSigma &lt;- exp(vParams[5])\n  dRho &lt;- (exp(vParams[6]) - exp(-vParams[6])) / (exp(vParams[6]) + exp(-vParams[6]))\n  \n  vY &lt;- dfData[, mOutcomeVars]\n  vZ &lt;- dfData[, mSelectionVars]\n  vX &lt;- cbind(1, dfData$X1)\n  vW &lt;- cbind(1, dfData$W1)\n  dLlSum &lt;- 0\n  for (i in 1:length(vY)) {\n    if (vZ[i] == 0) {\n      dLlSum &lt;- dLlSum + log(1 - pnorm(vW %*% vGamma)[i])\n    } else {\n      dLlSum &lt;- dLlSum + log(1 / dSigma * dnorm((vY[i] - (vX %*% vBeta)[i]) / dSigma) * pnorm(((vW %*% vGamma)[i] + dRho / dSigma * (vY[i] - (vX %*% vBeta)[i])) / sqrt(1 - dRho^2)))\n    }\n  }\n  return(-dLlSum)\n}\n\n# c\nvParams &lt;- optim(c(c(0, 0), c(0, 0), 0, 0), fHeckmanNegLogLik, dfData = df_heckman, mSelectionVars = 4, mOutcomeVars = 1, method = \"BFGS\")$par\n\n# d\ngamma_star &lt;- vParams[1:2]\nbeta_star &lt;- vParams[3:4]\nsigma_star &lt;- exp(vParams[5])\nrho_star &lt;- (exp(vParams[6]) - exp(-vParams[6])) / (exp(vParams[6]) + exp(-vParams[6]))\n\nprint(paste0(\"Gamma est: \", gamma_star, \" vs. true: \", gamma_true))\n#&gt; [1] \"Gamma est: 0.434869076735787 vs. true: 0.5\"\n#&gt; [2] \"Gamma est: -1.17799006585571 vs. true: -1\"\nprint(paste0(\"Beta est: \", beta_star, \" vs. true: \", beta_true))\n#&gt; [1] \"Beta est: 0.964253846625409 vs. true: 1\"\n#&gt; [2] \"Beta est: 1.921924920477 vs. true: 2\"\nprint(paste0(\"Sigma est: \", sigma_star, \" vs. true: \", sigma_true))\n#&gt; [1] \"Sigma est: 1.53499526849203 vs. true: 1.5\"\nprint(paste0(\"Rho est: \", rho_star, \" vs. true. \", rho_true))\n#&gt; [1] \"Rho est: 0.597269354835849 vs. true. 0.6\"",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Likelihood Exercises</span>"
    ]
  },
  {
    "objectID": "likelihood_exercises.html#vector-autoregression-var-likelihood-multivariate-normal-hard",
    "href": "likelihood_exercises.html#vector-autoregression-var-likelihood-multivariate-normal-hard",
    "title": "\n31  Likelihood Exercises\n",
    "section": "\n32.4 (4) Vector Autoregression (VAR) Likelihood (Multivariate Normal) (Hard)\n",
    "text": "32.4 (4) Vector Autoregression (VAR) Likelihood (Multivariate Normal) (Hard)\n\n\nConsider a VAR(1) model for two variables \\(y_{1t}, y_{2t}\\): \\(\\mathbf{y}_t = \\mathbf{c} + A \\mathbf{y}_{t-1} + \\boldsymbol{\\epsilon}_t\\), where \\(\\mathbf{y}_t = \\begin{pmatrix} y_{1t} \\\\ y_{2t} \\end{pmatrix}\\), \\(\\mathbf{c} = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}\\), \\(A = \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix}\\), and \\(\\boldsymbol{\\epsilon}_t \\sim N(\\mathbf{0}, \\Sigma)\\), with \\(\\Sigma = \\begin{pmatrix} \\sigma_{11} & \\sigma_{12} \\\\ \\sigma_{21} & \\sigma_{22} \\end{pmatrix}\\) being symmetric positive definite.\nThe conditional log-likelihood for one observation \\(\\mathbf{y}_t\\) given \\(\\mathbf{y}_{t-1}\\) is: \\(LL_t = -\\frac{k}{2}\\log(2\\pi) - \\frac{1}{2}\\log(|\\Sigma|) - \\frac{1}{2}(\\mathbf{y}_t - \\mathbf{c} - A \\mathbf{y}_{t-1})'\\Sigma^{-1}(\\mathbf{y}_t - \\mathbf{c} - A \\mathbf{y}_{t-1})\\), where \\(k=2\\) (number of variables).\nParameters: \\(\\mathbf{c}\\) (2 params), \\(A\\) (4 params), and unique elements of \\(\\Sigma\\) (3 params: \\(\\sigma_{11}, \\sigma_{22}, \\sigma_{12}\\) as \\(\\sigma_{21}=\\sigma_{12}\\)). Total 9 parameters.\nConstraints: \\(\\Sigma\\) must be positive definite. (Often ensure by parameterizing Cholesky factor \\(L\\) s.t. \\(\\Sigma = LL'\\)).\nData: Use Canada dataset from vars package, first two variables e and prod, differenced.\n\n\nlibrary(vars)\ndata(Canada)\nCanada_diff &lt;- data.frame(apply(Canada[,c(\"e\",\"prod\")], 2, diff))\nY_var_data &lt;- as.matrix(Canada_diff)\nT_var &lt;- nrow(Y_var_data) -1 # Number of observations for LL sum\n\n(Alternatively, simulate VAR data)\n\n\nWrite an R function fVAR1NegLogLik(vParams, mY) for the negative log-likelihood. vParams needs to be carefully unpacked into \\(\\mathbf{c}, A, \\Sigma\\).\n\n\nFor \\(\\Sigma\\), parameterize its Cholesky factor \\(L = \\begin{pmatrix} l_{11} & 0 \\\\ l_{21} & l_{22} \\end{pmatrix}\\). To ensure \\(l_{11}&gt;0, l_{22}&gt;0\\), use \\(l_{11}=\\exp(\\tilde{l}_{11}), l_{22}=\\exp(\\tilde{l}_{22})\\). \\(l_{21}\\) is unconstrained. \\(\\Sigma = LL'\\). The parameters in vParams related to \\(\\Sigma\\) will be \\((\\tilde{l}_{11}, l_{21}, \\tilde{l}_{22})\\).\n\n\nUse optim() to estimate the reparameterized parameters. This will be slow and sensitive to starting values. (OLS estimates can be good starting values for \\(c\\) and \\(A\\)).\n\n\n(Conceptual) How would you derive the gradient for this likelihood? (Matrix calculus needed).\n\n\n(C++ RcppArmadillo) Implement the VAR(1) negative log-likelihood calculation in C++. This would involve matrix operations for which Armadillo is well-suited (arma::solve, arma::log_det).\n\n\n\n# data\nsuppressMessages(library(vars))\n#&gt; Warning: pakke 'vars' blev bygget under R version 4.3.3\n#&gt; Warning: pakke 'strucchange' blev bygget under R version 4.3.3\n#&gt; Warning: pakke 'zoo' blev bygget under R version 4.3.3\n#&gt; Warning: pakke 'sandwich' blev bygget under R version 4.3.3\n#&gt; Warning: pakke 'urca' blev bygget under R version 4.3.3\n#&gt; Warning: pakke 'lmtest' blev bygget under R version 4.3.3\ndata(Canada)\nCanada_diff &lt;- data.frame(apply(Canada[,c(\"e\",\"prod\")], 2, diff))\nY_var_data &lt;- as.matrix(Canada_diff)\nT_var &lt;- nrow(Y_var_data) -1 # Number of observations for LL sum\n\n# a + b\nfVAR1NegLogLik &lt;- function(vParams, mY) {\n  vC &lt;- vParams[1:2]\n  mA &lt;- matrix(vParams[3:6], 2, 2)\n  mSigma &lt;- matrix(c(exp(vParams[7]), vParams[8], 0, exp(vParams[9])), 2, 2)\n  mSigma &lt;- mSigma %*% t(mSigma)\n  iK &lt;- ncol(mY)\n  \n  dSum &lt;- 0\n  for (t in 2:nrow(mY)) {\n    dSum &lt;- dSum - iK / 2 * log(2 * pi) - 0.5 * log(det(mSigma)) - 0.5 * t(mY[t, ] - vC - mA %*% mY[t - 1, ]) %*% solve(mSigma) %*% (mY[t, ] - vC - mA %*% mY[t - 1, ])\n  }\n  return(-dSum)\n}\n\n# c\nstart_c_var &lt;- c(0,0)\nstart_A_var &lt;- as.vector(t(diag(0.5, 2))) \nstart_L_tilde_var &lt;- c(log(1), 0, log(1))\nstart_params_var &lt;- c(start_c_var, start_A_var, start_L_tilde_var)\noptim_res_var &lt;- optim(start_params_var, fVAR1NegLogLik, mY = Y_var_data, method = \"L-BFGS-B\", lower = c(rep(-Inf, 6), -30, -Inf, -30), upper = c(rep(Inf, 9)))\noptim_res_var$par\n#&gt; [1]  0.10033694  0.06371892  0.67883561  0.12431749  0.19044272  0.27667404\n#&gt; [7] -0.96917510  0.01153122 -0.38346578\n\n\ndouble fVAR1NegLogLik(vec vParams, mat mY) {\n  vec vC = zeros&lt;vec&gt;(2);\n  vC[0] = vParams[0];\n  vC[1] = vParams[1];\n  mat mA(2,2);\n  mA(0,0) = vParams[2];\n  mA(1,0) = vParams[3];\n  mA(0,1) = vParams[4];\n  mA(1,1) = vParams[5];\n  mat mSigma(2,2);\n  mSigma(0,0) = exp(vParams[6]);\n  mSigma(0,1) = 0;\n  mSigma(1,0) = vParams[7];\n  mSigma(1,1) = exp(vParams[8]);\n  mSigma = mSigma * mSigma.t();\n  double iK = mY.n_cols;\n  double dPi = atan(1)*4;\n  \n  double dSum = 0.0;\n  for (arma::uword t = 1; t &lt; mY.n_rows; t++) {\n    vec u_t = mY.row(t).t() - vC - mA * mY.row(t-1).t();\n    dSum = dSum - iK / 2.0 * log(2.0 * dPi) - 0.5 * log(det(mSigma)) - 0.5 * as_scalar(trans(u_t) * mSigma.i() * (u_t));\n  }\n  \n  return(-dSum);\n}\n\n\nsuppressMessages(library(Rcpp))\nsuppressMessages(library(RcppArmadillo))\nsourceCpp(\"likelihood_exercises.cpp\")",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Likelihood Exercises</span>"
    ]
  },
  {
    "objectID": "likelihood_exercises.html#logistic-regression-full-implementation-hard",
    "href": "likelihood_exercises.html#logistic-regression-full-implementation-hard",
    "title": "\n31  Likelihood Exercises\n",
    "section": "\n33.1 (1) Logistic Regression: Full Implementation (Hard)\n",
    "text": "33.1 (1) Logistic Regression: Full Implementation (Hard)\n\n\nA Logistic model for a binary outcome \\(y_i \\in \\{0,1\\}\\) is \\(P(y_i=1 | \\mathbf{x}_i) = \\Lambda(\\mathbf{x}_i'\\boldsymbol{\\beta}) = \\frac{\\exp(\\mathbf{x}_i'\\boldsymbol{\\beta})}{1+\\exp(\\mathbf{x}_i'\\boldsymbol{\\beta})}\\).\nThe log-likelihood is \\(LL(\\boldsymbol{\\beta}) = \\sum_{i=1}^n [y_i (\\mathbf{x}_i'\\boldsymbol{\\beta}) - \\log(1+\\exp(\\mathbf{x}_i'\\boldsymbol{\\beta}))]\\).\nData (same as Probit example):\n\n\nset.seed(123)\nN_log &lt;- 200\nX1_log &lt;- rnorm(N_log)\nX2_log &lt;- runif(N_log)\nX_matrix_log &lt;- cbind(1, X1_log, X2_log) \nbeta_true_log &lt;- c(-0.5, 1.2, -0.8)\nlinear_predictor_log &lt;- X_matrix_log %*% beta_true_log\nprob_y1_log &lt;- exp(linear_predictor_log) / (1 + exp(linear_predictor_log))\nY_binary_log &lt;- rbinom(N_log, 1, prob_y1_log)\n\n\n\nTasks:\n\n\n\nR Likelihood: Write an R function fLogisticNegLogLik(vBeta, mX, vY) for the negative log-likelihood.\n\n\n\n\nR Gradient & Hessian: Analytically derive the gradient vector and Hessian matrix of the log-likelihood.\n\n\nGradient: \\(\\nabla LL(\\boldsymbol{\\beta}) = \\sum_{i=1}^n (y_i - \\Lambda(\\mathbf{x}_i'\\boldsymbol{\\beta})) \\mathbf{x}_i\\).\nHessian: \\(H(\\boldsymbol{\\beta}) = -\\sum_{i=1}^n \\Lambda(\\mathbf{x}_i'\\boldsymbol{\\beta})(1-\\Lambda(\\mathbf{x}_i'\\boldsymbol{\\beta})) \\mathbf{x}_i \\mathbf{x}_i'\\). Write R functions fLogisticGradient(vBeta, mX, vY) and fLogisticHessian(vBeta, mX, vY). For the Hessian, return the negative of it if your Newton-Raphson is set up for maximization using \\(H^{-1}g\\).\n\n\n\n\nR Newton-Raphson: Implement the Newton-Raphson algorithm (for maximization) using your functions from (a)-(b) to find the MLEs for \\(\\boldsymbol{\\beta}\\). Your Newton function should take the target function (negative LL for minimization, or LL for maximization), gradient, Hessian, starting values, tolerance, and max iterations. It should return a list with optimal parameters, function value, and convergence status.\n\n\n\noptim() Comparison: Use optim() with method “BFGS” and your negative log-likelihood from (a) and gradient from (b) (negated if optim minimizes). Compare the results and number of iterations/evaluations with your Newton-Raphson.\n\n\n\nC++ Likelihood: Implement fLogisticNegLogLik in C++ (logistic_neg_log_lik_cpp) using RcppArmadillo. Use arma::log1p(exp(Xb)) for \\(\\log(1+\\exp(\\mathbf{x}_i'\\boldsymbol{\\beta}))\\) for numerical stability if \\(\\mathbf{x}_i'\\boldsymbol{\\beta}\\) is large. Compare speed of one evaluation.\n\n\n\nR Package: Create a small R package named “MyLogistic” containing your R functions fLogisticNegLogLik, fLogisticGradient, fLogisticHessian, your Newton-Raphson solver, and the C++ likelihood function (with an R wrapper). Document the main estimation function (e.g., your Newton-Raphson solver). Ensure C++ dependencies are correctly specified in DESCRIPTION. Build and test the package.\n\n\n\n\n\n# data\nset.seed(123)\nN_log &lt;- 200\nX1_log &lt;- rnorm(N_log)\nX2_log &lt;- runif(N_log)\nX_matrix_log &lt;- cbind(1, X1_log, X2_log) \nbeta_true_log &lt;- c(-0.5, 1.2, -0.8)\nlinear_predictor_log &lt;- X_matrix_log %*% beta_true_log\nprob_y1_log &lt;- exp(linear_predictor_log) / (1 + exp(linear_predictor_log))\nY_binary_log &lt;- rbinom(N_log, 1, prob_y1_log)\n\n#a\nfLogisticLogLik &lt;- function(vBeta, mX, vY) {\n  dSum &lt;- 0\n  for (i in 1:length(vY)) {\n    dSum &lt;- dSum + (vY[i] * (mX %*% vBeta)[i] - log(1 + exp(mX %*% vBeta)[i]))\n  }\n  return(dSum)\n  \n  #  alternatively\n  #return(-sum(vY * mX %*% vBeta - log(1 + exp(mX %*% vBeta))))\n}\n\n#b\nfLogisticGradient &lt;- function(vBeta, mX, vY) {\n  dProb &lt;- exp(mX %*% vBeta) / (1 + exp(mX %*% vBeta))\n  dSum &lt;- numeric(length(vBeta))\n  for (i in 1:length(vY)) {\n    dSum &lt;- dSum + (vY[i] - dProb[i]) * mX[i, ]\n  }\n  return(dSum)\n  \n  # alternatively\n  # return(t(mX) %*% (vY - dProb))\n}\n\nfLogisticHessian &lt;- function(vBeta, mX, vY) {\n  dProb &lt;- exp(mX %*% vBeta) / (1 + exp(mX %*% vBeta))\n  mHessianSum &lt;- matrix(0, ncol(mX), ncol(mX))\n  for (i in 1:length(vY)) {\n    mHessianSum &lt;- mHessianSum + dProb[i] * (1 - dProb[i]) * mX[i, ] %*% t(mX[i, ])\n  }\n  return(-mHessianSum)\n  \n  # alternatively\n  # return(- t(mX) %*% diag(as.numeric(dProb * (1 - dProb))) %*% mX)\n  # return(-crossprod(mX, mX * as.numeric(dProb * (1 - dProb))))\n}\n\n#c\nfNewton &lt;- function(f, fScore, fHessian, vY, mX, add.constant = TRUE, init.vals = NULL, max.iter = 200, dTol = 1e-9) {\n  i &lt;- 0\n  vB &lt;- init.vals\n  \n  # Keep updating until stopping criterion or max iterations reached\n  while ((max(abs(fScore(vB, mX, vY))) &gt; dTol) && (i &lt; max.iter)) {\n    # Newton-Raphson updating\n    vB &lt;- vB - solve(fHessian(vB, mX, vY), fScore(vB, mX, vY))\n    i &lt;- i + 1\n    #cat(\"At iteration\", n, \"the value of the parameter is:\", dParam, \"\\n\")\n  }\n  \n  if (i == max.iter) {\n    return(list(\n      beta_hat = NULL, \n      log_likelihood_opt = NULL, \n      score_opt = NULL,\n      hessian_opt = NULL,\n      log_likelihood_null = NULL,\n      #predicted_probabilities = NULL,\n      iterations = i,  \n      msg = \"Algorithm failed to converge. Maximum iterations reached.\")\n    )\n  } else {\n    return(list(\n      beta_hat = vB, \n      log_likelihood_opt = f(vB, mX, vY), \n      score_opt = fScore(vB, mX, vY),\n      hessian_opt = fHessian(vB, mX, vY),\n      log_likelihood_null = f(vB, mX, vY),\n      #predicted_probabilities = exp(mX %*% vB) / (1 + exp(mX %*% vB)),\n      iterations = i,  \n      msg = \"Algorithm converged\")\n    )\n  }\n}\n\nfNewton(fLogisticLogLik, fLogisticGradient, fLogisticHessian, vY = Y_binary_log, mX = X_matrix_log, init.vals = c(0, 0, 0))\n#&gt; $beta_hat\n#&gt;              X1_log    X2_log \n#&gt; -0.313394  1.609719 -0.590626 \n#&gt; \n#&gt; $log_likelihood_opt\n#&gt; [1] -100.5938\n#&gt; \n#&gt; $score_opt\n#&gt;                      X1_log        X2_log \n#&gt; -1.963759e-10  6.407941e-10 -1.425498e-10 \n#&gt; \n#&gt; $hessian_opt\n#&gt;                     X1_log     X2_log\n#&gt; [1,] -33.374735  -4.528574 -16.508856\n#&gt; [2,]  -4.528574 -16.413238  -2.941622\n#&gt; [3,] -16.508856  -2.941622 -10.913058\n#&gt; \n#&gt; $log_likelihood_null\n#&gt; [1] -100.5938\n#&gt; \n#&gt; $iterations\n#&gt; [1] 5\n#&gt; \n#&gt; $msg\n#&gt; [1] \"Algorithm converged\"\n\n#d\noptim(c(0, 0, 0), fLogisticLogLik, gr = fLogisticGradient, mX = X_matrix_log, vY = Y_binary_log, method = \"BFGS\", control=list(fnscale=-1))\n#&gt; $par\n#&gt; [1] -0.3133828  1.6097293 -0.5906087\n#&gt; \n#&gt; $value\n#&gt; [1] -100.5938\n#&gt; \n#&gt; $counts\n#&gt; function gradient \n#&gt;       15        8 \n#&gt; \n#&gt; $convergence\n#&gt; [1] 0\n#&gt; \n#&gt; $message\n#&gt; NULL\n\n\ndouble logistic_neg_log_lik_cpp(vec vBeta, mat mX, vec vY) {\n  double dSum = 0.0;\n  vec linear_term = mX * vBeta;\n  int n = vY.size();\n  for (int i = 0; i &lt; n; i++) {\n    dSum += (vY(i) * linear_term(i)) - log(1 + exp(linear_term(i)));\n  }\n  return(dSum);\n}\n\nf(vB, mX, vY)\n\n\nsuppressMessages(library(Rcpp))\nsuppressMessages(library(RcppArmadillo))\nsourceCpp(\"likelihood_exercises.cpp\")\n\nfLogisticLogLik(c(-0.313394, 1.609719, -0.590626), X_matrix_log, Y_binary_log)\n#&gt; [1] -100.5938\nlogistic_neg_log_lik_cpp(c(-0.313394, 1.609719, -0.590626), X_matrix_log, Y_binary_log)\n#&gt; [1] -100.5938",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Likelihood Exercises</span>"
    ]
  },
  {
    "objectID": "likelihood_exercises.html#ar1-model-with-students-t-errors-extremely-hard",
    "href": "likelihood_exercises.html#ar1-model-with-students-t-errors-extremely-hard",
    "title": "\n31  Likelihood Exercises\n",
    "section": "\n33.2 (2) AR(1) Model with Student’s t-errors (Extremely Hard)\n",
    "text": "33.2 (2) AR(1) Model with Student’s t-errors (Extremely Hard)\n\n\nConsider an AR(1) model: \\(y_t = \\phi y_{t-1} + \\epsilon_t\\), where \\(\\epsilon_t \\sim \\text{i.i.d. Standardized Student's t}(\\nu)\\).\nThe PDF of a standardized Student’s t-distribution with \\(\\nu\\) degrees of freedom is \\(f(\\epsilon_t | \\nu) = \\frac{\\Gamma((\\nu+1)/2)}{\\sqrt{\\pi(\\nu-2)}\\Gamma(\\nu/2)} \\left(1 + \\frac{\\epsilon_t^2}{\\nu-2}\\right)^{-(\\nu+1)/2}\\). (Note: \\(\\sigma\\) (scale) is implicitly 1 here for the standardized version, or rather, scale is \\(\\sqrt{(\\nu-2)/\\nu}\\) times the scale of a non-standardized t to make variance 1. The formula given is for a t-dist scaled to have variance 1.)\nParameters are \\(\\boldsymbol{\\theta} = (\\phi, \\nu)\\). Constraints: \\(|\\phi|&lt;1\\) (stationarity), \\(\\nu &gt; 2\\) (for defined variance).\nThe conditional log-likelihood for \\(y_t\\) given \\(y_{t-1}\\) is \\(\\log f(y_t - \\phi y_{t-1} | \\nu)\\). The total log-likelihood sums these over \\(t=2, \\dots, T\\) (conditioning on \\(y_1\\)).\nData:\n\n\nset.seed(800)\nT_ar &lt;- 300\nphi_true &lt;- 0.7\nnu_true &lt;- 5 # Degrees of freedom\n# Simulate standardized t errors (variance = 1)\nerrors_t &lt;- rt(T_ar, df = nu_true) * sqrt((nu_true - 2) / nu_true) \ny_ar &lt;- numeric(T_ar)\ny_ar[1] &lt;- errors_t[1] / sqrt(1 - phi_true^2) # Start from unconditional variance\nfor(t_idx in 2:T_ar) {\n  y_ar[t_idx] &lt;- phi_true * y_ar[t_idx-1] + errors_t[t_idx]\n}\n\n\n\nTasks:\n\n\n\nR Likelihood: Write an R function fAR1tNegLogLik(vParams, vY) for the negative log-likelihood. vParams = c(phi, nu). Use lgamma() for \\(\\log(\\Gamma(\\cdot))\\).\n\n\n\nReparameterization:\n\n\n\n\\(\\phi = \\tanh(\\tilde{\\phi})\\) (ensures \\(|\\phi|&lt;1\\))\n\n\\(\\nu = 2 + \\exp(\\tilde{\\nu})\\) (ensures \\(\\nu&gt;2\\)) Modify your likelihood to take \\(\\tilde{\\boldsymbol{\\theta}} = (\\tilde{\\phi}, \\tilde{\\nu})\\).\n\n\n\n\nNumerical Derivatives: Use numDeriv::grad() and numDeriv::hessian() (if allowed/available) to compute the gradient and Hessian of the reparameterized negative log-likelihood at a test point (e.g., \\(\\tilde{\\phi}=0, \\tilde{\\nu}=\\log(3)\\) which means \\(\\phi=0, \\nu=5\\)).\n\n\n\nCustom Newton-Raphson (on reparameterized LL): Implement Newton-Raphson using the numerical derivatives from (c) to estimate \\(\\tilde{\\boldsymbol{\\theta}}\\). If numerical derivatives are too slow or unstable for the Hessian within the loop, you might use BFGS update for Hessian or just use optim.\n\n\n\noptim() with Numerical Gradient: Use optim() with method “BFGS” and your reparameterized negative log-likelihood. Provide the gradient numerically using numDeriv::grad() in the gr argument of optim.\n\n\n\nResults: Transform estimates back to \\(\\phi^*, \\nu^*\\) and compare with true values.\n\n\n\nC++ Likelihood (Challenge): Implement the core calculation \\(\\log f(\\epsilon_t | \\nu)\\) in C++. You will need R::lgammafn and R::dt (be careful with R::dt’s arguments, it might not be for standardized t directly, you might need to implement the PDF formula). Sum these in an outer C++ loop. Compare evaluation speed.\n\n\n\nSimulation Study (Conceptual): How would you set up a small Monte Carlo simulation study to evaluate the performance (bias, RMSE) of your MLE estimator for \\(\\phi\\) and \\(\\nu\\) using, say, 100 replications of data like the one generated? (This part is about describing the loop and storage, not full implementation unless time allows). How could parallel processing help here?\n\n\n\n\n\n#data\nset.seed(800)\nT_ar &lt;- 300\nphi_true &lt;- 0.7\nnu_true &lt;- 5 # Degrees of freedom\n# Simulate standardized t errors (variance = 1)\nerrors_t &lt;- rt(T_ar, df = nu_true) * sqrt((nu_true - 2) / nu_true) \ny_ar &lt;- numeric(T_ar)\ny_ar[1] &lt;- errors_t[1] / sqrt(1 - phi_true^2) # Start from unconditional variance\nfor(t_idx in 2:T_ar) {\n  y_ar[t_idx] &lt;- phi_true * y_ar[t_idx-1] + errors_t[t_idx]\n}\n\n#a + b\nfAR1tNegLogLik &lt;- function(vParams, vY) {\n  dPhi &lt;- tanh(vParams[1])\n  dNu &lt;- 2 + exp(vParams[2])\n  vEps &lt;- numeric(length(vY))\n  \n  dSum &lt;- 0\n  for (t in 2:length(vY)) {\n    vEps[t] &lt;- vY[t] - dPhi * vY[t-1]\n    dSum &lt;- dSum + lgamma((dNu+1)/2) - log(sqrt(pi * (dNu-2))) - lgamma(dNu/2) - (dNu + 1) / 2 * log(1 + vEps[t]^2/ (dNu - 2))\n  }\n  return(-dSum)\n}\n\n#c\nsuppressMessages(library(numDeriv))\nfAR1tGrad &lt;- function(f, vParams, ...) {\n  return(grad(f, vParams, ...))\n}\nfAR1tHess &lt;- function(f, vParams, ...) {\n  return(hessian(f, vParams, ...))\n}\n\nfAR1tGrad(fAR1tNegLogLik, c(0, log(3)), vY = y_ar)\n#&gt; [1] -353.44783  -12.69966\nfAR1tHess(fAR1tNegLogLik, c(0, log(3)), vY = y_ar)\n#&gt;           [,1]     [,2]\n#&gt; [1,] 205.88262  3.80849\n#&gt; [2,]   3.80849 23.23789\n\n#d\nfNewton &lt;- function(f, fScore, fHessian, init.vals = NULL, max.iter = 200, dTol = 1e-9, ...) {\n  i &lt;- 0\n  vPars &lt;- init.vals\n  \n  while ((max(abs(fScore(f, vPars, ...))) &gt; dTol) && (i &lt; max.iter)) {\n    vPars &lt;- vPars - solve(fHessian(f, vPars, ...), fScore(f, vPars, ...))\n    i &lt;- i + 1\n  }\n  \n  if (i == max.iter) {\n    return(list(\n      curr_params = vPars, \n      curr_log_likelihood_opt = f(vPars, ...), \n      curr_score_opt = fScore(f, vPars, ...),\n      curr_hessian_opt = fHessian(f, vPars, ...),\n      iterations = i,  \n      msg = \"Algorithm failed to converge. Maximum iterations reached.\")\n    )\n  } else {\n    return(list(\n      params = vPars, \n      log_likelihood_opt = f(vPars, ...), \n      score_opt = fScore(f, vPars, ...),\n      hessian_opt = fHessian(f, vPars, ...),\n      iterations = i,  \n      msg = \"Algorithm converged\")\n    )\n  }\n}\n\nfNewton(fAR1tNegLogLik, fAR1tGrad, fAR1tHess, vY = y_ar, init.vals = c(0.2, log(3)))\n#&gt; $params\n#&gt; [1] 0.8557711 1.1124668\n#&gt; \n#&gt; $log_likelihood_opt\n#&gt; [1] 410.3974\n#&gt; \n#&gt; $score_opt\n#&gt; [1] -2.601885e-10  5.501685e-10\n#&gt; \n#&gt; $hessian_opt\n#&gt;             [,1]       [,2]\n#&gt; [1,] 200.2957152 -0.5986651\n#&gt; [2,]  -0.5986651  9.6342894\n#&gt; \n#&gt; $iterations\n#&gt; [1] 92\n#&gt; \n#&gt; $msg\n#&gt; [1] \"Algorithm converged\"\nvPars_temp &lt;- fNewton(fAR1tNegLogLik, fAR1tGrad, fAR1tHess, vY = y_ar, init.vals = c(0.2, log(3)))$params\n\ndPhi_star &lt;- tanh(vPars_temp[1])\ndNu_star &lt;- 2 + exp(vPars_temp[2])\n\nprint(paste0(\"Phi-est: \", dPhi_star, \" vs true: \", phi_true))\n#&gt; [1] \"Phi-est: 0.694072436111299 vs true: 0.7\"\nprint(paste0(\"Nu-est: \", dNu_star, \" vs true: \", nu_true))\n#&gt; [1] \"Nu-est: 5.04185272789097 vs true: 5\"\n\n# e\nvPars_temp &lt;- optim(c(0.2, log(3)), fn = fAR1tNegLogLik, gr = function(par, ...) { fAR1tGrad(fAR1tNegLogLik, par, ...) }, vY = y_ar, method = \"BFGS\")$par\n\n# f\ndPhi_star &lt;- tanh(vPars_temp[1])\ndNu_star &lt;- 2 + exp(vPars_temp[2])\n\nprint(paste0(\"Phi-est: \", dPhi_star, \" vs true: \", phi_true))\n#&gt; [1] \"Phi-est: 1 vs true: 0.7\"\nprint(paste0(\"Nu-est: \", dNu_star, \" vs true: \", nu_true))\n#&gt; [1] \"Nu-est: 6.97158263449599 vs true: 5\"",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Likelihood Exercises</span>"
    ]
  },
  {
    "objectID": "likelihood_exercises.html#zero-inflated-poisson-zip-regression-likelihood-extremely-hard",
    "href": "likelihood_exercises.html#zero-inflated-poisson-zip-regression-likelihood-extremely-hard",
    "title": "\n31  Likelihood Exercises\n",
    "section": "\n33.3 (3) Zero-Inflated Poisson (ZIP) Regression Likelihood (Extremely Hard)\n",
    "text": "33.3 (3) Zero-Inflated Poisson (ZIP) Regression Likelihood (Extremely Hard)\n\n\nA ZIP model assumes that zero outcomes \\(y_i=0\\) can come from two sources:\n\nA “certain zero” state (e.g., never uses a service), with probability \\(\\pi_i\\).\nA Poisson count process that happens to produce a zero, with probability \\((1-\\pi_i) \\cdot P(Count=0|\\lambda_i)\\).\n\n\nThe probability mass function is: \\(P(y_i=k) = \\pi_i \\cdot I(k=0) + (1-\\pi_i) \\cdot \\frac{e^{-\\lambda_i}\\lambda_i^k}{k!}\\) for \\(k=0,1,2,\\dots\\) where \\(I(k=0)\\) is an indicator function (1 if \\(k=0\\), 0 otherwise).\nOften, \\(\\pi_i\\) and \\(\\lambda_i\\) are modeled with covariates:\n\n\n\\(\\text{logit}(\\pi_i) = \\mathbf{w}_i'\\boldsymbol{\\gamma}\\) (Logistic regression for zero-inflation probability)\n\n\\(\\log(\\lambda_i) = \\mathbf{x}_i'\\boldsymbol{\\beta}\\) (Log-linear model for Poisson mean)\n\n\nParameters: \\(\\boldsymbol{\\theta} = (\\boldsymbol{\\gamma}', \\boldsymbol{\\beta}')\\).\nData Simulation:\n\n\nset.seed(999)\nN_zip &lt;- 250\nW1_zip &lt;- rnorm(N_zip); W_matrix_zip &lt;- cbind(1, W1_zip) # For pi\nX1_zip &lt;- runif(N_zip); X_matrix_zip_lambda &lt;- cbind(1, X1_zip) # For lambda\n    \ngamma_true_zip &lt;- c(-1, 0.8) # logit(pi) params\nbeta_true_zip &lt;- c(0.5, 1.5)   # log(lambda) params\n    \nlogit_pi_vals &lt;- W_matrix_zip %*% gamma_true_zip\npi_vals &lt;- exp(logit_pi_vals) / (1 + exp(logit_pi_vals))\n    \nlog_lambda_vals &lt;- X_matrix_zip_lambda %*% beta_true_zip\nlambda_vals &lt;- exp(log_lambda_vals)\n    \nY_zip &lt;- numeric(N_zip)\nfor(i_zip in 1:N_zip) {\n  if (runif(1) &lt; pi_vals[i_zip]) {\n    Y_zip[i_zip] &lt;- 0 # Certain zero\n  } else {\n    Y_zip[i_zip] &lt;- rpois(1, lambda_vals[i_zip]) # From Poisson\n  }\n}\n# hist(Y_zip, breaks=-0.5:(max(Y_zip)+0.5)) # Lots of zeros?\n\n\n\nTasks:\n\n\n\nR Likelihood: Write an R function fZIPNegLogLik(vParams, vY, mW, mX) for the negative log-likelihood. vParams will contain both \\(\\boldsymbol{\\gamma}\\) and \\(\\boldsymbol{\\beta}\\). You need to unpack them carefully.\n\n\n\nOptimization: Use optim() (“BFGS” or “Nelder-Mead”) to find the MLEs for \\(\\boldsymbol{\\gamma}\\) and \\(\\boldsymbol{\\beta}\\). This is complex; good starting values are crucial (e.g., \\(\\boldsymbol{\\gamma}\\) from a logit model predicting \\(y_i=0\\) vs \\(y_i&gt;0\\), and \\(\\boldsymbol{\\beta}\\) from a Poisson regression on \\(y_i[y_i&gt;0]\\)).\n\n\n\nC++ Likelihood (Point calculation): Implement a C++ function zip_log_lik_point_cpp(int yi, double logit_pi_i, double log_lambda_i) that calculates the log-likelihood for a single observation \\(y_i\\), given its specific \\(\\text{logit}(\\pi_i)\\) and \\(\\log(\\lambda_i)\\). Use R::pnorm (for logit to prob), R::dpois.\n\n\n\nFull C++ Likelihood (Loop): Write a C++ function fZIPNegLogLikCpp(arma::vec vGamma, arma::vec vBeta, arma::ivec vY, arma::mat mW, arma::mat mX) that calculates the total negative log-likelihood by iterating through observations and calling a C++ point likelihood (or calculating directly). This will require computing \\(\\mathbf{w}_i'\\boldsymbol{\\gamma}\\) and \\(\\mathbf{x}_i'\\boldsymbol{\\beta}\\) for each \\(i\\).\n\n\n\nComparison: Compare the speed of a single evaluation of your R and C++ full likelihood functions.\n\n\n\nPackage (Conceptual): If you were to package this, what would be the main user-facing R function? What arguments would it take? What would it return? How would you handle the C++ code?\n\n\n\n\n\n#data\nset.seed(999)\nN_zip &lt;- 250\nW1_zip &lt;- rnorm(N_zip); W_matrix_zip &lt;- cbind(1, W1_zip) # For pi\nX1_zip &lt;- runif(N_zip); X_matrix_zip_lambda &lt;- cbind(1, X1_zip) # For lambda\n    \ngamma_true_zip &lt;- c(-1, 0.8) # logit(pi) params\nbeta_true_zip &lt;- c(0.5, 1.5)   # log(lambda) params\n    \nlogit_pi_vals &lt;- W_matrix_zip %*% gamma_true_zip\npi_vals &lt;- exp(logit_pi_vals) / (1 + exp(logit_pi_vals))\n    \nlog_lambda_vals &lt;- X_matrix_zip_lambda %*% beta_true_zip\nlambda_vals &lt;- exp(log_lambda_vals)\n    \nY_zip &lt;- numeric(N_zip)\nfor(i_zip in 1:N_zip) {\n  if (runif(1) &lt; pi_vals[i_zip]) {\n    Y_zip[i_zip] &lt;- 0 # Certain zero\n  } else {\n    Y_zip[i_zip] &lt;- rpois(1, lambda_vals[i_zip]) # From Poisson\n  }\n}\n\n#a\nfZIPNegLogLik &lt;- function(vParams, vY, mW, mX) {\n  vGamma &lt;- vParams[1:ncol(mW)]\n  vBeta &lt;- vParams[(ncol(mW) + 1):(ncol(mW) + ncol(mX))]\n  \n  logit_pi_i &lt;- mW %*% vGamma\n  pi_i &lt;- plogis(logit_pi_i)\n  log_lambda_i &lt;- mX %*% vBeta\n  lambda_i &lt;- exp(log_lambda_i)\n  \n  dSum &lt;- 0\n  for (i in 1:length(vY)) {\n    if (vY[i] == 0) {\n      dSum &lt;- dSum + log(pi_i[i] + (1 - pi_i[i]) * ((exp(-lambda_i[i]) * lambda_i[i]^vY[i]) / factorial(vY[i])))\n    } else {\n      dSum &lt;- dSum + log((1 - pi_i[i]) * ((exp(-lambda_i[i]) * lambda_i[i]^vY[i]) / factorial(vY[i])))\n    }\n  }\n  return(-dSum)\n}\n\noptim(c(0, 0, 0, 0), fZIPNegLogLik, vY = Y_zip, mW = W_matrix_zip, mX = X_matrix_zip_lambda, method = \"BFGS\")\n#&gt; $par\n#&gt; [1] -1.0449956  0.6241883  0.5144958  1.5029647\n#&gt; \n#&gt; $value\n#&gt; [1] 490.5832\n#&gt; \n#&gt; $counts\n#&gt; function gradient \n#&gt;       26       10 \n#&gt; \n#&gt; $convergence\n#&gt; [1] 0\n#&gt; \n#&gt; $message\n#&gt; NULL",
    "crumbs": [
      "Problem sets",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Likelihood Exercises</span>"
    ]
  }
]